{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb8b16b3533c83c",
   "metadata": {},
   "source": [
    "# Replication of Clayson et al., 2023 Preprocessing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.utils import set_log_file\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aafc23f7d0533a",
   "metadata": {},
   "source": [
    "SST INFO: \n",
    "\n",
    "Triale z reakcj¹ wyprzedzaj¹c¹ stop (przedwczesn¹) wyró¿nione zosta³y numerem 1 (w kontracie do 0) na czwartej pozycji w triggerze i zosta³y wykluczone z \n",
    "\"Segmentation Global\".\n",
    "\n",
    "TODO\n",
    "\n",
    "- SST-005 - sygna³ zreferowany wy³¹cznie do A1, poniewa¿ A2 jest zbyt zaszumiona\n",
    "- SST-044 - sygna³ zreferowany wy³¹cznie do A2, poniewa¿ A1 jest mocniej zaszumiona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb43fd47897e7af",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3ba90e198d2cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318f7c90cebce3d",
   "metadata": {},
   "source": [
    "Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2570b867ca739b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a custom logger for preprocessing INFO\n",
    "logger_preprocessing_info = logging.getLogger('preprocessing_info')\n",
    "logger_preprocessing_info.setLevel(logging.INFO)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a custom logger for errors\n",
    "logger_errors_info = logging.getLogger('errors')\n",
    "logger_errors_info.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9bbbeafc9c8c4",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62ce10ab154b16",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def read_trigger_map(file_name):\n",
    "    line_count = 0\n",
    "    trigger_map = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read each line and increment the counter\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "            trigger = (match.group(1), match.group(2), )\n",
    "            trigger_map.append(trigger)\n",
    "        except:\n",
    "            pass\n",
    "        while line:\n",
    "            line_count += 1\n",
    "            line = file.readline()\n",
    "            try:\n",
    "                match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "                trigger = (match.group(1), match.group(2), )\n",
    "                trigger_map.append(trigger)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    assert len(trigger_map) == line_count, \\\n",
    "        f'The length of trigger file ({line_count}) not equals length of created trigger_map ({len(trigger_map)})'\n",
    "\n",
    "    return trigger_map\n",
    "\n",
    "def create_triggers_dict(trigger_map):\n",
    "    triggers_codes = [item[1] for item in trigger_map]\n",
    "    # Create an ordered dictionary to maintain order and remove duplicates\n",
    "    unique_ordered_dict = OrderedDict.fromkeys(triggers_codes)\n",
    "    numbered_dict = {key: 1000 + number for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    reversed_numbered_dict = {1000 + number: key for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    return numbered_dict, reversed_numbered_dict\n",
    "\n",
    "def replace_trigger_names(raw, participant_id, trigger_map, new_response_event_dict=None, replace=False, search='RE'):\n",
    "    # Replace event IDs in the Raw object\n",
    "    events = mne.find_events(raw, stim_channel='Status')\n",
    "    new_events_list = events.copy()\n",
    "    \n",
    "    # add trigger to corrupted bdf files - too short reaction \n",
    "    if paradigm == 'GNG' and (participant_id == 'B-GNG-199' or participant_id == 'B-GNG-208'):\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "    # add missing RE triggers to bdf file - to short time between stop trigger and reaction trigger\n",
    "    ids = ['SST-165', 'SST-211', 'SST-122', 'SST-088','SST-045','SST-012','SST-083','SST-136','SST-125']\n",
    "    if paradigm == 'SST' and participant_id in ids:\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "    \n",
    "    # delete ghost event 65312\n",
    "    if paradigm == 'SST' and (participant_id == 'SST-181'):\n",
    "        for idx, event in enumerate(events):\n",
    "            if event[2] == 65312:\n",
    "                new_events_list = np.concatenate([events[:idx, :], events[idx+1:, :]]) \n",
    "                break  \n",
    "    \n",
    "    # delete ghost event 0: 130816\n",
    "    if paradigm == 'SST' and (participant_id == 'SST-075'):\n",
    "        for idx, event in enumerate(events):\n",
    "            if event[2] == 130816:\n",
    "                new_events_list = np.concatenate([events[:idx, :], events[idx+1:, :]]) \n",
    "                break   \n",
    "    \n",
    "    if paradigm == 'SST' and participant_id == 'SST-130':\n",
    "        \n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "\n",
    "        for idx, event in enumerate(new_events_list):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([new_events_list[:idx, :], [[new_events_list[idx-1][0] + delta_time, 0, 65281]], new_events_list[idx:, :]]) \n",
    "                break\n",
    "                           \n",
    "    logger_preprocessing_info.info(f'EVENTS: {new_events_list}')\n",
    "\n",
    "    assert len(new_events_list) == len(trigger_map), \\\n",
    "            f'The length of trigger map ({len(trigger_map)}) not equals length of events in eeg recording ({len(new_events_list)})'\n",
    "\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    for idx, event in enumerate(new_events_list):\n",
    "        event_id = str(event[2])[-1]\n",
    "        trigger_id = trigger_map[idx][0]\n",
    "        trigger_new_code = trigger_map[idx][1]\n",
    "        \n",
    "        if event_id != trigger_id:\n",
    "            logger_errors_info.info(f'An event {idx} has different number than in provided file. {trigger_id} expected, {str(event[2])} found. Triggers may need to be checked.')\n",
    "\n",
    "        trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "        new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "    annot_from_events = mne.annotations_from_events(\n",
    "        events=new_events_list,\n",
    "        event_desc=mapping,\n",
    "        sfreq=raw.info[\"sfreq\"],\n",
    "        orig_time=raw.info[\"meas_date\"],\n",
    "    )\n",
    "    raw_copy = raw.copy()\n",
    "    raw_copy.set_annotations(annot_from_events)\n",
    "\n",
    "    return raw_copy\n",
    "\n",
    "def find_items_matching_regex(dictionary, regex_list):\n",
    "    matching_items = {}\n",
    "    for regex in regex_list:\n",
    "        pattern = re.compile(regex)\n",
    "        matching_items.update({key: value for key, value in dictionary.items() if pattern.match(key)})\n",
    "    return matching_items\n",
    "\n",
    "@dataclass\n",
    "class ParticipantTriggerMappingContext:\n",
    "    event_dict: dict\n",
    "    events_mapping: dict\n",
    "    new_event_dict: dict\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.event_dict}\\n{self.events_mapping}\\n{self.new_event_dict}\"\n",
    "\n",
    "def create_events_mappings(trigger_map, case='RE') -> ParticipantTriggerMappingContext:\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    if case == 'RE':\n",
    "        new_event_dict = {\"correct_response\": 0, \"error_response\": 1, \"incorrect_go_response\": 2}\n",
    "        events_mapping = {\n",
    "            'correct_response': [],\n",
    "            'error_response': [],\n",
    "            'incorrect_go_response' : []\n",
    "        }\n",
    "        \n",
    "        # find response events from experimental blocks\n",
    "        regex_pattern = [r'RE\\*image\\*.*\\*0\\*.*', r'RE\\*image\\*.*\\*-\\*.*']\n",
    "        event_dict = find_items_matching_regex(trigger_map_codes, regex_pattern)\n",
    "    \n",
    "        for event_id in event_dict.keys():\n",
    "            event_id_splitted = event_id.split('*')\n",
    "    \n",
    "            if (event_id_splitted[3] == '-') and (event_id_splitted[-1] == event_id_splitted[-2]):\n",
    "                events_mapping['correct_response'].append(event_dict[event_id])\n",
    "            elif (event_id_splitted[3] == '-') and (event_id_splitted[-1] != event_id_splitted[-2]):\n",
    "                events_mapping['incorrect_go_response'].append(event_dict[event_id])\n",
    "            elif (str(event_id_splitted[3]) == '0') and (event_id_splitted[-1] != '-'):\n",
    "                events_mapping['error_response'].append(event_dict[event_id])\n",
    "    \n",
    "        \n",
    "    elif case == 'STIM':\n",
    "        new_event_dict = {\"inhibited_stop\": 0, \"uninhibited_stop\": 1}\n",
    "        events_mapping = {\n",
    "            'inhibited_stop': [],\n",
    "            'uninhibited_stop': [],\n",
    "        }\n",
    "        # find all target stimuli events from experimental blocks\n",
    "        regex_pattern = [r'ST.*']\n",
    "        event_dict = find_items_matching_regex(trigger_map_codes, regex_pattern)\n",
    "\n",
    "        for event_id in event_dict.keys():\n",
    "            event_id_splitted = event_id.split('*')\n",
    "\n",
    "            if (event_id_splitted[-1] == '-') and (event_id_splitted[3] == '0'):\n",
    "                events_mapping['inhibited_stop'].append(event_dict[event_id])\n",
    "            elif (event_id_splitted[-1] != '-') and (event_id_splitted[3] == '0'):\n",
    "                events_mapping['uninhibited_stop'].append(event_dict[event_id])\n",
    "\n",
    "    else:\n",
    "        logger_errors_info('Not known case. Possible cases: \\'RE\\' for response, \\'STIM\\` for stimuli, and \\`FBCK\\` for feedback-locked events extraction.')\n",
    "        # todo raise an Error\n",
    "        event_dict = {}\n",
    "        events_mapping = {}\n",
    "        new_event_dict = {}\n",
    "\n",
    "\n",
    "    return ParticipantTriggerMappingContext(event_dict=event_dict, \n",
    "                                            events_mapping=events_mapping,\n",
    "                                            new_event_dict=new_event_dict)\n",
    "\n",
    "def create_epochs(\n",
    "        raw,\n",
    "        context: ParticipantTriggerMappingContext,\n",
    "        tmin=-.1,\n",
    "        tmax=.6,\n",
    "        reject=None,\n",
    "        reject_by_annotation=False,\n",
    "):\n",
    "    # select specific events\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=context.event_dict)\n",
    "\n",
    "    # Merge different events of one kind\n",
    "    for mapping in context.events_mapping:\n",
    "        events = mne.merge_events(\n",
    "            events=events,\n",
    "            ids=context.events_mapping[mapping],\n",
    "            new_id=context.new_event_dict[mapping],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=events,\n",
    "        event_id=context.new_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=reject_by_annotation,\n",
    "        preload=True,\n",
    "        reject=reject,\n",
    "        picks=['eeg', 'eog'],\n",
    "        on_missing = 'warn',\n",
    "    )\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "\n",
    "\n",
    "# def ocular_correction_ica(raw, raw_unfiltered, veog=[], heog=[], info=False, from_template='auto', context=None):\n",
    "#     filtered_raw_ica = raw_unfiltered.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "    \n",
    "#     ica_epochs = create_epochs(\n",
    "#         filtered_raw_ica,\n",
    "#         context,\n",
    "#     )\n",
    "\n",
    "#     ica = mne.preprocessing.ICA(\n",
    "#         n_components=15,\n",
    "#         method='infomax',\n",
    "#         max_iter=\"auto\",\n",
    "#         random_state=random_state\n",
    "#     )\n",
    "#     ica.fit(filtered_raw_ica)\n",
    "    \n",
    "#     ica.exclude = []\n",
    "    \n",
    "#     # find which ICs match the VEOG pattern\n",
    "#     veog_indices_eogs, veog_scores = ica.find_bads_eog(\n",
    "#         filtered_raw_ica,\n",
    "#         ch_name=veog,\n",
    "#         threshold=0.9,\n",
    "#         measure='correlation'\n",
    "#     )\n",
    "\n",
    "#     # find which ICs match the HEOG pattern\n",
    "#     heog_indices_eogs, heog_scores = ica.find_bads_eog(\n",
    "#         filtered_raw_ica,\n",
    "#         ch_name=heog,\n",
    "#         threshold=0.6,\n",
    "#         measure='correlation'\n",
    "#     )\n",
    "    \n",
    "#     logger_preprocessing_info.info(f'EOG based excluded ICA components:\\nVEOG: {veog_indices_eogs}\\nHEOG: {heog_indices_eogs}')\n",
    "\n",
    "    \n",
    "#     heog_indices = []\n",
    "#     veog_indices = []\n",
    "    \n",
    "#     if info:\n",
    "#         logger_preprocessing_info.info('ICA components')\n",
    "#         fig = ica.plot_components()\n",
    "  \n",
    "#     if (len(veog_indices_eogs + heog_indices_eogs) == 0 and from_template == 'auto') or from_template == True:\n",
    "#         logger_preprocessing_info.info('Using templates...')\n",
    "#         templates_ica = pd.read_pickle('public_data/eog_templates.pkl')\n",
    "\n",
    "#         template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "#         template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "#         # todo: make try - except and if faile - do another ICA with 20 components. Default amount of components: 15\n",
    "        \n",
    "#         try:\n",
    "#             mne.preprocessing.corrmap(\n",
    "#                 [ica], \n",
    "#                 template=template_veog_component, \n",
    "#                 threshold=0.9, \n",
    "#                 label=\"veog blink\", \n",
    "#                 plot=False\n",
    "#             )\n",
    "#             mne.preprocessing.corrmap(\n",
    "#                 [ica], \n",
    "#                 template=template_heog_component, \n",
    "#                 threshold=0.8, \n",
    "#                 label=\"heog blink\", \n",
    "#                 plot=False\n",
    "#             )\n",
    "\n",
    "#             veog_indices = ica.labels_['veog blink']\n",
    "#             heog_indices = ica.labels_['heog blink']\n",
    "            \n",
    "#             if (len(veog_indices_eogs) == 1) and (len(veog_indices) != 1) and (veog_indices_eogs[0] in veog_indices):\n",
    "#                 veog_indices = veog_indices_eogs\n",
    "#                 logger_errors_info.info(f'Too much VEOG patterns found. Comparing to VEOG channels')\n",
    "            \n",
    "#             if (len(heog_indices_eogs) == 1) and (len(heog_indices) != 1) and (heog_indices_eogs[0] in heog_indices):\n",
    "#                 heog_indices = heog_indices_eogs\n",
    "#                 logger_errors_info.info(f'Too much HEOG patterns found. Comparing to HEOG channels')\n",
    "            \n",
    "#         except:\n",
    "#             logger_errors_info.info(f'No patterns found. Trying to use results from EOG channels')\n",
    "#             if (len(veog_indices_eogs) == 1) and (len(veog_indices) == 0):\n",
    "#                 veog_indices = veog_indices_eogs\n",
    "#             if (len(heog_indices_eogs) == 1) and (len(heog_indices) == 0):\n",
    "#                 heog_indices = heog_indices_eogs\n",
    "\n",
    "#     logger_preprocessing_info.info(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "#     fig = ica.plot_components(veog_indices + heog_indices)\n",
    "    \n",
    "#     assert len((veog_indices + heog_indices)) == 2, f'Number of ICA components to exclude ({len(veog_indices + heog_indices)}) is different than 2' \n",
    "    \n",
    "#     ica.exclude = veog_indices + heog_indices\n",
    "    \n",
    "#     reconstructed_raw = raw.copy()\n",
    "#     ica.apply(reconstructed_raw)\n",
    "    \n",
    "#     del filtered_raw_ica\n",
    "\n",
    "#     return reconstructed_raw\n",
    "\n",
    "def ica_epochs(filtered_raw_ica, participant_context):\n",
    "    ica_epochs = create_epochs(\n",
    "        filtered_raw_ica,\n",
    "        participant_context,\n",
    "    )\n",
    "\n",
    "    reject_criteria = dict(eeg=200e-6)\n",
    "    ica_epochs.drop_bad(reject=reject_criteria)\n",
    "\n",
    "\n",
    "    ica = mne.preprocessing.ICA(\n",
    "        n_components=.99,\n",
    "        method='infomax',\n",
    "        max_iter=\"auto\",\n",
    "        random_state=random_state\n",
    "    )\n",
    "    # ica.fit(filtered_raw_ica)\n",
    "    ica.fit(ica_epochs)\n",
    "    \n",
    "    heog_indices = []\n",
    "    veog_indices = []\n",
    "\n",
    "    print('ICA components')\n",
    "    fig = ica.plot_components()\n",
    "\n",
    "    templates_ica = pd.read_pickle('public_data/eog_templates.pkl')\n",
    "\n",
    "    template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "    template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "    mne.preprocessing.corrmap(\n",
    "        [ica], \n",
    "        template=template_veog_component,\n",
    "        threshold=0.9,\n",
    "        label=\"veog blink\", \n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    mne.preprocessing.corrmap(\n",
    "        [ica], \n",
    "        template=template_heog_component, \n",
    "        label=\"heog blink\", \n",
    "        threshold=0.8,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    veog_indices = ica.labels_['veog blink']\n",
    "    heog_indices = ica.labels_['heog blink']\n",
    "\n",
    "    logger_preprocessing_info.info(f'Pattern based excluded epochs-based ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "    \n",
    "    return veog_indices, heog_indices\n",
    "    \n",
    "\n",
    "def ocular_correction_ica(raw, raw_unfiltered, veog=[], heog=[], info=False, from_template='auto', context=None):\n",
    "    filtered_raw_ica = raw_unfiltered.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "    \n",
    "    # ica_epochs = create_epochs(\n",
    "    #     filtered_raw_ica,\n",
    "    #     context,\n",
    "    # )\n",
    "\n",
    "    ica = mne.preprocessing.ICA(\n",
    "        n_components=30,\n",
    "        method='infomax',\n",
    "        max_iter=\"auto\",\n",
    "        random_state=random_state\n",
    "    )\n",
    "    ica.fit(filtered_raw_ica)\n",
    "    \n",
    "    ica.exclude = []\n",
    "    \n",
    "    # find which ICs match the VEOG pattern\n",
    "    veog_indices_eogs, veog_scores = ica.find_bads_eog(\n",
    "        filtered_raw_ica,\n",
    "        ch_name=veog,\n",
    "        threshold=0.9,\n",
    "        measure='correlation'\n",
    "    )\n",
    "\n",
    "    # find which ICs match the HEOG pattern\n",
    "    heog_indices_eogs, heog_scores = ica.find_bads_eog(\n",
    "        filtered_raw_ica,\n",
    "        ch_name=heog,\n",
    "        threshold=0.6,\n",
    "        measure='correlation'\n",
    "    )\n",
    "    \n",
    "    logger_preprocessing_info.info(f'EOG based excluded ICA components:\\nVEOG: {veog_indices_eogs}\\nHEOG: {heog_indices_eogs}')\n",
    "\n",
    "    \n",
    "    heog_indices = []\n",
    "    veog_indices = []\n",
    "    \n",
    "    if info:\n",
    "        logger_preprocessing_info.info('ICA components')\n",
    "        fig = ica.plot_components()\n",
    "  \n",
    "    if (len(veog_indices_eogs + heog_indices_eogs) == 0 and from_template == 'auto') or from_template == True:\n",
    "        logger_preprocessing_info.info('Using templates...')\n",
    "        templates_ica = pd.read_pickle('public_data/eog_templates.pkl')\n",
    "\n",
    "        template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "        template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "        # todo: make try - except and if faile - do another ICA with 20 components. Default amount of components: 15\n",
    "        \n",
    "        try:\n",
    "            mne.preprocessing.corrmap(\n",
    "                [ica], \n",
    "                template=template_veog_component, \n",
    "                threshold=0.9, \n",
    "                label=\"veog blink\", \n",
    "                plot=False\n",
    "            )\n",
    "            veog_indices = ica.labels_['veog blink']\n",
    "            \n",
    "            if (len(veog_indices_eogs) == 1) and (len(veog_indices) != 1) and (veog_indices_eogs[0] in veog_indices):\n",
    "                veog_indices = veog_indices_eogs\n",
    "                logger_errors_info.info(f'Too much VEOG patterns found. Comparing to VEOG channels')\n",
    "        except:\n",
    "            logger_errors_info.info(f'No VEOG patterns found. Trying to use results from EOG channels')\n",
    "            if (len(veog_indices_eogs) == 1) and (len(veog_indices) == 0):\n",
    "                veog_indices = veog_indices_eogs\n",
    "            \n",
    "        try:  \n",
    "            mne.preprocessing.corrmap(\n",
    "                [ica], \n",
    "                template=template_heog_component, \n",
    "                threshold=0.8, \n",
    "                label=\"heog blink\", \n",
    "                plot=False\n",
    "            )\n",
    "\n",
    "            heog_indices = ica.labels_['heog blink']\n",
    "\n",
    "            \n",
    "            if (len(heog_indices_eogs) == 1) and (len(heog_indices) != 1) and (heog_indices_eogs[0] in heog_indices):\n",
    "                heog_indices = heog_indices_eogs\n",
    "                logger_errors_info.info(f'Too much HEOG patterns found. Comparing to HEOG channels')\n",
    "            \n",
    "        except:\n",
    "            logger_errors_info.info(f'No HEOG patterns found. Trying to use results from EOG channels')\n",
    "            if (len(heog_indices_eogs) == 1) and (len(heog_indices) == 0):\n",
    "                heog_indices = heog_indices_eogs\n",
    "\n",
    "    logger_preprocessing_info.info(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "    fig = ica.plot_components(veog_indices + heog_indices)\n",
    "    \n",
    "    if len((veog_indices + heog_indices)) != 2:\n",
    "        logger_errors_info.info('Number of raw-based ICA components is different than 2. Trying epoch-based ICA...')\n",
    "        veog_indices, heog_indices = ica_epochs(filtered_raw_ica, participant_context)\n",
    "        \n",
    "    assert len((veog_indices + heog_indices)) == 2, f'Number of ICA components to exclude ({len(veog_indices + heog_indices)}) is different than 2' \n",
    "    \n",
    "    \n",
    "    ica.exclude = veog_indices + heog_indices\n",
    "    \n",
    "    reconstructed_raw = raw.copy()\n",
    "    ica.apply(reconstructed_raw)\n",
    "    \n",
    "    del filtered_raw_ica\n",
    "\n",
    "    return reconstructed_raw\n",
    "\n",
    "def get_k_nearest_neighbors(target_ch_name, epochs, k=6):\n",
    "    \"\"\"\n",
    "    Finds k nearest neighbors of given channel according to the 3D channels positions from the Epoch INFO\n",
    "    :param target_ch_name: String\n",
    "        Name of the target channel.\n",
    "    :param epochs: mne Epochs\n",
    "        Epochs with info attribute that consists of channels positions.\n",
    "    :param k: int\n",
    "        Number of neighbors to use by default for kneighbors queries. \n",
    "    :return: \n",
    "        indices: ndarray of shape (n_neighbors)\n",
    "            Indices of the nearest channels.\n",
    "        neighbor_ch_names: ndarray of shape (n_neighbors)\n",
    "            Names of the nearest channels.   \n",
    "    \"\"\"\n",
    "    epochs_copy_eeg_channels = epochs.copy().pick('eeg')\n",
    "    info = epochs_copy_eeg_channels.info\n",
    "    ch_names = epochs_copy_eeg_channels.info['ch_names']\n",
    "    \n",
    "    chs = [info[\"chs\"][pick] for pick in np.arange(0,len(ch_names))]\n",
    "    electrode_positions_3d =[]\n",
    "    \n",
    "    for ch in chs:\n",
    "        electrode_positions_3d.append((ch['ch_name'], ch[\"loc\"][:3]))\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=k+1, algorithm='auto')\n",
    "    ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d])\n",
    "\n",
    "    neighbors_model.fit(ch_coordinates)\n",
    "\n",
    "    target_ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d if ch_name_coordinates[0] == target_ch_name])\n",
    "    \n",
    "    target_ch_coordinates = target_ch_coordinates.reshape(1,-1)\n",
    "\n",
    "    distances, indices = neighbors_model.kneighbors(target_ch_coordinates)\n",
    "    neighbor_ch_names = []\n",
    "    \n",
    "    # Log the nearest neighbors without the first (self) neighbor\n",
    "    logger_preprocessing_info.debug(f\"{k} Nearest Neighbors of {target_ch_name}:\")\n",
    "    for i, (distance, index) in enumerate(zip(distances.flatten(), indices.flatten())):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            neighbor_point = electrode_positions_3d[index]\n",
    "            logger_preprocessing_info.debug(f\"Neighbor {i + 1}: Index {index}, Distance {distance:.2f}, Coordinates {neighbor_point}\")\n",
    "            neighbor_ch_names.append(neighbor_point[0])\n",
    "            \n",
    "    return indices.flatten()[1:], np.array(neighbor_ch_names)\n",
    "\n",
    "def find_bad_trails(epochs):\n",
    "    \"\"\"\n",
    "    Channels that meet following conditions will be marked as bad for the trail:\n",
    "        (1) Channels with a voltage difference of 100 μV through the duration of the epoch;\n",
    "        (2) Channels that were flat;\n",
    "        (3) Channels with more than a 30 μV difference with the nearest six neighbors; \n",
    "    :param mne Epochs \n",
    "        Epochs to find bad channels per trial. \n",
    "    :return: drop_log: tuple of n_trials length\n",
    "        Tuple representing bad channels names per trial.\n",
    "    \"\"\"\n",
    "    epochs_copy = epochs.copy()\n",
    "    \n",
    "    # channels with a voltage difference of 100 μV through the duration of the epoch\n",
    "    reject_criteria = dict(eeg=100e-6)\n",
    "    # flat channels (less than 1 µV of peak-to-peak difference)\n",
    "    flat_criteria = dict(eeg=1e-6)\n",
    "    \n",
    "    epochs_copy.drop_bad(reject=reject_criteria, flat=flat_criteria)\n",
    "    drop_log = epochs_copy.drop_log\n",
    "\n",
    "    # channels with more than a 30 μV difference with the nearest six neighbors\n",
    "    for idx, _ in enumerate(epochs_copy):\n",
    "        epoch = epochs[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "        for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "            mean_channel_data = np.array(np.mean(epoch_data[0,ch_idx,:]))\n",
    "    \n",
    "            ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "                target_ch_name = ch_name,\n",
    "                epochs = epoch,\n",
    "                k=6\n",
    "            )\n",
    "    \n",
    "            mean_neighbors_data = np.array([np.mean(epoch_data[0, ch_neighbor_index, :]) \n",
    "                                            for ch_neighbor_index in ch_neighbors_indices])\n",
    "    \n",
    "            # # if channels has more than a 30 μV difference with the nearest six neighbors\n",
    "            if (abs(mean_neighbors_data - mean_channel_data) > 30e-6).all():\n",
    "                logger_preprocessing_info.info(f'Channel {ch_name} has more than a 30 μV difference with the nearest six neighbors at {idx} trail.\\n Mean channel data: {mean_channel_data}\\nMean neighbors data: {mean_neighbors_data}')\n",
    "    \n",
    "                new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx]\n",
    "                drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))\n",
    "\n",
    "    del epochs_copy\n",
    "    \n",
    "    return drop_log\n",
    "\n",
    "def calculate_percentage(tuple_of_tuples, element):\n",
    "    total_tuples = len(tuple_of_tuples)\n",
    "    # Avoid division by zero\n",
    "    if total_tuples == 0:\n",
    "        return 0 \n",
    "\n",
    "    tuples_with_element = sum(1 for inner_tuple in tuple_of_tuples if element in inner_tuple)\n",
    "    percentage = (tuples_with_element / total_tuples)\n",
    "    return percentage\n",
    "\n",
    "def find_global_bad_channels(epochs, drop_log):\n",
    "    '''\n",
    "    (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4;\n",
    "    (2) Channels that were marked as bad for more than 20% of epochs;\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :return: \n",
    "    '''\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True)\n",
    "    concatenated_epochs_data = np.concatenate(epochs_data, axis=1)\n",
    "    \n",
    "    global_bad_channels_drop_log = {}\n",
    "    \n",
    "    # (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4\n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        channel_data = concatenated_epochs_data[ch_idx]\n",
    "    \n",
    "        ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "            target_ch_name = ch_name,\n",
    "            epochs = epochs_copy,\n",
    "            k=6\n",
    "        )\n",
    "        channel_neighbors_data = np.array([concatenated_epochs_data[ch_neighbor_index]\n",
    "                                           for ch_neighbor_index in ch_neighbors_indices])\n",
    "        channels_corr = np.tril(np.corrcoef(channel_neighbors_data, channel_data), k=-1)\n",
    "    \n",
    "        if (abs(channels_corr[-1][:-1]) < .4).all():\n",
    "            logger_preprocessing_info.info(f'Channel {ch_name} has < 0.4 abs corr with six nearest neighbors. Set as globally BAD. Channel corrs with neighbors: {channels_corr[-1][:-1]}')\n",
    "    \n",
    "            # mark channel as globally bad\n",
    "            global_bad_channels_drop_log[ch_name] = ['LOW CORR NEIGH']\n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "    \n",
    "    # (2) Channels that were marked as bad for more than 20% of epochs   \n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        percentage = calculate_percentage(drop_log, ch_name)\n",
    "    \n",
    "        if percentage > 0.2:\n",
    "            logger_preprocessing_info.info(f'Channel {ch_name} is bad for {percentage} percent of epochs. Set as globally BAD.')\n",
    "    \n",
    "            if ch_name in global_bad_channels_drop_log:\n",
    "                global_bad_channels_drop_log[ch_name].append('BAD FOR MORE THAN 20%')\n",
    "            else:\n",
    "                global_bad_channels_drop_log[ch_name] = ['BAD FOR MORE THAN 20%']\n",
    "    \n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "            \n",
    "    del epochs_copy\n",
    "            \n",
    "    return drop_log, global_bad_channels_drop_log\n",
    "\n",
    "def mark_bad_trials(epochs, drop_log, threshold=0.1):\n",
    "    \"\"\"\n",
    "    If more than 10% of channels were marked bad for an epoch (trial), the entire epoch was rejected\n",
    "    :param drop_log: \n",
    "    :return: trials_to_drop: ndarray\n",
    "    '\"\"\"\n",
    "    trials_to_drop_indices = []\n",
    "    epochs_copy = epochs.copy()\n",
    "\n",
    "    assert len(epochs_copy) == len(drop_log), f'Length of epochs ({len(epochs_copy)}) not equals length of drop_log ({len(drop_log)}). Cannot mark trials as BAD.'\n",
    "    threshold_items = int(threshold * len(epochs_copy.info['ch_names']))\n",
    "    for idx, item in enumerate(drop_log):\n",
    "        if len(item) > threshold_items:\n",
    "            logger_preprocessing_info.info(f'More than 10% of channels are bad for trial {idx}. Trial set as \\'TO DROP\\'')\n",
    "            trials_to_drop_indices.append(idx)\n",
    "    \n",
    "    # update drop_log\n",
    "    for trial_idx in trials_to_drop_indices:\n",
    "        drop_log = tuple(drop_log[i] + ('TO DROP',) if i == trial_idx else item for i, item in enumerate(drop_log))\n",
    "    \n",
    "    return trials_to_drop_indices, drop_log\n",
    "\n",
    "\n",
    "def reject_bad_trials(epochs, drop_log, trials_to_drop_indices=None):\n",
    "    clean_epochs = epochs.copy()\n",
    "    \n",
    "    clean_epochs = clean_epochs.drop(\n",
    "        indices = trials_to_drop_indices,\n",
    "        reason = 'MORE THAN 10% CHANNELS MARKED AS BAD',\n",
    "    )\n",
    "    \n",
    "    # update drop_log\n",
    "    for trial_idx in trials_to_drop_indices:\n",
    "        drop_log = tuple(('REJECTED',) if i == trial_idx else element for i, element in enumerate(drop_log))\n",
    "    \n",
    "    return clean_epochs, drop_log\n",
    "\n",
    "def interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log):\n",
    "    \"\"\"\n",
    "    Bad channels were interpolated using spherical splines\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :param global_bad_channels_drop_log: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_copy.info['bads'] = list(global_bad_channels_drop_log.keys())\n",
    "    epochs_interpolated_bad_channels = epochs_copy.interpolate_bads(method='spline')\n",
    "\n",
    "    # update drop log to remove interpolated channels\n",
    "    updated_drop_log = drop_log\n",
    "    for ch_name in list(global_bad_channels_drop_log.keys()):\n",
    "        updated_drop_log = tuple(tuple(element for element in drop_log_item if element != ch_name) for drop_log_item in updated_drop_log)\n",
    "    \n",
    "    return epochs_interpolated_bad_channels, updated_drop_log\n",
    "\n",
    "def create_erps_waves(epochs, drop_log, new_response_event_dict, type = 'error_response', tmin=0, tmax=0.1, picks=['FCz']):\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True, picks=picks, tmin=tmin, tmax=tmax)\n",
    "    events = epochs_copy.events\n",
    "\n",
    "    eeg_data = []\n",
    "\n",
    "    if (len(epochs_data) == len(drop_log)) & (len(events) == len(drop_log)):\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if events[idx][-1] == new_response_event_dict[type]:\n",
    "                if ('TO DROP' in item) or any(element in item for element in picks):\n",
    "                    pass\n",
    "                else:\n",
    "                    eeg_data.append(epochs_data[idx])\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(eeg_data)\n",
    "    else:\n",
    "        logger_preprocessing_info.info(f'Epochs length is not equal to drop_log length:\\nepochs: {len(epochs_data)}\\ndrop_log: {len(drop_log)}')\n",
    "        return None\n",
    "\n",
    "def create_erps(epochs, drop_log, new_response_event_dict, type = 'error_response', tmin=0, tmax=0.1, picks=['FCz']):\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True, picks=picks, tmin=tmin, tmax=tmax)\n",
    "    events = epochs_copy.events\n",
    "\n",
    "    erps_data = []\n",
    "\n",
    "    if (len(epochs_data) == len(drop_log)) & (len(events) == len(drop_log)):\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if events[idx][-1] == new_response_event_dict[type]:\n",
    "                if ('TO DROP' in item) or any(element in item for element in picks):\n",
    "                    erps_data.append([None])\n",
    "                else:\n",
    "                    erps_data.append(np.mean(epochs_data[idx], axis=-1))\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(erps_data)\n",
    "    else:\n",
    "        logger_preprocessing_info.info(f'Epochs length is not equal to drop_log length:\\nepochs: {len(epochs_data)}\\ndrop_log: {len(drop_log)}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea2632f8b5d423",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_eeg(input_fname, participant_id, context, case='RE', trigger_fname=None):\n",
    "    # check if exists ICA cache\n",
    "    try:\n",
    "        raw_corrected_eogs = mne.io.read_raw_fif(f'{ica_cache_dir_path}{participant_id}_raw.fif.gz', preload=True)\n",
    "    except:\n",
    "        # 0. read bdf\n",
    "        raw = mne.io.read_raw_bdf(\n",
    "            input_fname,\n",
    "            eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "            exclude=['EXG5', 'EXG6'],\n",
    "            preload=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            raw = raw.set_montage('biosemi64')\n",
    "        except ValueError as e:\n",
    "            if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "                raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "                logger_preprocessing_info.info('On missing')\n",
    "            else:\n",
    "                logger_preprocessing_info.info('Lacks important channels!')\n",
    "\n",
    "        # 1. replace trigger names\n",
    "        trigger_map = read_trigger_map(trigger_fname)\n",
    "        raw_new_triggers = replace_trigger_names(raw, participant_id, trigger_map)\n",
    "\n",
    "        # 2. re-reference: to mastoids\n",
    "        raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "\n",
    "        # 3. 4-th order Butterworth filters\n",
    "        raw_filtered = raw_ref.copy().filter(\n",
    "            l_freq=.1,\n",
    "            h_freq=30.0,\n",
    "            n_jobs=10,\n",
    "            method='iir',\n",
    "            iir_params=None,\n",
    "        )\n",
    "\n",
    "        # 4. Notch filter at 50 Hz\n",
    "        raw_filtered = raw_filtered.notch_filter(\n",
    "            freqs=np.arange(50, (raw_filtered.info['sfreq'] / 2), 50),\n",
    "            n_jobs=10,\n",
    "        )\n",
    "\n",
    "        # 5. ocular correction with ICA\n",
    "        raw_corrected_eogs = ocular_correction_ica(\n",
    "            raw_filtered, \n",
    "            raw_ref, \n",
    "            heog=['EXG3', 'EXG4'], \n",
    "            veog=['EXG1', 'EXG2'], \n",
    "            from_template=True, \n",
    "            info=True,\n",
    "            context=context,\n",
    "        )\n",
    "\n",
    "        raw_corrected_eogs.save(f'{ica_cache_dir_path}{participant_id}_raw.fif.gz', overwrite=True)\n",
    "\n",
    "    # 6. segmentation -400 to 800 ms around the response\n",
    "    raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "    epochs = create_epochs(\n",
    "        raw_corrected_eogs_drop_ref,\n",
    "        context=context,\n",
    "        tmin=-.4,\n",
    "        tmax=.8,\n",
    "        reject=None,\n",
    "        reject_by_annotation=False,\n",
    "    )\n",
    "\n",
    "    # 7. Trial-wise Bad Channels Identification\n",
    "    drop_log = find_bad_trails(epochs)\n",
    "\n",
    "    # 8. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "    drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)\n",
    "\n",
    "    # 9. calculate trails to remove\n",
    "    trials_to_drop_indices, drop_log = mark_bad_trials(epochs, drop_log, threshold=0.1)\n",
    "\n",
    "    # 10. Interpolate bad channels (and thus update drop log)\n",
    "    interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)\n",
    "\n",
    "    # 11. Remove participants that have less then 6 trials\n",
    "    clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "    if len(clean_epochs) < 6:\n",
    "        logger_errors_info.info(f\"Participant ID: {participant_id} has not enough clean trials\")\n",
    "\n",
    "    # 13. Baseline correction\n",
    "    if case == 'RE':\n",
    "        interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)\n",
    "    elif case == 'STIM':\n",
    "        interpolated_epochs.apply_baseline(baseline=(-0.2, 0),)\n",
    "    else:\n",
    "        logger_errors_info.info('Not know case. Setting baseline from -0.2 to 0')\n",
    "        interpolated_epochs.apply_baseline(baseline=(-0.2, 0),)\n",
    "\n",
    "    return interpolated_epochs, drop_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2082a55bf73ab7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log_separately(epochs, drop_log, participant_id):\n",
    "    # save drop_log\n",
    "    with open(f'{preprocessed_data_dir_path}drop_log_{participant_id}.json', 'w') as fjson:\n",
    "        json.dump(drop_log, fjson)\n",
    "\n",
    "    # save Epoch object\n",
    "    epochs.save(f'{preprocessed_data_dir_path}preprocessed_{participant_id}-epo.fif', overwrite=True)\n",
    "\n",
    "    return logger_preprocessing_info.info('Epochs saved to fif. Drop log saved to json.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431faf1c3221a26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log(epochs, drop_log, participant_id):\n",
    "    item = pd.DataFrame({\n",
    "        'epochs': [epochs],\n",
    "        'drop_log': [drop_log],\n",
    "    })\n",
    "    \n",
    "    item.to_pickle(f'{preprocessed_data_dir_path}preprocessed_{participant_id}.pkl')\n",
    "    \n",
    "    return logger_preprocessing_info.info('Epochs saved to pickle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002388a62c3c418",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def read_behavioral_file(participant_id):\n",
    "    \n",
    "    behavioral_data_df = pd.read_csv(f'{behavioral_dir_path}beh_{participant_id}.csv')\n",
    "\n",
    "    trial_numerator = 1\n",
    "    trial_numbers = []\n",
    "    for i in range(0, len(behavioral_data_df)):\n",
    "        # if behavioral_data_df.iloc[i]['block_type'] != 'experiment':\n",
    "        #     trial_numbers.append(0)\n",
    "        # else:\n",
    "        trial_numbers.append(trial_numerator)\n",
    "        trial_numerator+=1\n",
    "    \n",
    "    behavioral_data_df['trial number'] = trial_numbers\n",
    "    return behavioral_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e0c957e9e500d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def save_epochs_with_behavioral_data_long(epochs, drop_log, participant_id, case='RE'):\n",
    "    \n",
    "    if paradigm == 'GNG':\n",
    "    \n",
    "        # read behavioral file\n",
    "        behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "        beh_data_uninhibited_nogo_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial type'] != 'go') &\n",
    "            (behavioral_data_df['reaction'] == False)\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of uninhibited NOGO trials: {len(beh_data_uninhibited_nogo_responses_df)}')\n",
    "\n",
    "        beh_data_inhibited_nogo_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial type'] != 'go') &\n",
    "            (behavioral_data_df['reaction'] == True)\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of inhibited NOGO trials: {len(beh_data_inhibited_nogo_responses_df)}')\n",
    "\n",
    "        beh_data_correct_go_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial type'] == 'go') &\n",
    "            (behavioral_data_df['response'] == 'num_separator')\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "        results_df = pd.DataFrame()\n",
    "        epochs_df = pd.DataFrame()\n",
    "        behavioral_df = pd.DataFrame()\n",
    "\n",
    "        if case == 'RE':\n",
    "            behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        elif case == 'STIM':\n",
    "            behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df, beh_data_inhibited_nogo_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "        assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "        results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{participant_id}.pkl')\n",
    "        logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "    \n",
    "    if paradigm == 'FLA':\n",
    "        # read behavioral file\n",
    "        print('in saving')\n",
    "        behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "        beh_data_incorrect_incongruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'incongruent') &\n",
    "            (behavioral_data_df['reaction'] == 'incorrect') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of incorrect incongruent trials: {len(beh_data_incorrect_incongruent_responses_df)}')\n",
    "\n",
    "        beh_data_correct_incongruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'incongruent') &\n",
    "            (behavioral_data_df['reaction'] == 'correct') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of correct incongruent trials: {len(beh_data_correct_incongruent_responses_df)}')\n",
    "        \n",
    "        beh_data_incorrect_congruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'congruent') &\n",
    "            (behavioral_data_df['reaction'] == 'incorrect') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number incorrect congruent trials: {len(beh_data_incorrect_congruent_responses_df)}')\n",
    "\n",
    "        beh_data_correct_congruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'congruent') &\n",
    "            (behavioral_data_df['reaction'] == 'correct') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number correct congruent trials: {len(beh_data_correct_congruent_responses_df)}')\n",
    "\n",
    "        results_df = pd.DataFrame()\n",
    "        epochs_df = pd.DataFrame()\n",
    "        behavioral_df = pd.DataFrame()\n",
    "        \n",
    "        if case == 'RE':\n",
    "            behavioral_df = pd.concat([beh_data_incorrect_incongruent_responses_df, beh_data_correct_incongruent_responses_df, beh_data_incorrect_congruent_responses_df, beh_data_correct_congruent_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        elif case == 'STIM':\n",
    "            behavioral_df = pd.concat([beh_data_incorrect_incongruent_responses_df, beh_data_correct_incongruent_responses_df, beh_data_incorrect_congruent_responses_df, beh_data_correct_congruent_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "        assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "        results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{participant_id}.pkl')\n",
    "        logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "     \n",
    "    if paradigm == 'SST':\n",
    "        # read behavioral file\n",
    "        behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "        beh_data_inhibited_stop_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'] == 0) &\n",
    "            (behavioral_data_df['RE_time'].isna())\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of correctly inhibited STOP trials: {len(beh_data_inhibited_stop_df)}')\n",
    "\n",
    "        beh_data_uninhibited_stop_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'] == 0) &\n",
    "            (behavioral_data_df['RE_time'].notna())\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of incorrectly uninhibited STOP trials: {len(beh_data_uninhibited_stop_df)}')\n",
    "\n",
    "        beh_data_correct_go_responses_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'].isna()) &\n",
    "            # (behavioral_data_df['STOP_TYPE'] != 1) &\n",
    "            (behavioral_data_df['RE_key'] == behavioral_data_df['RE_true'])\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "        beh_data_incorrect_go_responses_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'] != 0) &\n",
    "            (behavioral_data_df['STOP_TYPE'] != 1) &\n",
    "            (behavioral_data_df['RE_key'] != behavioral_data_df['RE_true']) &\n",
    "            (behavioral_data_df['RE_key'].notna())\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number incorrect GO trials: {len(beh_data_incorrect_go_responses_df)}')\n",
    "\n",
    "        results_df = pd.DataFrame()\n",
    "        epochs_df = pd.DataFrame()\n",
    "        behavioral_df = pd.DataFrame()\n",
    "        \n",
    "        if case == 'RE':\n",
    "            behavioral_df = pd.concat([beh_data_uninhibited_stop_df, beh_data_incorrect_go_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        elif case == 'STIM':\n",
    "            behavioral_df = pd.concat([beh_data_inhibited_stop_df, beh_data_uninhibited_stop_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "        assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "        results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{participant_id}.pkl')\n",
    "        logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "        \n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a429187fd12024",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57fece67b0970c",
   "metadata": {},
   "source": [
    "Set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1085cc81721c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GNG | SST | Flanker\n",
    "paradigm = 'SST'\n",
    "# RE | STIM | FBCK\n",
    "case = 'STIM'\n",
    "# todo think whether move global vars as paradigm and case info some kind of data/case class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821407c002c73997",
   "metadata": {},
   "source": [
    "Set paths base on globals values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7845cbc91ba0164",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trigger_dir_path = f'data/{paradigm}/raw/triggers/'\n",
    "bdf_dir_path = f'data/{paradigm}/raw/bdfs/'\n",
    "behavioral_dir_path = f'data/{paradigm}/behavioral/'\n",
    "preprocessed_data_dir_path = f'data/{paradigm}/preprocessed/{case}/'\n",
    "logger_dir_path = f'data/{paradigm}/'\n",
    "ica_cache_dir_path = f'data/{paradigm}/ica_cache/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cca2995cdf5cd1",
   "metadata": {},
   "source": [
    "Set output files for loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e5d98842985c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a file handler for preprocessing and set the level to INFO\n",
    "file_handler_preprocessing = logging.FileHandler(f'data/{paradigm}/{case}_preprocessing.txt')\n",
    "file_handler_preprocessing.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for preprocessing\n",
    "formatter_preprocessing = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_preprocessing.setFormatter(formatter_preprocessing)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_preprocessing_info.addHandler(file_handler_preprocessing)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a file handler for errors and set the level to INFO\n",
    "file_handler_errors = logging.FileHandler(f'data/{paradigm}/{case}_errors.txt')\n",
    "file_handler_errors.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for errors\n",
    "formatter_errors = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_errors.setFormatter(formatter_errors)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_errors_info.addHandler(file_handler_errors)\n",
    "\n",
    "##### MNE ###################################################\n",
    "# Create logger for MNE logs\n",
    "logger_f_name = f'data/{paradigm}/{case}_MNE-logs.txt'\n",
    "set_log_file(fname=logger_f_name, output_format=\"%(asctime)s - %(message)s\", overwrite=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f41004d3f9f42f",
   "metadata": {},
   "source": [
    "Read participant IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8dba67e70f4fb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id_list = [item.split('.')[0] for item in os.listdir(bdf_dir_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16216d1b5baba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for participant_id in id_list:\n",
    "    print(f'{participant_id}\\n')\n",
    "    bdf_fname = f'{bdf_dir_path}{participant_id}.bdf'\n",
    "    trigger_fname = f'{trigger_dir_path}triggerMap_{participant_id}.txt'\n",
    "\n",
    "    logger_preprocessing_info.info(f'#### PARTICIPANT ID: {participant_id} #########')\n",
    "    logger_errors_info.info(f'#### PARTICIPANT ID: {participant_id} #########')\n",
    "\n",
    "    try:\n",
    "        trigger_map = read_trigger_map(trigger_fname)\n",
    "        participant_context = create_events_mappings(trigger_map, case=case)\n",
    "        logger_preprocessing_info.info(f'Context: {participant_context}')\n",
    "        \n",
    "        epochs_preprocessed, drop_log = pre_process_eeg(\n",
    "            input_fname=bdf_fname,\n",
    "            participant_id = participant_id,\n",
    "            context=participant_context,\n",
    "            trigger_fname=trigger_fname,\n",
    "            case=case,\n",
    "        )\n",
    "    \n",
    "        _ = save_epochs_with_behavioral_data_long(\n",
    "            epochs_preprocessed,\n",
    "            drop_log,\n",
    "            participant_id,\n",
    "            case=case,\n",
    "        )\n",
    "    except Exception as e:        \n",
    "        logger_errors_info.info(f\"{e}\")\n",
    "    \n",
    "    logger_preprocessing_info.info(f'\\n')\n",
    "    logger_errors_info.info(f'\\n')\n",
    "\n",
    "print(f'##########\\n DONE\\n')   \n",
    "# Restore MNE logging to std out     \n",
    "set_log_file(fname=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e07170-6dcb-41b8-a596-9757b7b670c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42836b-9880-4f18-a350-59a0603e5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "beh_data_inhibited_stop_df = behavioral_data_df.iloc[30:][\n",
    "    (behavioral_data_df['STOP_TYPE'] == 0) &\n",
    "    (behavioral_data_df['RE_time'].isna())\n",
    "    ]\n",
    "logger_preprocessing_info.info(f'Number of correctly inhibited STOP trials: {len(beh_data_inhibited_stop_df)}')\n",
    "\n",
    "beh_data_uninhibited_stop_df = behavioral_data_df.iloc[30:][\n",
    "    (behavioral_data_df['STOP_TYPE'] == 0) &\n",
    "    (behavioral_data_df['RE_time'].notna())\n",
    "    ]\n",
    "logger_preprocessing_info.info(f'Number of incorrectly uninhibited STOP trials: {len(beh_data_uninhibited_stop_df)}')\n",
    "\n",
    "beh_data_correct_go_responses_df = behavioral_data_df.iloc[30:][\n",
    "    (behavioral_data_df['STOP_TYPE'].isna()) &\n",
    "    # (behavioral_data_df['STOP_TYPE'] != 1) &\n",
    "    (behavioral_data_df['RE_key'] == behavioral_data_df['RE_true'])\n",
    "    ]\n",
    "logger_preprocessing_info.info(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "beh_data_incorrect_go_responses_df = behavioral_data_df.iloc[30:][\n",
    "    (behavioral_data_df['STOP_TYPE'] != 0) &\n",
    "    (behavioral_data_df['STOP_TYPE'] != 1) &\n",
    "    (behavioral_data_df['RE_key'] != behavioral_data_df['RE_true']) &\n",
    "    (behavioral_data_df['RE_key'].notna())\n",
    "    ]\n",
    "logger_preprocessing_info.info(f'Number incorrect GO trials: {len(beh_data_incorrect_go_responses_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd915ad-c72a-41e9-aa55-badcd627a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beh_data_incorrect_go_responses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f2151-50e4-43ee-9ca9-de53e226a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=epochs_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474bdec-779d-4151-b83e-da0ef8c40212",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "epochs_df = pd.DataFrame()\n",
    "behavioral_df = pd.DataFrame()\n",
    "\n",
    "if case == 'RE':\n",
    "    behavioral_df = pd.concat([beh_data_uninhibited_stop_df, beh_data_incorrect_go_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "    logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "    logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "    # assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "    for idx, _ in enumerate(epochs):\n",
    "        epoch = epochs[idx]\n",
    "        epoch_type = list(epoch.event_id.keys())\n",
    "        assert len(epoch_type) == 1, \\\n",
    "            f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "        drop_log_item = drop_log[idx]\n",
    "\n",
    "        this_df = pd.DataFrame({\n",
    "            'epoch': [epoch],\n",
    "            'event': epoch_type,\n",
    "            'drop_log': [drop_log_item],\n",
    "        })\n",
    "\n",
    "        epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "    # Set the indexes of epochs to match reactions\n",
    "    indexes = behavioral_df.index\n",
    "    epochs_df.set_index(indexes, inplace=True)\n",
    "    results_df = pd.concat([behavioral_df, epochs_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b9be28-bb69-4229-be62-c6bd1c2d0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1612cf-d6bd-4ffd-8ade-7c7407faff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210739182f0bf7e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4627c-804f-4a3b-bb4a-999098b13c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_trigger_names2(raw, participant_id, trigger_map, new_response_event_dict=None, replace=False, search='RE'):\n",
    "    # Replace event IDs in the Raw object\n",
    "    events = mne.find_events(raw, stim_channel='Status')\n",
    "    new_events_list = events.copy()\n",
    "    \n",
    "    # add trigger to corrupted bdf files - too short reaction \n",
    "    if paradigm == 'GNG' and (participant_id == 'B-GNG-199' or participant_id == 'B-GNG-208'):\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "    \n",
    "    ids = ['SST-165', 'SST-211', 'SST-122', 'SST-088','SST-045','SST-012','SST-083','SST-136','SST-125']\n",
    "    if paradigm == 'SST' and participant_id in ids:\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "                \n",
    "    if paradigm == 'SST' and participant_id == 'SST-130':\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "        \n",
    "        for idx, event in enumerate(new_events_list):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([new_events_list[:idx, :], [[new_events_list[idx-1][0] + delta_time, 0, 65281]], new_events_list[idx:, :]]) \n",
    "                break\n",
    "                \n",
    "    \n",
    "    # delete ghost event 65312\n",
    "    if paradigm == 'SST' and (participant_id == 'SST-181'):\n",
    "        for idx, event in enumerate(events):\n",
    "            if event[2] == 65312:\n",
    "                new_events_list = np.concatenate([events[:idx, :], events[idx+1:, :]]) \n",
    "                break  \n",
    "    \n",
    "    # delete ghost event 0: 130816\n",
    "    if paradigm == 'SST' and (participant_id == 'SST-075'):\n",
    "        for idx, event in enumerate(events):\n",
    "            if event[2] == 130816:\n",
    "                new_events_list = np.concatenate([events[:idx, :], events[idx+1:, :]]) \n",
    "                break             \n",
    "                \n",
    "        \n",
    "    logger_preprocessing_info.info(f'EVENTS: {new_events_list}')\n",
    "\n",
    "    # assert len(new_events_list) == len(trigger_map), \\\n",
    "    #         f'The length of trigger map ({len(trigger_map)}) not equals length of events in eeg recording ({len(new_events_list)})'\n",
    "\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    for idx, event in enumerate(new_events_list):\n",
    "        event_id = str(event[2])[-1]\n",
    "        trigger_id = trigger_map[idx][0]\n",
    "        trigger_new_code = trigger_map[idx][1]\n",
    "        \n",
    "        print(f'{event[2]} {trigger_id}')\n",
    "        \n",
    "        if event_id != trigger_id:\n",
    "            logger_errors_info.info(f'An event {idx} has different number than in provided file. {trigger_id} expected, {str(event[2])} found. Triggers may need to be checked.')\n",
    "\n",
    "        trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "        new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "    annot_from_events = mne.annotations_from_events(\n",
    "        events=new_events_list,\n",
    "        event_desc=mapping,\n",
    "        sfreq=raw.info[\"sfreq\"],\n",
    "        orig_time=raw.info[\"meas_date\"],\n",
    "    )\n",
    "    raw_copy = raw.copy()\n",
    "    raw_copy.set_annotations(annot_from_events)\n",
    "\n",
    "    return raw_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a7bd0-a126-4088-a7db-c7649fa65665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST-165\n",
    "# SST-211\n",
    "# SST-122\n",
    "# SST-088\n",
    "# SST-045\n",
    "# SST-012\n",
    "# SST-024 (The length of trigger map (816) not equals length of events in eeg recording (776))\n",
    "# SST-083\n",
    "# SST-136\n",
    "# SST-130 (The length of trigger map (848) not equals length of events in eeg recording (846))\n",
    "# SST-125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2015d4a86f612fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id_ = 'SST-130'\n",
    "paradigm= 'SST'\n",
    "case = 'RE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed9f14-fef1-419e-b5aa-f01eec01f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_dir_path = f'data/{paradigm}/raw/triggers/'\n",
    "bdf_dir_path = f'data/{paradigm}/raw/bdfs/'\n",
    "behavioral_dir_path = f'data/{paradigm}/behavioral/'\n",
    "# preprocessed_data_dir_path = f'data/{paradigm}/preprocessed/{case}/'\n",
    "logger_dir_path = f'data/{paradigm}/'\n",
    "# ica_cache_dir_path = f'data/{paradigm}/ica_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdd5d60e11594c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)  # Set the desired logging level\n",
    "\n",
    "logger_errors_info.addHandler(console_handler)\n",
    "logger_preprocessing_info.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c545bf3cad5b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bdf_fname = f'{bdf_dir_path}{id_}.bdf' \n",
    "trigger_fname = f'{trigger_dir_path}triggerMap_{id_}.txt'\n",
    "logger_f_name = f'{logger_dir_path}{case}_logs.txt'\n",
    "\n",
    "# 0. read bdf\n",
    "raw = mne.io.read_raw_bdf(\n",
    "    bdf_fname,\n",
    "    eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "    exclude=['EXG5', 'EXG6'],\n",
    "    preload=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    raw = raw.set_montage('biosemi64')\n",
    "except ValueError as e:\n",
    "    if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "        raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "        logger_preprocessing_info.info('On missing')\n",
    "    else:\n",
    "        logger_preprocessing_info.info('Lacks important channels!')\n",
    "\n",
    "# 1. replace trigger names\n",
    "trigger_map = read_trigger_map(trigger_fname)\n",
    "raw_new_triggers = replace_trigger_names2(raw, id_, trigger_map)\n",
    "participant_context = create_events_mappings(trigger_map, case=case)\n",
    "\n",
    "# 2. re-reference: to mastoids\n",
    "# raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "\n",
    "# # 3. 4-th order Butterworth filters\n",
    "# raw_filtered = raw_ref.copy().filter(\n",
    "#     l_freq=.1,\n",
    "#     h_freq=30.0,\n",
    "#     n_jobs=10,\n",
    "#     method='iir',\n",
    "#     iir_params=None,\n",
    "# )\n",
    "\n",
    "# # 4. Notch filter at 50 Hz\n",
    "# raw_filtered = raw_filtered.notch_filter(\n",
    "#     freqs=np.arange(50, (raw_filtered.info['sfreq'] / 2), 50),\n",
    "#     n_jobs=10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5048f85-2b66-439d-814f-42afdbe43a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "heog=['EXG3', 'EXG4']\n",
    "veog=['EXG1', 'EXG2'] \n",
    "\n",
    "ica_epochs = create_epochs(\n",
    "    filtered_raw_ica,\n",
    "    participant_context,\n",
    ")\n",
    "\n",
    "ica = mne.preprocessing.ICA(\n",
    "    n_components=15,\n",
    "    method='infomax',\n",
    "    max_iter=\"auto\",\n",
    "    random_state=random_state\n",
    ")\n",
    "# ica.fit(filtered_raw_ica)\n",
    "ica.fit(filtered_raw_ica)\n",
    "\n",
    "ica.exclude = []\n",
    "\n",
    "# find which ICs match the VEOG pattern\n",
    "veog_indices_eogs, veog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=veog,\n",
    "    threshold=0.9,\n",
    "    measure='correlation'\n",
    ")\n",
    "\n",
    "# find which ICs match the HEOG pattern\n",
    "heog_indices_eogs, heog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=heog,\n",
    "    threshold=0.6,\n",
    "    measure='correlation'\n",
    ")\n",
    "\n",
    "print(f'EOG based excluded ICA components:\\nVEOG: {veog_indices_eogs}\\nHEOG: {heog_indices_eogs}')\n",
    "\n",
    "\n",
    "heog_indices = []\n",
    "veog_indices = []\n",
    "\n",
    "print('ICA components')\n",
    "fig = ica.plot_components()\n",
    "\n",
    "templates_ica = pd.read_pickle('public_data/eog_templates.pkl')\n",
    "\n",
    "template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "mne.preprocessing.corrmap(\n",
    "    [ica], \n",
    "    template=template_veog_component,\n",
    "    threshold=0.9,\n",
    "    label=\"veog blink\", \n",
    "    plot=False\n",
    ")\n",
    "\n",
    "mne.preprocessing.corrmap(\n",
    "    [ica], \n",
    "    template=template_heog_component, \n",
    "    label=\"heog blink\", \n",
    "    threshold=0.8,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "veog_indices = ica.labels_['veog blink']\n",
    "heog_indices = ica.labels_['heog blink']\n",
    "\n",
    "print(veog_indices)\n",
    "print(heog_indices)\n",
    "\n",
    "\n",
    "# if (len(veog_indices_eogs) == 1) and (len(veog_indices) != 1) and (veog_indices_eogs[0] in veog_indices):\n",
    "#     veog_indices = veog_indices_eogs\n",
    "# if (len(heog_indices_eogs) == 1) and (len(heog_indices) != 1) and (heog_indices_eogs[0] in heog_indices):\n",
    "#     heog_indices = heog_indices_eogs\n",
    "# logger_errors_info.info(f'Too much patterns found. Comparing to EOG channels')\n",
    "\n",
    "\n",
    "# logger_preprocessing_info.info(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "\n",
    "# assert len((veog_indices + heog_indices)) == 2, f'Number of ICA components to exclude ({len(veog_indices + heog_indices)}) is different than 2' \n",
    "\n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "\n",
    "# reconstructed_raw = raw.copy()\n",
    "# ica.apply(reconstructed_raw)\n",
    "\n",
    "# del filtered_raw_ica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f66e36-ebd6-4a39-a7f7-bdc85b1f02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_corrected_eogs.save(f'{ica_cache_dir_path}{id}_raw.fif.gz', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fa26d-ec61-4005-ad20-b0ed66d8314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. segmentation -400 to 800 ms around the response\n",
    "raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "epochs = create_epochs(\n",
    "    raw_corrected_eogs_drop_ref,\n",
    "    context=context,\n",
    "    tmin=-.4,\n",
    "    tmax=.8,\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    ")\n",
    "\n",
    "# 7. Trial-wise Bad Channels Identification\n",
    "drop_log = find_bad_trails(epochs)\n",
    "\n",
    "# 8. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)\n",
    "\n",
    "# 9. calculate trails to remove\n",
    "trials_to_drop_indices, drop_log = mark_bad_trials(epochs, drop_log, threshold=0.1)\n",
    "\n",
    "# 10. Interpolate bad channels (and thus update drop log)\n",
    "interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)\n",
    "\n",
    "# 11. Remove participants that have less then 6 trials\n",
    "clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "if len(clean_epochs) < 6:\n",
    "    logger_errors_info.info(f\"Participant ID: {id} has not enough clean trials\")\n",
    "\n",
    "# 13. Baseline correction\n",
    "if case == 'RE':\n",
    "    interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)\n",
    "elif case == 'STIM':\n",
    "    interpolated_epochs.apply_baseline(baseline=(-0.2, 0),)\n",
    "else:\n",
    "    logger_errors_info.info('Not know case. Setting baseline from -0.2 to 0')\n",
    "    interpolated_epochs.apply_baseline(baseline=(-0.2, 0),)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450576de07e784e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6ad8c6e166a8a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41121cf44e8b472a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id = 'B-GNG-102'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65258cf1cbc6fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs_df = pd.read_pickle(f'{preprocessed_data_dir_path}preprocessed_{id}.pkl')\n",
    "epochs = epochs_df['epochs'].to_numpy().flatten()[0]\n",
    "drop_log = epochs_df['drop_log'].to_numpy().flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bd4a3b107d0b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "error_wave = create_erps_waves(\n",
    "    epochs, \n",
    "    drop_log, \n",
    "    type='uninhibited_response', \n",
    "    tmin=-0.1, \n",
    "    tmax=0.6, \n",
    "    picks=['FCz'],\n",
    "    new_response_event_dict = {\"inhibited_response\": 0, \"uninhibited_response\": 1}\n",
    ")\n",
    "correct_wave = create_erps_waves(\n",
    "    epochs, \n",
    "    drop_log, \n",
    "    type='inhibited_response', \n",
    "    tmin=-0.1, \n",
    "    tmax=0.6, \n",
    "    picks=['FCz'],\n",
    "    new_response_event_dict = {\"inhibited_response\": 0, \"uninhibited_response\": 1}\n",
    ")\n",
    "\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten())\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(correct_wave, axis=0).flatten())), np.mean(correct_wave, axis=0).flatten())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86a0d7c352570e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "error_wave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294fd3733c3d47e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ERPs scoring\n",
    "ern_single_trials = create_erps(epochs_preprocessed, drop_log, type='error_response', tmin=0, tmax=0.1, picks=['FCz'])\n",
    "pe_single_trials = create_erps(epochs_preprocessed, drop_log, type='error_response', tmin=0.2, tmax=0.4, picks=['Pz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb751664fc106984",
   "metadata": {},
   "source": [
    "---\n",
    "## For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0c51998a3561b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_fname = 'data/GNG/raw/bdfs/B-GNG-102.bdf'\n",
    "raw = mne.io.read_raw_bdf(\n",
    "    input_fname, \n",
    "    eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'], \n",
    "    exclude=['EXG5', 'EXG6'], \n",
    "    preload=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    raw = raw.set_montage('biosemi64')\n",
    "except ValueError as e:\n",
    "    if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "        raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "        print('On missing')\n",
    "    else:\n",
    "        print('Lacks important channels!')\n",
    "\n",
    "\n",
    "file_path = 'data/GNG/raw/triggers/triggerMap_B-GNG-102.txt'\n",
    "trigger_map = read_trigger_map(file_path)\n",
    "# raw_new_triggers = replace_trigger_names(raw, trigger_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f89394daba94e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trigger_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818baa87274afec9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'TG' in trigger_map[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9811c59714e3d3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def add_response_info(trigger_map):\n",
    "    new_trigger_map = trigger_map.copy()\n",
    "    for idx, trigger in enumerate(new_trigger_map):\n",
    "        if 'TG' in trigger[1]:\n",
    "            print('in target')\n",
    "            if 'RE' in new_trigger_map[idx+1][1]:\n",
    "                print('adding RE')\n",
    "                new_trigger = (trigger[0], trigger[1][:-1]+'R')\n",
    "                new_trigger_map[idx] = new_trigger\n",
    "                \n",
    "    return new_trigger_map   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c076b616a9552",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ne_tg_map = add_response_info(trigger_map)\n",
    "# ne_tg_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97dc24b632e9dc4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ne_tg_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f8e9cf4857fec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Replace event IDs in the Raw object\n",
    "events = mne.find_events(raw, stim_channel='Status')\n",
    "new_events_list = events.copy()\n",
    "print(f'EVENTS: {new_events_list}')\n",
    "\n",
    "assert len(events) == len(trigger_map), \\\n",
    "    f'The length of trigger map ({len(trigger_map)}) not equals length of events in eeg recording ({len(events)})'\n",
    "\n",
    "trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "for idx, event in enumerate(events):\n",
    "    event_id = str(event[2])[-1]\n",
    "    trigger_id = trigger_map[idx][0]\n",
    "    trigger_new_code = trigger_map[idx][1]\n",
    "    \n",
    "    print(event)\n",
    "    print(event_id)\n",
    "    print(trigger_id)\n",
    "# \n",
    "    if event_id != trigger_id:\n",
    "        print(f'An event {idx} has different number than in provided file. {trigger_id} expected, {str(event[2])} found. Triggers may need to be checked.')\n",
    "\n",
    "    trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "    new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "annot_from_events = mne.annotations_from_events(\n",
    "    events=new_events_list,\n",
    "    event_desc=mapping,\n",
    "    sfreq=raw.info[\"sfreq\"],\n",
    "    orig_time=raw.info[\"meas_date\"],\n",
    ")\n",
    "raw_copy = raw.copy()\n",
    "raw_copy.set_annotations(annot_from_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab16a51807ceb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(new_events_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe805bfa311b0c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814df4cfa3da18c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib.use('Qt5Agg')\n",
    "# plt.switch_backend('QtAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621e0923940b5c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = raw_copy.plot(start=1404, duration=15)\n",
    "# plt.pause(0.0001)\n",
    "# %matplotlib\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e25e7b089c301",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mne.viz.plot_raw(raw_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80136436d945d215",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. re-reference: to mastoids\n",
    "raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "# fig = raw_ref.plot(start=60, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc89381c09684f1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2. 4-th order Butterworth filters\n",
    "raw_filtered = raw_ref.copy().filter(\n",
    "    l_freq=.1,\n",
    "    h_freq=30.0,\n",
    "    n_jobs=10,\n",
    "    method='iir',\n",
    "    iir_params=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797466c92a2ed62f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 3. Notch filter at 50 Hz\n",
    "raw_filtered = raw_filtered.notch_filter(\n",
    "    freqs=np.arange(50, 251, 50),\n",
    "    n_jobs=10,\n",
    "    # method='iir',\n",
    "    # iir_params=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9034b1d9f83e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 5. ocular artifact correction with ICA\n",
    "# raw_corrected_eogs = ocular_correction_ica(raw_filtered, raw_ref, heog=['EXG3', 'EXG4'], veog=['EXG1', 'EXG2'])\n",
    "heog=['EXG3', 'EXG4']\n",
    "veog=['EXG1', 'EXG2']\n",
    "filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "ica = mne.preprocessing.ICA(\n",
    "    n_components=12,\n",
    "    method='infomax',\n",
    "    max_iter=\"auto\",\n",
    "    random_state=random_state\n",
    ")\n",
    "ica.fit(filtered_raw_ica)\n",
    "\n",
    "ica.exclude = []\n",
    "\n",
    "# find which ICs match the VEOG pattern\n",
    "veog_indices, veog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=veog,\n",
    "    threshold=0.9,\n",
    "    measure='correlation'\n",
    ")\n",
    "\n",
    "# find which ICs match the HEOG pattern\n",
    "heog_indices, heog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=heog,\n",
    "    threshold=0.7,\n",
    "    measure='correlation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e52e31553fb12",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('ICA components')\n",
    "fig = ica.plot_components()\n",
    "\n",
    "print(f\"VEOG indices: {veog_indices}\\nVEOG scores: {veog_scores}\\n\")\n",
    "print(f\"HEOG indices: {heog_indices}\\nHEOG scores: {heog_scores}\\n\")\n",
    "\n",
    "\n",
    "print(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "fig = ica.plot_components(veog_indices + heog_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9a8dc7e7e4174",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if len(veog_indices + heog_indices) == 0 and from_template == 'auto':\n",
    "# print('No ICA component correlates with EOG channels. Using templates...')\n",
    "# ica_template = pd.read_pickle('data/eog_templates.pkl')\n",
    "# veog_template = ica_template[['VEOG']].to_numpy()\n",
    "# heog_template = ica_template[['HEOG']].to_numpy()\n",
    "# \n",
    "# mne.preprocessing.corrmap([ica], template=veog_template, threshold=0.9)\n",
    "# mne.preprocessing.corrmap([ica], template=heog_template, threshold=0.8)\n",
    "\n",
    "\n",
    "# elif from_template is True:\n",
    "#     print('Using templates...')\n",
    "\n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw.copy()\n",
    "# ica.apply(reconstructed_raw)\n",
    "\n",
    "template_veog_component = ica.get_components()[:, veog_indices[0]]\n",
    "template_heog_component = ica.get_components()[:, heog_indices[0]]\n",
    "\n",
    "templates_ica = pd.DataFrame({\n",
    "    'VEOG': [template_veog_component],\n",
    "    'HEOG': [template_heog_component],\n",
    "})\n",
    "templates_ica.to_pickle('data/eog_templates.pkl')\n",
    "#########\n",
    "templates_ica = pd.read_pickle('data/eog_templates.pkl')\n",
    "\n",
    "template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "\n",
    "mne.preprocessing.corrmap([ica], template=template_veog_component, threshold=0.9, label=\"veog blink\", plot=False)\n",
    "mne.preprocessing.corrmap([ica], template=template_heog_component, threshold=0.8, label=\"heog blink\", plot=False)\n",
    "\n",
    "to_exclude = ica.labels_['veog blink'] + ica.labels_['heog blink']\n",
    "print(to_exclude)\n",
    "\n",
    "\n",
    "\n",
    "# del filtered_raw_ica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a762401f4206c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ica.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d5e9b55631419",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "epochs = create_epochs(\n",
    "    raw_corrected_eogs_drop_ref, \n",
    "    context=participant_context\n",
    "    tmin=-.4, \n",
    "    tmax=.8,\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    ")\n",
    "\n",
    "fig = epochs.plot(n_epochs=20, n_channels=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da372e30630abee1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 6. Trial-wise Bad Channels Identification\n",
    "drop_log = find_bad_trails(epochs) \n",
    "drop_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a08ff9538219b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 7. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29521e825f342e62",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 8. calculate trails to remove\n",
    "trials_to_drop_indices, drop_log = calculate_bad_trials(drop_log, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbd355474b393",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 10. Interpolate bad channels (and thus update drop log)\n",
    "interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c235a759bbd5c9b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 12. Remove participants that have less then 6 trials\n",
    "clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "if len(clean_epochs) < 6:\n",
    "      print(f\"Participant ID: {id} has not enough clean trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589b68b16f1edd5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 13. Baseline correction\n",
    "interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af257db7dda387d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "drop_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31ac2c89fe2836",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 13. ERPs scoring\n",
    "ern_single_trials = create_erps(interpolated_epochs, drop_log, type='error_response', tmin=0, tmax=0.1, picks=['FCz'])\n",
    "pe_single_trials = create_erps(interpolated_epochs, drop_log, type='error_response', tmin=0.2, tmax=0.4, picks=['Pz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923162a9bc466d5e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "error_wave = create_erps_waves(interpolated_epochs, drop_log, type='error_response', tmin=-0.1, tmax=0.6, picks=['FCz'])\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efeff532643e93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b4bb1037c5b3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "# \n",
    "# ica = mne.preprocessing.ICA(\n",
    "#     n_components=12,\n",
    "#     method='infomax',\n",
    "#     max_iter=\"auto\",\n",
    "#     random_state=random_state\n",
    "# )\n",
    "# ica.fit(filtered_raw_ica)\n",
    "# \n",
    "# ica.exclude = []\n",
    "# heog=['EXG3', 'EXG4']\n",
    "# veog=['EXG1', 'EXG2']\n",
    "# \n",
    "# # find which ICs match the VEOG pattern\n",
    "# veog_indices, veog_scores = ica.find_bads_eog(\n",
    "#     filtered_raw_ica,\n",
    "#     ch_name=veog,\n",
    "#     threshold=0.9,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# # \n",
    "# print(f'{veog_indices}')\n",
    "# \n",
    "# heog_indices, heog_scores = ica.find_bads_eog(\n",
    "#     filtered_raw_ica,\n",
    "#     ch_name=heog,\n",
    "#     threshold=0.7,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# print(f'{heog_indices}')\n",
    "# \n",
    "# \n",
    "# print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "# \n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw_filtered.copy()\n",
    "# ica.apply(reconstructed_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5b98ffe961431",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ica.exclude = []\n",
    "# heog=['EXG3', 'EXG4']\n",
    "# veog=['EXG1', 'EXG2']\n",
    "# \n",
    "# # find which ICs match the VEOG pattern\n",
    "# veog_indices, veog_scores = ica.find_bads_eog(\n",
    "#     raw_ref,\n",
    "#     ch_name=veog,\n",
    "#     threshold=0.9,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# # \n",
    "# print(f'{veog_indices}')\n",
    "# \n",
    "# heog_indices, heog_scores = ica.find_bads_eog(\n",
    "#     raw_ref,\n",
    "#     ch_name=heog,\n",
    "#     threshold=0.7,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# print(f'{heog_indices}')\n",
    "# \n",
    "#  \n",
    "# print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "# \n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw_filtered.copy()\n",
    "# ica.apply(reconstructed_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3f1c5698c88bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(heog_scores)\n",
    "print(veog_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinability",
   "language": "python",
   "name": "erpinability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
