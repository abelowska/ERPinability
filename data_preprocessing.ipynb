{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Replication of Clayson et al., 2023 Preprocessing scripts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcb8b16b3533c83c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.utils import set_log_file\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "GNG INFO: \n",
    "\n",
    "- B-GNG-005 - signal should be re-reffered only to the A1 (A2 is very noisy);\n",
    "\n",
    "--- checked ---\n",
    "- B-GNG-102 - instead (8), trigger on the position 328218 (1015 idx) is (15) -> it is very close to the previous event (7), so probably the signals added and biosemi registered 8+7 (but it works). Todo: check if wave looks similar as in bva preprocessed.\n",
    "\n",
    "- B-GNG-069 - instead (1), triger on the position 359803 (1168 idx) is (9) -> it is very close to the target (8), so probably the signals added and biosemi registered 8+1 (but it works). Todo: check if wave looks similar as in bva preprocessed.\n",
    "---\n",
    "Exclude:\n",
    "- B-GNG-086:\n",
    "    - 117 GO trials without reaction; \n",
    "    - no trials with good feedback (fast responses); \n",
    "    - 13 uninhibited NoGo trials;\n",
    "        \n",
    "- B-GNG-075: no EEG recording;\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98aafc23f7d0533a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb43fd47897e7af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_state = 42"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93c3ba90e198d2cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loggers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b318f7c90cebce3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a custom logger for preprocessing INFO\n",
    "logger_preprocessing_info = logging.getLogger('preprocessing_info')\n",
    "logger_preprocessing_info.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler for preprocessing and set the level to INFO\n",
    "file_handler_preprocessing = logging.FileHandler('data/preprocessing.txt')\n",
    "file_handler_preprocessing.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for preprocessing\n",
    "formatter_preprocessing = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_preprocessing.setFormatter(formatter_preprocessing)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_preprocessing_info.addHandler(file_handler_preprocessing)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a custom logger for errors\n",
    "logger_errors_info = logging.getLogger('errors')\n",
    "logger_errors_info.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler for errors and set the level to INFO\n",
    "file_handler_errors = logging.FileHandler('data/errors.txt')\n",
    "file_handler_errors.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for errors\n",
    "formatter_errors = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_errors.setFormatter(formatter_errors)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_errors_info.addHandler(file_handler_errors)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e2570b867ca739b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79b9bbbeafc9c8c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_response_info(trigger_map):\n",
    "    new_trigger_map = trigger_map.copy()\n",
    "    for idx, trigger in enumerate(new_trigger_map):\n",
    "        if 'TG' in trigger[1]:\n",
    "            if idx + 1 < len(new_trigger_map):\n",
    "                if 'RE' in new_trigger_map[idx+1][1]:\n",
    "                    new_trigger = (trigger[0], trigger[1][:-1]+'R')\n",
    "                    new_trigger_map[idx] = new_trigger\n",
    "\n",
    "    return new_trigger_map\n",
    "\n",
    "def add_feedback_info(trigger_map, behavioral_file):\n",
    "    return\n",
    "\n",
    "def read_trigger_map(file_name):\n",
    "    line_count = 0\n",
    "    trigger_map = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read each line and increment the counter\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "            trigger = (match.group(1), match.group(2), )\n",
    "            trigger_map.append(trigger)\n",
    "        except:\n",
    "            pass\n",
    "        while line:\n",
    "            line_count += 1\n",
    "            line = file.readline()\n",
    "            try:\n",
    "                match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "                trigger = (match.group(1), match.group(2), )\n",
    "                trigger_map.append(trigger)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    assert len(trigger_map) == line_count, \\\n",
    "        f'The length of trigger file ({line_count}) not equals length of created trigger_map ({len(trigger_map)})'\n",
    "    \n",
    "    trigger_map_response_info = add_response_info(trigger_map)\n",
    "\n",
    "    return trigger_map_response_info\n",
    "\n",
    "def create_triggers_dict(trigger_map):\n",
    "    triggers_codes = [item[1] for item in trigger_map]\n",
    "    # Create an ordered dictionary to maintain order and remove duplicates\n",
    "    unique_ordered_dict = OrderedDict.fromkeys(triggers_codes)\n",
    "    numbered_dict = {key: 1000 + number for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    reversed_numbered_dict = {1000 + number: key for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    return numbered_dict, reversed_numbered_dict\n",
    "\n",
    "def replace_trigger_names(raw, trigger_map, new_response_event_dict=None, replace=False, search='RE'):\n",
    "    # Replace event IDs in the Raw object\n",
    "    events = mne.find_events(raw, stim_channel='Status')\n",
    "    new_events_list = events.copy()\n",
    "    logger_preprocessing_info.info(f'EVENTS: {new_events_list}')\n",
    "\n",
    "    assert len(events) == len(trigger_map), \\\n",
    "        f'The length of trigger map ({len(trigger_map)}) not equals length of events in eeg recording ({len(events)})'\n",
    "\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    for idx, event in enumerate(events):\n",
    "        event_id = str(event[2])[-1]\n",
    "        trigger_id = trigger_map[idx][0]\n",
    "        trigger_new_code = trigger_map[idx][1]\n",
    "        \n",
    "        if event_id != trigger_id:\n",
    "            logger_errors_info.info(f'An event {idx} has different number than in provided file. {trigger_id} expected, {str(event[2])} found. Triggers may need to be checked.')\n",
    "\n",
    "        trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "        new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "    annot_from_events = mne.annotations_from_events(\n",
    "        events=new_events_list,\n",
    "        event_desc=mapping,\n",
    "        sfreq=raw.info[\"sfreq\"],\n",
    "        orig_time=raw.info[\"meas_date\"],\n",
    "    )\n",
    "    raw_copy = raw.copy()\n",
    "    raw_copy.set_annotations(annot_from_events)\n",
    "\n",
    "    return raw_copy\n",
    "\n",
    "def find_items_matching_regex(dictionary, regex_list):\n",
    "    matching_items = {}\n",
    "    for regex in regex_list:\n",
    "        pattern = re.compile(regex)\n",
    "        matching_items.update({key: value for key, value in dictionary.items() if pattern.match(key)})\n",
    "    return matching_items\n",
    "\n",
    "@dataclass\n",
    "class ParticipantTriggerMappingContext:\n",
    "    event_dict: dict\n",
    "    events_mapping: dict\n",
    "    new_event_dict: dict\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.event_dict}\\n{self.events_mapping}\\n{self.new_event_dict}\"\n",
    "\n",
    "def create_events_mappings(trigger_map, case='RE') -> ParticipantTriggerMappingContext:\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    if case == 'RE':\n",
    "        new_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "        events_mapping = {\n",
    "            'correct_response': [],\n",
    "            'error_response': [],\n",
    "        }\n",
    "        # find response events from experimental blocks\n",
    "        regex_pattern = [r'RE\\*ex\\*.*\\*num_separator']\n",
    "        event_dict = find_items_matching_regex(trigger_map_codes, regex_pattern)\n",
    "    \n",
    "        for event_id in event_dict.keys():\n",
    "            event_id_splitted = event_id.split('*')\n",
    "    \n",
    "            if (event_id_splitted[2][0] == event_id_splitted[3][0]) and (\n",
    "                    event_id_splitted[3][-1] == '1'):\n",
    "                events_mapping['correct_response'].append(event_dict[event_id])\n",
    "            else:\n",
    "                events_mapping['error_response'].append(event_dict[event_id])\n",
    "        \n",
    "    elif case == 'STIM':\n",
    "        new_event_dict = {\"inhibited_nogo\": 0, \"uninhibited_nogo\": 1, \"successful_go\": 2}\n",
    "        events_mapping = {\n",
    "            'inhibited_nogo': [],\n",
    "            'uninhibited_nogo': [],\n",
    "            'successful_go': [],\n",
    "        }\n",
    "        # find all NoGo stimuli events from experimental blocks\n",
    "        no_go_regex_pattern = [\n",
    "            r'TG\\*ex\\*1_n\\*1_c_2\\*.*',\n",
    "            r'TG\\*ex\\*1_n\\*2_c_1\\*.*',\n",
    "            r'TG\\*ex\\*2_n\\*1_c_1\\*.*',\n",
    "            r'TG\\*ex\\*2_n\\*2_c_2\\*.*',\n",
    "        ]\n",
    "        no_go_event_dict = find_items_matching_regex(trigger_map_codes, no_go_regex_pattern)\n",
    "\n",
    "        # find all succesfull Go stimuli events from experimental blocks\n",
    "        go_regex_pattern = [\n",
    "            r'TG\\*ex\\*1_n\\*1_c_1\\*R',\n",
    "            r'TG\\*ex\\*2_n\\*2_c_1\\*R',\n",
    "        ]\n",
    "        go_event_dict = find_items_matching_regex(trigger_map_codes, go_regex_pattern)\n",
    "\n",
    "        # separate trials with responses (uninhibited) from inhibited ones\n",
    "        for event_id in no_go_event_dict.keys():\n",
    "            if event_id[-1] == '-':\n",
    "                events_mapping['inhibited_nogo'].append(no_go_event_dict[event_id])\n",
    "            elif event_id[-1] == 'R':\n",
    "                events_mapping['uninhibited_nogo'].append(no_go_event_dict[event_id])\n",
    "\n",
    "        for event_id in go_event_dict.keys():\n",
    "            if event_id[-1] == 'R':\n",
    "                events_mapping['successful_go'].append(go_event_dict[event_id])\n",
    "                \n",
    "        event_dict = {**no_go_event_dict, **go_event_dict}\n",
    "    \n",
    "    elif case == 'FBCK':\n",
    "        logger_errors_info.info('Not implemented for feedback and stimuli')\n",
    "        event_dict = {}\n",
    "        events_mapping = {}\n",
    "        new_event_dict = {}\n",
    "    else:\n",
    "        logger_errors_info('Not known case. Possible cases: \\'RE\\' for response, \\'STIM\\` for stimuli, and \\`FBCK\\` for feedback-locked events extraction.')\n",
    "        # todo raise an Error\n",
    "        event_dict = {}\n",
    "        events_mapping = {}\n",
    "        new_event_dict = {}\n",
    "\n",
    "\n",
    "    return ParticipantTriggerMappingContext(event_dict=event_dict, \n",
    "                                            events_mapping=events_mapping,\n",
    "                                            new_event_dict=new_event_dict)\n",
    "\n",
    "def create_epochs(\n",
    "        raw,\n",
    "        context: ParticipantTriggerMappingContext,\n",
    "        tmin=-.1,\n",
    "        tmax=.6,\n",
    "        reject=None,\n",
    "        reject_by_annotation=False,\n",
    "):\n",
    "    # select specific events\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=context.event_dict)\n",
    "\n",
    "    # Merge different events of one kind\n",
    "    for mapping in context.events_mapping:\n",
    "        events = mne.merge_events(\n",
    "            events=events,\n",
    "            ids=context.events_mapping[mapping],\n",
    "            new_id=context.new_event_dict[mapping],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=events,\n",
    "        event_id=context.new_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=reject_by_annotation,\n",
    "        preload=True,\n",
    "        reject=reject,\n",
    "        picks=['eeg', 'eog'],\n",
    "    )\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "\n",
    "def ocular_correction_ica(raw, raw_unfiltered, veog=[], heog=[], info=False, from_template='auto'):\n",
    "    filtered_raw_ica = raw_unfiltered.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "    ica = mne.preprocessing.ICA(\n",
    "        n_components=12,\n",
    "        method='infomax',\n",
    "        max_iter=\"auto\",\n",
    "        random_state=random_state\n",
    "    )\n",
    "    ica.fit(filtered_raw_ica)\n",
    "    \n",
    "    ica.exclude = []\n",
    "    \n",
    "    # find which ICs match the VEOG pattern\n",
    "    veog_indices, veog_scores = ica.find_bads_eog(\n",
    "        filtered_raw_ica,\n",
    "        ch_name=veog,\n",
    "        threshold=0.9,\n",
    "        measure='correlation'\n",
    "    )\n",
    "\n",
    "    # find which ICs match the HEOG pattern\n",
    "    heog_indices, heog_scores = ica.find_bads_eog(\n",
    "        filtered_raw_ica,\n",
    "        ch_name=heog,\n",
    "        threshold=0.7,\n",
    "        measure='correlation'\n",
    "    )   \n",
    "    \n",
    "    if info:\n",
    "        logger_preprocessing_info.info('ICA components')\n",
    "        fig = ica.plot_components()\n",
    "        logger_preprocessing_info.info(f\"VEOG indices: {veog_indices}\\nVEOG scores: {veog_scores}\\n\")\n",
    "        logger_preprocessing_info.info(f\"HEOG indices: {heog_indices}\\nHEOG scores: {heog_scores}\\n\")\n",
    "  \n",
    "    if (len(veog_indices + heog_indices) == 0 and from_template == 'auto') or from_template == True:\n",
    "        logger_preprocessing_info.info('Using templates...')\n",
    "        templates_ica = pd.read_pickle('data/eog_templates.pkl')\n",
    "\n",
    "        template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "        template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "        mne.preprocessing.corrmap(\n",
    "            [ica], \n",
    "            template=template_veog_component, \n",
    "            threshold=0.9, \n",
    "            label=\"veog blink\", \n",
    "            plot=False\n",
    "        )\n",
    "        mne.preprocessing.corrmap(\n",
    "            [ica], \n",
    "            template=template_heog_component, \n",
    "            threshold=0.8, \n",
    "            label=\"heog blink\", \n",
    "            plot=False\n",
    "        )\n",
    "\n",
    "        veog_indices = ica.labels_['veog blink']\n",
    "        heog_indices = ica.labels_['heog blink']\n",
    "\n",
    "    logger_preprocessing_info.info(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "    fig = ica.plot_components(veog_indices + heog_indices)\n",
    "    \n",
    "    if len(veog_indices + heog_indices) < 2:\n",
    "        logger_errors_info.info(f\"Number of ICA components to exclude ({len(veog_indices + heog_indices)}) is lesser than 2\")\n",
    "    \n",
    "    ica.exclude = veog_indices + heog_indices\n",
    "    \n",
    "    reconstructed_raw = raw.copy()\n",
    "    ica.apply(reconstructed_raw)\n",
    "    \n",
    "    del filtered_raw_ica\n",
    "\n",
    "    return reconstructed_raw\n",
    "\n",
    "\n",
    "def get_k_nearest_neighbors(target_ch_name, epochs, k=6):\n",
    "    \"\"\"\n",
    "    Finds k nearest neighbors of given channel according to the 3D channels positions from the Epoch INFO\n",
    "    :param target_ch_name: String\n",
    "        Name of the target channel.\n",
    "    :param epochs: mne Epochs\n",
    "        Epochs with info attribute that consists of channels positions.\n",
    "    :param k: int\n",
    "        Number of neighbors to use by default for kneighbors queries. \n",
    "    :return: \n",
    "        indices: ndarray of shape (n_neighbors)\n",
    "            Indices of the nearest channels.\n",
    "        neighbor_ch_names: ndarray of shape (n_neighbors)\n",
    "            Names of the nearest channels.   \n",
    "    \"\"\"\n",
    "    epochs_copy_eeg_channels = epochs.copy().pick('eeg')\n",
    "    info = epochs_copy_eeg_channels.info\n",
    "    ch_names = epochs_copy_eeg_channels.info['ch_names']\n",
    "    \n",
    "    chs = [info[\"chs\"][pick] for pick in np.arange(0,len(ch_names))]\n",
    "    electrode_positions_3d =[]\n",
    "    \n",
    "    for ch in chs:\n",
    "        electrode_positions_3d.append((ch['ch_name'], ch[\"loc\"][:3]))\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=k+1, algorithm='auto')\n",
    "    ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d])\n",
    "\n",
    "    neighbors_model.fit(ch_coordinates)\n",
    "\n",
    "    target_ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d if ch_name_coordinates[0] == target_ch_name])\n",
    "    \n",
    "    target_ch_coordinates = target_ch_coordinates.reshape(1,-1)\n",
    "\n",
    "    distances, indices = neighbors_model.kneighbors(target_ch_coordinates)\n",
    "    neighbor_ch_names = []\n",
    "    \n",
    "    # Log the nearest neighbors without the first (self) neighbor\n",
    "    logger_preprocessing_info.debug(f\"{k} Nearest Neighbors of {target_ch_name}:\")\n",
    "    for i, (distance, index) in enumerate(zip(distances.flatten(), indices.flatten())):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            neighbor_point = electrode_positions_3d[index]\n",
    "            logger_preprocessing_info.debug(f\"Neighbor {i + 1}: Index {index}, Distance {distance:.2f}, Coordinates {neighbor_point}\")\n",
    "            neighbor_ch_names.append(neighbor_point[0])\n",
    "            \n",
    "    return indices.flatten()[1:], np.array(neighbor_ch_names)\n",
    "\n",
    "\n",
    "def find_bad_trails(epochs):\n",
    "    \"\"\"\n",
    "    Channels that meet following conditions will be marked as bad for the trail:\n",
    "        (1) Channels with a voltage difference of 100 μV through the duration of the epoch;\n",
    "        (2) Channels that were flat;\n",
    "        (3) Channels with more than a 30 μV difference with the nearest six neighbors; \n",
    "    :param mne Epochs \n",
    "        Epochs to find bad channels per trial. \n",
    "    :return: drop_log: tuple of n_trials length\n",
    "        Tuple representing bad channels names per trial.\n",
    "    \"\"\"\n",
    "    epochs_copy = epochs.copy()\n",
    "    \n",
    "    # channels with a voltage difference of 100 μV through the duration of the epoch\n",
    "    reject_criteria = dict(eeg=100e-6)\n",
    "    # flat channels (less than 1 µV of peak-to-peak difference)\n",
    "    flat_criteria = dict(eeg=1e-6)\n",
    "    \n",
    "    epochs_copy.drop_bad(reject=reject_criteria, flat=flat_criteria)\n",
    "    drop_log = epochs_copy.drop_log\n",
    "\n",
    "    # channels with more than a 30 μV difference with the nearest six neighbors\n",
    "    for idx, _ in enumerate(epochs_copy):\n",
    "        epoch = epochs[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "        for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "            mean_channel_data = np.array(np.mean(epoch_data[0,ch_idx,:]))\n",
    "    \n",
    "            ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "                target_ch_name = ch_name,\n",
    "                epochs = epoch,\n",
    "                k=6\n",
    "            )\n",
    "    \n",
    "            mean_neighbors_data = np.array([np.mean(epoch_data[0, ch_neighbor_index, :]) \n",
    "                                            for ch_neighbor_index in ch_neighbors_indices])\n",
    "    \n",
    "            # # if channels has more than a 30 μV difference with the nearest six neighbors\n",
    "            if (abs(mean_neighbors_data - mean_channel_data) > 30e-6).all():\n",
    "                logger_preprocessing_info.info(f'Channel {ch_name} has more than a 30 μV difference with the nearest six neighbors at {idx} trail.\\n Mean channel data: {mean_channel_data}\\nMean neighbors data: {mean_neighbors_data}')\n",
    "    \n",
    "                new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx]\n",
    "                drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))\n",
    "\n",
    "    del epochs_copy\n",
    "    \n",
    "    return drop_log\n",
    "\n",
    "def calculate_percentage(tuple_of_tuples, element):\n",
    "    total_tuples = len(tuple_of_tuples)\n",
    "    # Avoid division by zero\n",
    "    if total_tuples == 0:\n",
    "        return 0 \n",
    "\n",
    "    tuples_with_element = sum(1 for inner_tuple in tuple_of_tuples if element in inner_tuple)\n",
    "    percentage = (tuples_with_element / total_tuples)\n",
    "    return percentage\n",
    "\n",
    "def find_global_bad_channels(epochs, drop_log):\n",
    "    '''\n",
    "    (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4;\n",
    "    (2) Channels that were marked as bad for more than 20% of epochs;\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :return: \n",
    "    '''\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True)\n",
    "    concatenated_epochs_data = np.concatenate(epochs_data, axis=1)\n",
    "    \n",
    "    global_bad_channels_drop_log = {}\n",
    "    \n",
    "    # (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4\n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        channel_data = concatenated_epochs_data[ch_idx]\n",
    "    \n",
    "        ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "            target_ch_name = ch_name,\n",
    "            epochs = epochs_copy,\n",
    "            k=6\n",
    "        )\n",
    "        channel_neighbors_data = np.array([concatenated_epochs_data[ch_neighbor_index]\n",
    "                                           for ch_neighbor_index in ch_neighbors_indices])\n",
    "        channels_corr = np.tril(np.corrcoef(channel_neighbors_data, channel_data), k=-1)\n",
    "    \n",
    "        if (abs(channels_corr[-1][:-1]) < .4).all():\n",
    "            logger_preprocessing_info.info(f'Channel {ch_name} has < 0.4 abs corr with six nearest neighbors. Set as globally BAD. Channel corrs with neighbors: {channels_corr[-1][:-1]}')\n",
    "    \n",
    "            # mark channel as globally bad\n",
    "            global_bad_channels_drop_log[ch_name] = ['LOW CORR NEIGH']\n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "    \n",
    "    # (2) Channels that were marked as bad for more than 20% of epochs   \n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        percentage = calculate_percentage(drop_log, ch_name)\n",
    "    \n",
    "        if percentage > 0.2:\n",
    "            logger_preprocessing_info.info(f'Channel {ch_name} is bad for {percentage} percent of epochs. Set as globally BAD.')\n",
    "    \n",
    "            if ch_name in global_bad_channels_drop_log:\n",
    "                global_bad_channels_drop_log[ch_name].append('BAD FOR MORE THAN 20%')\n",
    "            else:\n",
    "                global_bad_channels_drop_log[ch_name] = ['BAD FOR MORE THAN 20%']\n",
    "    \n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "            \n",
    "    del epochs_copy\n",
    "            \n",
    "    return drop_log, global_bad_channels_drop_log\n",
    "\n",
    "def mark_bad_trials(epochs, drop_log, threshold=0.1):\n",
    "    \"\"\"\n",
    "    If more than 10% of channels were marked bad for an epoch (trial), the entire epoch was rejected\n",
    "    :param drop_log: \n",
    "    :return: trials_to_drop: ndarray\n",
    "    '\"\"\"\n",
    "    trials_to_drop_indices = []\n",
    "    epochs_copy = epochs.copy()\n",
    "\n",
    "    assert len(epochs_copy) == len(drop_log), f'Length of epochs ({len(epochs_copy)}) not equals length of drop_log ({len(drop_log)}). Cannot mark trials as BAD.'\n",
    "    threshold_items = int(threshold * len(epochs_copy.info['ch_names']))\n",
    "    for idx, item in enumerate(drop_log):\n",
    "        if len(item) > threshold_items:\n",
    "            logger_preprocessing_info.info(f'More than 10% of channels are bad for trial {idx}. Trial set as \\'TO DROP\\'')\n",
    "            trials_to_drop_indices.append(idx)\n",
    "    \n",
    "    # update drop_log\n",
    "    for trial_idx in trials_to_drop_indices:\n",
    "        drop_log = tuple(drop_log[i] + ('TO DROP',) if i == trial_idx else item for i, item in enumerate(drop_log))\n",
    "    \n",
    "    return trials_to_drop_indices, drop_log\n",
    "\n",
    "def reject_bad_trials(epochs, drop_log, trials_to_drop_indices=None):\n",
    "    clean_epochs = epochs.copy()\n",
    "    \n",
    "    clean_epochs = clean_epochs.drop(\n",
    "        indices = trials_to_drop_indices,\n",
    "        reason = 'MORE THAN 10% CHANNELS MARKED AS BAD',\n",
    "    )\n",
    "    \n",
    "    # update drop_log\n",
    "    for trial_idx in trials_to_drop_indices:\n",
    "        drop_log = tuple(('REJECTED',) if i == trial_idx else element for i, element in enumerate(drop_log))\n",
    "    \n",
    "    return clean_epochs, drop_log\n",
    "\n",
    "def interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log):\n",
    "    \"\"\"\n",
    "    Bad channels were interpolated using spherical splines\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :param global_bad_channels_drop_log: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_copy.info['bads'] = list(global_bad_channels_drop_log.keys())\n",
    "    epochs_interpolated_bad_channels = epochs_copy.interpolate_bads(method='spline')\n",
    "\n",
    "    # update drop log to remove interpolated channels\n",
    "    updated_drop_log = drop_log\n",
    "    for ch_name in list(global_bad_channels_drop_log.keys()):\n",
    "        updated_drop_log = tuple(tuple(element for element in drop_log_item if element != ch_name) for drop_log_item in updated_drop_log)\n",
    "    \n",
    "    return epochs_interpolated_bad_channels, updated_drop_log\n",
    "\n",
    "def create_erps_waves(epochs, drop_log, new_response_event_dict, type = 'error_response', tmin=0, tmax=0.1, picks=['FCz']):\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True, picks=picks, tmin=tmin, tmax=tmax)\n",
    "    events = epochs_copy.events\n",
    "\n",
    "    eeg_data = []\n",
    "\n",
    "    if (len(epochs_data) == len(drop_log)) & (len(events) == len(drop_log)):\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if events[idx][-1] == new_response_event_dict[type]:\n",
    "                if ('TO DROP' in item) or any(element in item for element in picks):\n",
    "                    pass\n",
    "                else:\n",
    "                    eeg_data.append(epochs_data[idx])\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(eeg_data)\n",
    "    else:\n",
    "        logger_preprocessing_info.info(f'Epochs length is not equal to drop_log length:\\nepochs: {len(epochs_data)}\\ndrop_log: {len(drop_log)}')\n",
    "        return None\n",
    "\n",
    "def create_erps(epochs, drop_log, new_response_event_dict, type = 'error_response', tmin=0, tmax=0.1, picks=['FCz']):\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True, picks=picks, tmin=tmin, tmax=tmax)\n",
    "    events = epochs_copy.events\n",
    "\n",
    "    erps_data = []\n",
    "\n",
    "    if (len(epochs_data) == len(drop_log)) & (len(events) == len(drop_log)):\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if events[idx][-1] == new_response_event_dict[type]:\n",
    "                if ('TO DROP' in item) or any(element in item for element in picks):\n",
    "                    erps_data.append([None])\n",
    "                else:\n",
    "                    erps_data.append(np.mean(epochs_data[idx], axis=-1))\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(erps_data)\n",
    "    else:\n",
    "        logger_preprocessing_info.info(f'Epochs length is not equal to drop_log length:\\nepochs: {len(epochs_data)}\\ndrop_log: {len(drop_log)}')\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc62ce10ab154b16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pre_process_eeg(input_fname, context, trigger_fname=None):\n",
    "    # 0. read bdf\n",
    "    raw = mne.io.read_raw_bdf(\n",
    "        input_fname,\n",
    "        eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "        exclude=['EXG5', 'EXG6'],\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        raw = raw.set_montage('biosemi64')\n",
    "    except ValueError as e:\n",
    "        if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "            raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "            logger_preprocessing_info.info('On missing')\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Lacks important channels!')\n",
    "    \n",
    "    # 1. replace trigger names\n",
    "    trigger_map = read_trigger_map(trigger_fname)\n",
    "    raw_new_triggers = replace_trigger_names(raw, trigger_map)\n",
    "\n",
    "    # 2. re-reference: to mastoids\n",
    "    raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "\n",
    "    # 3. 4-th order Butterworth filters\n",
    "    raw_filtered = raw_ref.copy().filter(\n",
    "        l_freq=.1,\n",
    "        h_freq=30.0,\n",
    "        n_jobs=10,\n",
    "        method='iir',\n",
    "        iir_params=None,\n",
    "    )\n",
    "\n",
    "    # 4. Notch filter at 50 Hz\n",
    "    raw_filtered = raw_filtered.notch_filter(\n",
    "        freqs=np.arange(50, (raw_filtered.info['sfreq'] / 2), 50),\n",
    "        n_jobs=10,\n",
    "    )\n",
    "    \n",
    "    # 5. ocular correction with ICA\n",
    "    raw_corrected_eogs = ocular_correction_ica(\n",
    "        raw_filtered, \n",
    "        raw_ref, \n",
    "        heog=['EXG3', 'EXG4'], \n",
    "        veog=['EXG1', 'EXG2'], \n",
    "        from_template=True, \n",
    "        info=True,\n",
    "    )\n",
    "\n",
    "    # 6. segmentation -400 to 800 ms around the response\n",
    "    raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "    epochs = create_epochs(\n",
    "        raw_corrected_eogs_drop_ref,\n",
    "        context=context,\n",
    "        tmin=-.4,\n",
    "        tmax=.8,\n",
    "        reject=None,\n",
    "        reject_by_annotation=False,\n",
    "    )\n",
    "\n",
    "    # 7. Trial-wise Bad Channels Identification\n",
    "    drop_log = find_bad_trails(epochs)\n",
    "\n",
    "    # 8. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "    drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)\n",
    "\n",
    "    # 9. calculate trails to remove\n",
    "    trials_to_drop_indices, drop_log = mark_bad_trials(epochs, drop_log, threshold=0.1)\n",
    "\n",
    "    # 10. Interpolate bad channels (and thus update drop log)\n",
    "    interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)\n",
    "\n",
    "    # 11. Remove participants that have less then 6 trials\n",
    "    clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "    if len(clean_epochs) < 6:\n",
    "        logger_preprocessing_info.info(f\"Participant ID: {id} has not enough clean trials\")\n",
    "        return []\n",
    "\n",
    "    # 13. Baseline correction\n",
    "    if case == 'RE':\n",
    "        interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)\n",
    "    elif case == 'STIM':\n",
    "        interpolated_epochs.apply_baseline(baseline=(-0.2, 0),)\n",
    "    else:\n",
    "        logger_preprocessing_info.info('Not know case. Setting baseline from -0.2 to 0')\n",
    "        interpolated_epochs.apply_baseline(baseline=(-0.2, 0),)\n",
    "\n",
    "    return interpolated_epochs, drop_log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edea2632f8b5d423"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log_separately(epochs, drop_log, id):\n",
    "    # save drop_log\n",
    "    with open(f'{preprocessed_data_dir_path}drop_log_{id}.json', 'w') as fjson:\n",
    "        json.dump(drop_log, fjson)\n",
    "\n",
    "    # save Epoch object\n",
    "    epochs.save(f'{preprocessed_data_dir_path}preprocessed_{id}-epo.fif', overwrite=True)\n",
    "\n",
    "    return logger_preprocessing_info.info('Epochs saved to fif. Drop log saved to json.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be2082a55bf73ab7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log(epochs, drop_log, id):\n",
    "    item = pd.DataFrame({\n",
    "        'epochs': [epochs],\n",
    "        'drop_log': [drop_log],\n",
    "    })\n",
    "    \n",
    "    item.to_pickle(f'{preprocessed_data_dir_path}preprocessed_{id}.pkl')\n",
    "    \n",
    "    return logger_preprocessing_info.info('Epochs saved to pickle.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2431faf1c3221a26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_behavioral_file(id):\n",
    "    \n",
    "    behavioral_data_df = pd.read_csv(f'{behavioral_dir_path}beh_{id}.csv')\n",
    "\n",
    "    trial_numerator = 1\n",
    "    trial_numbers = []\n",
    "    for i in range(0, len(behavioral_data_df)):\n",
    "        if behavioral_data_df.iloc[i]['block type'] != 'experiment':\n",
    "            trial_numbers.append(0)\n",
    "        else:\n",
    "            trial_numbers.append(trial_numerator)\n",
    "            trial_numerator+=1\n",
    "    \n",
    "    behavioral_data_df['trial number'] = trial_numbers\n",
    "    return behavioral_data_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6002388a62c3c418"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_epochs_with_behavioral_data_long(epochs, drop_log, id, case='RE'):\n",
    "    \n",
    "    # read behavioral file\n",
    "    behavioral_data_df = read_behavioral_file(id)\n",
    "\n",
    "    beh_data_uninhibited_nogo_responses_df = behavioral_data_df[\n",
    "        (behavioral_data_df['block type'] == 'experiment') &\n",
    "        (behavioral_data_df['trial type'] != 'go') &\n",
    "        (behavioral_data_df['reaction'] == False)\n",
    "        ]\n",
    "    logger_preprocessing_info.info(f'Number of uninhibited NOGO trials: {len(beh_data_uninhibited_nogo_responses_df)}')\n",
    "    \n",
    "    beh_data_inhibited_nogo_responses_df = behavioral_data_df[\n",
    "        (behavioral_data_df['block type'] == 'experiment') &\n",
    "        (behavioral_data_df['trial type'] != 'go') &\n",
    "        (behavioral_data_df['reaction'] == True)\n",
    "        ]\n",
    "    logger_preprocessing_info.info(f'Number of inhibited NOGO trials: {len(beh_data_inhibited_nogo_responses_df)}')\n",
    "    \n",
    "    beh_data_correct_go_responses_df = behavioral_data_df[\n",
    "        (behavioral_data_df['block type'] == 'experiment') &\n",
    "        (behavioral_data_df['trial type'] == 'go') &\n",
    "        (behavioral_data_df['response'] == 'num_separator')\n",
    "        ]\n",
    "    logger_preprocessing_info.info(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    epochs_df = pd.DataFrame()\n",
    "    behavioral_df = pd.DataFrame()\n",
    "    \n",
    "    if case == 'RE':\n",
    "        behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "        logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "        logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "        assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "                \n",
    "        for idx, _ in enumerate(epochs):\n",
    "            epoch = epochs[idx]\n",
    "            epoch_type = list(epoch.event_id.keys())\n",
    "            assert len(epoch_type) == 1, \\\n",
    "                f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "            drop_log_item = drop_log[idx]\n",
    "            \n",
    "            this_df = pd.DataFrame({\n",
    "                'epoch': [epoch],\n",
    "                'event': epoch_type,\n",
    "                'drop_log': [drop_log_item],\n",
    "            })\n",
    "\n",
    "            epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "        # Set the indexes of epochs to match reactions\n",
    "        indexes = behavioral_df.index\n",
    "        epochs_df.set_index(indexes, inplace=True)\n",
    "        results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "     \n",
    "    elif case == 'STIM':\n",
    "        behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df, beh_data_inhibited_nogo_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "        logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "        logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "        assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "        for idx, _ in enumerate(epochs):\n",
    "            epoch = epochs[idx]\n",
    "            epoch_type = list(epoch.event_id.keys())\n",
    "            assert len(epoch_type) == 1, \\\n",
    "                f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "            drop_log_item = drop_log[idx]\n",
    "\n",
    "            this_df = pd.DataFrame({\n",
    "                'epoch': [epoch],\n",
    "                'event': epoch_type,\n",
    "                'drop_log': [drop_log_item],\n",
    "            })\n",
    "\n",
    "            epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "        # Set the indexes of epochs to match reactions\n",
    "        indexes = behavioral_df.index\n",
    "        epochs_df.set_index(indexes, inplace=True)\n",
    "        results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "        \n",
    "    else:\n",
    "        logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "    assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "    results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{id}.pkl')\n",
    "    logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "    \n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e8e0c957e9e500d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a429187fd12024"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set globals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e57fece67b0970c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GNG | SST | Flanker\n",
    "paradigm = 'GNG'\n",
    "# RE | STIM | FBCK\n",
    "case = 'RE'\n",
    "# todo think whether move global vars as paradigm and case info some kind of data/case class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5b1085cc81721c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set paths base on globals values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "821407c002c73997"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trigger_dir_path = f'data/{paradigm}/raw/triggers/'\n",
    "bdf_dir_path = f'data/{paradigm}/raw/bdfs/'\n",
    "behavioral_dir_path = f'data/{paradigm}/behavioral/'\n",
    "preprocessed_data_dir_path = f'data/{paradigm}/preprocessed/{case}/'\n",
    "logger_dir_path = f'data/{paradigm}/'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7845cbc91ba0164"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set output files for loggers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71cca2995cdf5cd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a file handler for preprocessing and set the level to INFO\n",
    "file_handler_preprocessing = logging.FileHandler(f'data/{paradigm}/{case}_preprocessing.txt')\n",
    "file_handler_preprocessing.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for preprocessing\n",
    "formatter_preprocessing = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_preprocessing.setFormatter(formatter_preprocessing)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_preprocessing_info.addHandler(file_handler_preprocessing)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a file handler for errors and set the level to INFO\n",
    "file_handler_errors = logging.FileHandler(f'data/{paradigm}/{case}_errors.txt')\n",
    "file_handler_errors.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for errors\n",
    "formatter_errors = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_errors.setFormatter(formatter_errors)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_errors_info.addHandler(file_handler_errors)\n",
    "\n",
    "##### MNE ###################################################\n",
    "# Create logger for MNE logs\n",
    "logger_f_name = f'data/{paradigm}/{case}_MNE-logs.txt'\n",
    "set_log_file(fname=logger_f_name, output_format=\"%(asctime)s - %(message)s\", overwrite=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f3e5d98842985c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read participant IDs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f41004d3f9f42f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_list = [item.split('.')[0] for item in os.listdir(bdf_dir_path)]\n",
    "id_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66f8dba67e70f4fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for id in id_list:\n",
    "    bdf_fname = f'{bdf_dir_path}{id}.bdf'\n",
    "    trigger_fname = f'{trigger_dir_path}triggerMap_{id}.txt'\n",
    "\n",
    "    logger_preprocessing_info.info(f'#### PARTICIPANT ID: {id} #########')\n",
    "    logger_errors_info.info(f'#### PARTICIPANT ID: {id} #########')\n",
    "\n",
    "    try:\n",
    "        trigger_map = read_trigger_map(trigger_fname)\n",
    "        participant_context = create_events_mappings(trigger_map)\n",
    "        logger_preprocessing_info.info(f'Context: {participant_context}')\n",
    "        \n",
    "        epochs_preprocessed, drop_log = pre_process_eeg(\n",
    "            input_fname=bdf_fname,\n",
    "            context=participant_context,\n",
    "            trigger_fname=trigger_fname,\n",
    "        )\n",
    "    \n",
    "        _ = save_epochs_with_behavioral_data_long(\n",
    "            epochs_preprocessed,\n",
    "            drop_log,\n",
    "            id,\n",
    "            case=case,\n",
    "        )\n",
    "    except Exception as e:        \n",
    "        logger_errors_info.info(f\"{e}\")\n",
    "    \n",
    "    logger_preprocessing_info.info(f'\\n')\n",
    "    logger_errors_info.info(f'\\n')\n",
    "\n",
    "print(f'##########\\n DONE\\n')   \n",
    "# Restore MNE logging to std out     \n",
    "set_log_file(fname=None)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a16216d1b5baba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## TESTS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "210739182f0bf7e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id = 'A-GNG-219'\n",
    "case = 'RE'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2015d4a86f612fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)  # Set the desired logging level\n",
    "\n",
    "logger_errors_info.addHandler(console_handler)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8cdd5d60e11594c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bdf_fname = f'{bdf_dir_path}{id}.bdf' \n",
    "trigger_fname = f'{trigger_dir_path}triggerMap_{id}.txt'\n",
    "logger_f_name = f'{logger_dir_path}{case}_logs.txt'\n",
    "\n",
    "\n",
    "trigger_map = read_trigger_map(trigger_fname)\n",
    "participant_context = create_events_mappings(trigger_map, case=case)\n",
    "print(participant_context)\n",
    "\n",
    "# set_log_file(fname=logger_f_name, output_format=\"%(asctime)s - %(message)s\", overwrite=None)\n",
    "\n",
    "# epochs_preprocessed, drop_log = pre_process_eeg(\n",
    "#     input_fname=bdf_fname,\n",
    "#     trigger_fname=trigger_fname,\n",
    "#     context=participant_context,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a3c545bf3cad5b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_preprocessed2 = epochs_preprocessed.copy().crop(-0.1, 0.6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94ccbba7971895a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nogo = epochs_preprocessed2[\"inhibited_nogo\", \"uninhibited_nogo\"].average()\n",
    "# unin_nogo = epochs_preprocessed2[\"uninhibited_nogo\"].average()\n",
    "go = epochs_preprocessed2[\"successful_go\"].average()\n",
    "\n",
    "evokeds = {\n",
    "    # 'INH NOGO': in_nogo,\n",
    "    # 'UNIN NGOG': unin_nogo,\n",
    "    'NOGO': nogo,\n",
    "    'SUC GO': go,\n",
    "}\n",
    "\n",
    "mne.viz.plot_compare_evokeds(evokeds, picks='Cz')\n",
    "\n",
    "# fig = mne.viz.plot_evoked_topomap(unin_nogo, times=[-0.1, 0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4])\n",
    "fig = mne.viz.plot_evoked_topomap(nogo, times=[-0.1, 0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4])\n",
    "fig = mne.viz.plot_evoked_topomap(go, times=[-0.1, 0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3ef159cb8f44d62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d7f083779a1fce6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_nogo = epochs_preprocessed[\"inhibited_nogo\"].average()\n",
    "unin_nogo = epochs_preprocessed[\"uninhibited_nogo\"].average()\n",
    "go = epochs_preprocessed[\"successful_go\"].average()\n",
    "\n",
    "evokeds = {\n",
    "    'INH NOGO': in_nogo,\n",
    "    'UNIN NGOG': unin_nogo,\n",
    "    'SUC GO': go,\n",
    "}\n",
    "\n",
    "mne.viz.plot_compare_evokeds(evokeds)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc9ddf24a4e735"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_epochs_with_drop_log(epochs_preprocessed, drop_log, id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "209e083dc6d43b68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save_epochs_with_drop_log_separately(epochs_preprocessed, drop_log, id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710fde14cb77663d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_event_dict = {\"inhibited_nogo\": 0, \"uninhibited_nogo\": 1, \"successful_go\": 2}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1a0be44c164fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = save_epochs_with_behavioral_data_long(\n",
    "    epochs_preprocessed, \n",
    "    drop_log, \n",
    "    id,\n",
    "    case='STIM',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ecccbfc4463e5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507f15a5db4c6f93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "450576de07e784e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6ad8c6e166a8a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id = 'B-GNG-102'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41121cf44e8b472a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_df = pd.read_pickle(f'{preprocessed_data_dir_path}preprocessed_{id}.pkl')\n",
    "epochs = epochs_df['epochs'].to_numpy().flatten()[0]\n",
    "drop_log = epochs_df['drop_log'].to_numpy().flatten()[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65258cf1cbc6fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_wave = create_erps_waves(\n",
    "    epochs, \n",
    "    drop_log, \n",
    "    type='uninhibited_response', \n",
    "    tmin=-0.1, \n",
    "    tmax=0.6, \n",
    "    picks=['FCz'],\n",
    "    new_response_event_dict = {\"inhibited_response\": 0, \"uninhibited_response\": 1}\n",
    ")\n",
    "correct_wave = create_erps_waves(\n",
    "    epochs, \n",
    "    drop_log, \n",
    "    type='inhibited_response', \n",
    "    tmin=-0.1, \n",
    "    tmax=0.6, \n",
    "    picks=['FCz'],\n",
    "    new_response_event_dict = {\"inhibited_response\": 0, \"uninhibited_response\": 1}\n",
    ")\n",
    "\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten())\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(correct_wave, axis=0).flatten())), np.mean(correct_wave, axis=0).flatten())\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a4bd4a3b107d0b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_wave.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b86a0d7c352570e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ERPs scoring\n",
    "ern_single_trials = create_erps(epochs_preprocessed, drop_log, type='error_response', tmin=0, tmax=0.1, picks=['FCz'])\n",
    "pe_single_trials = create_erps(epochs_preprocessed, drop_log, type='error_response', tmin=0.2, tmax=0.4, picks=['Pz'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4294fd3733c3d47e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## For testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb751664fc106984"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_fname = 'data/GNG/raw/bdfs/B-GNG-102.bdf'\n",
    "raw = mne.io.read_raw_bdf(\n",
    "    input_fname, \n",
    "    eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'], \n",
    "    exclude=['EXG5', 'EXG6'], \n",
    "    preload=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    raw = raw.set_montage('biosemi64')\n",
    "except ValueError as e:\n",
    "    if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "        raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "        print('On missing')\n",
    "    else:\n",
    "        print('Lacks important channels!')\n",
    "\n",
    "\n",
    "file_path = 'data/GNG/raw/triggers/triggerMap_B-GNG-102.txt'\n",
    "trigger_map = read_trigger_map(file_path)\n",
    "# raw_new_triggers = replace_trigger_names(raw, trigger_map)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24a0c51998a3561b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trigger_map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a7f89394daba94e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'TG' in trigger_map[1][1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818baa87274afec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_response_info(trigger_map):\n",
    "    new_trigger_map = trigger_map.copy()\n",
    "    for idx, trigger in enumerate(new_trigger_map):\n",
    "        if 'TG' in trigger[1]:\n",
    "            print('in target')\n",
    "            if 'RE' in new_trigger_map[idx+1][1]:\n",
    "                print('adding RE')\n",
    "                new_trigger = (trigger[0], trigger[1][:-1]+'R')\n",
    "                new_trigger_map[idx] = new_trigger\n",
    "                \n",
    "    return new_trigger_map   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9811c59714e3d3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ne_tg_map = add_response_info(trigger_map)\n",
    "# ne_tg_map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8c076b616a9552"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ne_tg_map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b97dc24b632e9dc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace event IDs in the Raw object\n",
    "events = mne.find_events(raw, stim_channel='Status')\n",
    "new_events_list = events.copy()\n",
    "print(f'EVENTS: {new_events_list}')\n",
    "\n",
    "assert len(events) == len(trigger_map), \\\n",
    "    f'The length of trigger map ({len(trigger_map)}) not equals length of events in eeg recording ({len(events)})'\n",
    "\n",
    "trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "for idx, event in enumerate(events):\n",
    "    event_id = str(event[2])[-1]\n",
    "    trigger_id = trigger_map[idx][0]\n",
    "    trigger_new_code = trigger_map[idx][1]\n",
    "    \n",
    "    print(event)\n",
    "    print(event_id)\n",
    "    print(trigger_id)\n",
    "# \n",
    "    if event_id != trigger_id:\n",
    "        print(f'An event {idx} has different number than in provided file. {trigger_id} expected, {str(event[2])} found. Triggers may need to be checked.')\n",
    "\n",
    "    trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "    new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "annot_from_events = mne.annotations_from_events(\n",
    "    events=new_events_list,\n",
    "    event_desc=mapping,\n",
    "    sfreq=raw.info[\"sfreq\"],\n",
    "    orig_time=raw.info[\"meas_date\"],\n",
    ")\n",
    "raw_copy = raw.copy()\n",
    "raw_copy.set_annotations(annot_from_events)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a56f8e9cf4857fec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(new_events_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bab16a51807ceb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ebe805bfa311b0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# matplotlib.use('Qt5Agg')\n",
    "# plt.switch_backend('QtAgg')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a814df4cfa3da18c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = raw_copy.plot(start=1404, duration=15)\n",
    "# plt.pause(0.0001)\n",
    "# %matplotlib\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c621e0923940b5c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mne.viz.plot_raw(raw_copy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "585e25e7b089c301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. re-reference: to mastoids\n",
    "raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "# fig = raw_ref.plot(start=60, duration=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80136436d945d215"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. 4-th order Butterworth filters\n",
    "raw_filtered = raw_ref.copy().filter(\n",
    "    l_freq=.1,\n",
    "    h_freq=30.0,\n",
    "    n_jobs=10,\n",
    "    method='iir',\n",
    "    iir_params=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc89381c09684f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Notch filter at 50 Hz\n",
    "raw_filtered = raw_filtered.notch_filter(\n",
    "    freqs=np.arange(50, 251, 50),\n",
    "    n_jobs=10,\n",
    "    # method='iir',\n",
    "    # iir_params=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "797466c92a2ed62f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. ocular artifact correction with ICA\n",
    "# raw_corrected_eogs = ocular_correction_ica(raw_filtered, raw_ref, heog=['EXG3', 'EXG4'], veog=['EXG1', 'EXG2'])\n",
    "heog=['EXG3', 'EXG4']\n",
    "veog=['EXG1', 'EXG2']\n",
    "filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "ica = mne.preprocessing.ICA(\n",
    "    n_components=12,\n",
    "    method='infomax',\n",
    "    max_iter=\"auto\",\n",
    "    random_state=random_state\n",
    ")\n",
    "ica.fit(filtered_raw_ica)\n",
    "\n",
    "ica.exclude = []\n",
    "\n",
    "# find which ICs match the VEOG pattern\n",
    "veog_indices, veog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=veog,\n",
    "    threshold=0.9,\n",
    "    measure='correlation'\n",
    ")\n",
    "\n",
    "# find which ICs match the HEOG pattern\n",
    "heog_indices, heog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=heog,\n",
    "    threshold=0.7,\n",
    "    measure='correlation'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1b9034b1d9f83e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('ICA components')\n",
    "fig = ica.plot_components()\n",
    "\n",
    "print(f\"VEOG indices: {veog_indices}\\nVEOG scores: {veog_scores}\\n\")\n",
    "print(f\"HEOG indices: {heog_indices}\\nHEOG scores: {heog_scores}\\n\")\n",
    "\n",
    "\n",
    "print(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "fig = ica.plot_components(veog_indices + heog_indices)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598e52e31553fb12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if len(veog_indices + heog_indices) == 0 and from_template == 'auto':\n",
    "# print('No ICA component correlates with EOG channels. Using templates...')\n",
    "# ica_template = pd.read_pickle('data/eog_templates.pkl')\n",
    "# veog_template = ica_template[['VEOG']].to_numpy()\n",
    "# heog_template = ica_template[['HEOG']].to_numpy()\n",
    "# \n",
    "# mne.preprocessing.corrmap([ica], template=veog_template, threshold=0.9)\n",
    "# mne.preprocessing.corrmap([ica], template=heog_template, threshold=0.8)\n",
    "\n",
    "\n",
    "# elif from_template is True:\n",
    "#     print('Using templates...')\n",
    "\n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw.copy()\n",
    "# ica.apply(reconstructed_raw)\n",
    "\n",
    "template_veog_component = ica.get_components()[:, veog_indices[0]]\n",
    "template_heog_component = ica.get_components()[:, heog_indices[0]]\n",
    "\n",
    "templates_ica = pd.DataFrame({\n",
    "    'VEOG': [template_veog_component],\n",
    "    'HEOG': [template_heog_component],\n",
    "})\n",
    "templates_ica.to_pickle('data/eog_templates.pkl')\n",
    "#########\n",
    "templates_ica = pd.read_pickle('data/eog_templates.pkl')\n",
    "\n",
    "template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "\n",
    "mne.preprocessing.corrmap([ica], template=template_veog_component, threshold=0.9, label=\"veog blink\", plot=False)\n",
    "mne.preprocessing.corrmap([ica], template=template_heog_component, threshold=0.8, label=\"heog blink\", plot=False)\n",
    "\n",
    "to_exclude = ica.labels_['veog blink'] + ica.labels_['heog blink']\n",
    "print(to_exclude)\n",
    "\n",
    "\n",
    "\n",
    "# del filtered_raw_ica\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b9a8dc7e7e4174"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ica.labels_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c13a762401f4206c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "epochs = create_epochs(\n",
    "    raw_corrected_eogs_drop_ref, \n",
    "    context=participant_context\n",
    "    tmin=-.4, \n",
    "    tmax=.8,\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    ")\n",
    "\n",
    "fig = epochs.plot(n_epochs=20, n_channels=68)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "173d5e9b55631419"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. Trial-wise Bad Channels Identification\n",
    "drop_log = find_bad_trails(epochs) \n",
    "drop_log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da372e30630abee1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a08ff9538219b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8. calculate trails to remove\n",
    "trials_to_drop_indices, drop_log = calculate_bad_trials(drop_log, threshold=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29521e825f342e62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10. Interpolate bad channels (and thus update drop log)\n",
    "interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28fbd355474b393"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 12. Remove participants that have less then 6 trials\n",
    "clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "if len(clean_epochs) < 6:\n",
    "      print(f\"Participant ID: {id} has not enough clean trials\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c235a759bbd5c9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 13. Baseline correction\n",
    "interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1589b68b16f1edd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drop_log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af257db7dda387d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 13. ERPs scoring\n",
    "ern_single_trials = create_erps(interpolated_epochs, drop_log, type='error_response', tmin=0, tmax=0.1, picks=['FCz'])\n",
    "pe_single_trials = create_erps(interpolated_epochs, drop_log, type='error_response', tmin=0.2, tmax=0.4, picks=['Pz'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec31ac2c89fe2836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_wave = create_erps_waves(interpolated_epochs, drop_log, type='error_response', tmin=-0.1, tmax=0.6, picks=['FCz'])\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten(), )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "923162a9bc466d5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten(), )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0efeff532643e93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "# \n",
    "# ica = mne.preprocessing.ICA(\n",
    "#     n_components=12,\n",
    "#     method='infomax',\n",
    "#     max_iter=\"auto\",\n",
    "#     random_state=random_state\n",
    "# )\n",
    "# ica.fit(filtered_raw_ica)\n",
    "# \n",
    "# ica.exclude = []\n",
    "# heog=['EXG3', 'EXG4']\n",
    "# veog=['EXG1', 'EXG2']\n",
    "# \n",
    "# # find which ICs match the VEOG pattern\n",
    "# veog_indices, veog_scores = ica.find_bads_eog(\n",
    "#     filtered_raw_ica,\n",
    "#     ch_name=veog,\n",
    "#     threshold=0.9,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# # \n",
    "# print(f'{veog_indices}')\n",
    "# \n",
    "# heog_indices, heog_scores = ica.find_bads_eog(\n",
    "#     filtered_raw_ica,\n",
    "#     ch_name=heog,\n",
    "#     threshold=0.7,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# print(f'{heog_indices}')\n",
    "# \n",
    "# \n",
    "# print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "# \n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw_filtered.copy()\n",
    "# ica.apply(reconstructed_raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "643b4bb1037c5b3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ica.exclude = []\n",
    "# heog=['EXG3', 'EXG4']\n",
    "# veog=['EXG1', 'EXG2']\n",
    "# \n",
    "# # find which ICs match the VEOG pattern\n",
    "# veog_indices, veog_scores = ica.find_bads_eog(\n",
    "#     raw_ref,\n",
    "#     ch_name=veog,\n",
    "#     threshold=0.9,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# # \n",
    "# print(f'{veog_indices}')\n",
    "# \n",
    "# heog_indices, heog_scores = ica.find_bads_eog(\n",
    "#     raw_ref,\n",
    "#     ch_name=heog,\n",
    "#     threshold=0.7,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# print(f'{heog_indices}')\n",
    "# \n",
    "#  \n",
    "# print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "# \n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw_filtered.copy()\n",
    "# ica.apply(reconstructed_raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45b5b98ffe961431"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(heog_scores)\n",
    "print(veog_scores)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3e3f1c5698c88bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
