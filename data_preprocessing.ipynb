{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Replication of Clayson et al., 2023 Preprocessing scripts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcb8b16b3533c83c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb43fd47897e7af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "# GNG | SST | Flanker\n",
    "paradigm = 'GNG'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93c3ba90e198d2cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "GNG: \n",
    "- A-GNG-050 - zapis zaczπ≥ siÍ zbyt pÛüno, plik w Full Recode ma usuniÍte poczπtkowe triggery, makro no_check_iterator na poczπtku drzewka\n",
    "- B-GNG-128 - brakuje prawie ca≥ego treningu, plik w Full Recode ma usuniÍte poczπtkowe triggery, makro no_check_iterator na poczπtku drzewka\n",
    "\n",
    "- B-GNG-069 - dodatkowy trigger (9) na pozycji 359804, node Edit Markers na poczπtku drzewka\n",
    "- B-GNG-102 - dodatkowy trigger (15) na pozycji 328219, node Edit Markers na poczπtku drzewka\n",
    "\n",
    "- B-GNG-005 - sygna≥ zreferowany wy≥πcznie do A1, poniewaø A2 jest zbyt zaszumiona\n",
    "\n",
    "Exclude:\n",
    "- B-GNG-086 - 117 GO trials without reaction; no trials with good feedback (fast responses); 13 uninhibited NoGo trials;\n",
    "- B-GNG-075: no EEG recording;\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98aafc23f7d0533a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Paths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d26971eb67fcbb73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trigger_dir_path = f'data/{paradigm}/raw/triggers/'\n",
    "bdf_dir_path = f'data/{paradigm}/raw/bdfs/'\n",
    "behavioral_dir_path = f'data/{paradigm}/behavioral/'\n",
    "preprocessed_data_dir_path = f'data/{paradigm}/preprocessed/'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bae2ba232bcabc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79b9bbbeafc9c8c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_trigger_map(file_name):\n",
    "    line_count = 0\n",
    "    trigger_map = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read each line and increment the counter\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "            trigger = (match.group(1), match.group(2), )\n",
    "            trigger_map.append(trigger)\n",
    "        except:\n",
    "            pass\n",
    "        while line:\n",
    "            line_count += 1\n",
    "            line = file.readline()\n",
    "            try:\n",
    "                match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "                trigger = (match.group(1), match.group(2), )\n",
    "                trigger_map.append(trigger)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    assert len(trigger_map) == line_count\n",
    "\n",
    "    return trigger_map\n",
    "\n",
    "def create_triggers_dict(trigger_map):\n",
    "    triggers_codes = [item[1] for item in trigger_map]\n",
    "    # Create an ordered dictionary to maintain order and remove duplicates\n",
    "    unique_ordered_dict = OrderedDict.fromkeys(triggers_codes)\n",
    "    numbered_dict = {key: 1000 + number for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    reversed_numbered_dict = {1000 + number: key for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    return numbered_dict, reversed_numbered_dict\n",
    "\n",
    "def replace_trigger_names(raw, trigger_map, new_response_event_dict=None, replace=False, search='RE'):\n",
    "    # Replace event IDs in the Raw object\n",
    "    events = mne.find_events(raw, stim_channel='Status')\n",
    "    new_events_list = events.copy()\n",
    "\n",
    "    assert len(events) == len(trigger_map)\n",
    "\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    for idx, event in enumerate(events):\n",
    "        event_id = str(event[2])[-1]\n",
    "        trigger_id = trigger_map[idx][0]\n",
    "        trigger_new_code = trigger_map[idx][1]\n",
    "        assert event_id == trigger_id\n",
    "\n",
    "        trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "        new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "    annot_from_events = mne.annotations_from_events(\n",
    "        events=new_events_list,\n",
    "        event_desc=mapping,\n",
    "        sfreq=raw.info[\"sfreq\"],\n",
    "        orig_time=raw.info[\"meas_date\"],\n",
    "    )\n",
    "    raw_copy = raw.copy()\n",
    "    raw_copy.set_annotations(annot_from_events)\n",
    "\n",
    "    return raw_copy\n",
    "\n",
    "def find_items_matching_regex(dictionary, regex):\n",
    "    pattern = re.compile(regex)\n",
    "    matching_items = {key: value for key, value in dictionary.items() if pattern.match(key)}\n",
    "    return matching_items\n",
    "\n",
    "def create_events_mappings(trigger_map, case='RE'):\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    event_dict = {}\n",
    "    events_mapping = {}\n",
    "    new_response_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    if case == 'RE':\n",
    "        events_mapping = {\n",
    "            'correct_response': [],\n",
    "            'error_response': [],\n",
    "        }\n",
    "        # find response events from experimental blocks\n",
    "        regex_pattern = r'RE\\*ex\\*.*\\*num_separator'\n",
    "        event_dict = find_items_matching_regex(trigger_map_codes, regex_pattern)\n",
    "    \n",
    "        for event_id in event_dict.keys():\n",
    "            event_id_splitted = event_id.split('*')\n",
    "    \n",
    "            if (event_id_splitted[2][0] == event_id_splitted[3][0]) and (\n",
    "                    event_id_splitted[3][-1] == '1'):\n",
    "                events_mapping['correct_response'].append(event_dict[event_id])\n",
    "            else:\n",
    "                events_mapping['error_response'].append(event_dict[event_id])\n",
    "        \n",
    "    else:\n",
    "        print('Not implemented for feedback and stimuli')\n",
    "        \n",
    "    return event_dict, events_mapping, new_response_event_dict\n",
    "\n",
    "def create_epochs(\n",
    "        raw,\n",
    "        tmin=-.1,\n",
    "        tmax=.6,\n",
    "        events_to_select=None,  # event_dict\n",
    "        new_events_dict=None,  # new_event_dict\n",
    "        events_mapping=None,  # events_mapping\n",
    "        reject=None,\n",
    "        reject_by_annotation=False,\n",
    "):\n",
    "    # select specific events\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=events_to_select)\n",
    "\n",
    "    # Merge different events of one kind\n",
    "    for mapping in events_mapping:\n",
    "        events = mne.merge_events(\n",
    "            events=events,\n",
    "            ids=events_mapping[mapping],\n",
    "            new_id=new_events_dict[mapping],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=events,\n",
    "        event_id=new_events_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=reject_by_annotation,\n",
    "        preload=True,\n",
    "        reject=reject,\n",
    "        picks=['eeg', 'eog'],\n",
    "    )\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "\n",
    "def ocular_correction_ica(raw, raw_unfiltered, veog=[], heog=[], info=False, from_template='auto'):\n",
    "    # filtered_raw = raw.copy().filter(l_freq=1.0, h_freq=None)\n",
    "    # \n",
    "    # # peak-to-peak amplitude rejection parameters to exclude very noisy epochs\n",
    "    # epochs_ica = create_epochs(\n",
    "    #     filtered_raw,\n",
    "    #     tmin=-.4,\n",
    "    #     tmax=.8,\n",
    "    #     reject=dict(eeg=100e-5),\n",
    "    # )\n",
    "    # \n",
    "    # ica = mne.preprocessing.ICA(\n",
    "    #     n_components=36,\n",
    "    #     method='infomax',\n",
    "    #     max_iter=\"auto\",\n",
    "    #     random_state=random_state\n",
    "    # )\n",
    "    # ica.fit(epochs_ica)\n",
    "    # \n",
    "    # ica.exclude = []\n",
    "    # # find which ICs match the VEOG pattern\n",
    "    # eog_epochs = mne.preprocessing.create_eog_epochs(\n",
    "    #     filtered_raw,\n",
    "    #     tmin=-0.5, tmax=0.5\n",
    "    # )\n",
    "    # \n",
    "    # eog_epochs.plot_image(combine=\"mean\")\n",
    "    # eog_epochs.average().plot_joint()\n",
    "    # \n",
    "    # veog_indices, veog_scores = ica.find_bads_eog(\n",
    "    #     eog_epochs,\n",
    "    #     ch_name=veog,\n",
    "    #     threshold=0.9,\n",
    "    #     measure='correlation'\n",
    "    # )\n",
    "    # heog_indices, heog_scores = ica.find_bads_eog(\n",
    "    #     eog_epochs,\n",
    "    #     ch_name=heog,\n",
    "    #     threshold=0.8,\n",
    "    #     measure='correlation'\n",
    "    # )\n",
    "    # \n",
    "    # print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "    # \n",
    "    # ica.plot_components(veog_indices + heog_indices)\n",
    "    # \n",
    "    # ica.exclude = veog_indices + heog_indices\n",
    "    # \n",
    "    # reconstructed_epochs = epochs.copy()\n",
    "    # ica.apply(reconstructed_epochs)\n",
    "\n",
    "    filtered_raw_ica = raw_unfiltered.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "    ica = mne.preprocessing.ICA(\n",
    "        n_components=12,\n",
    "        method='infomax',\n",
    "        max_iter=\"auto\",\n",
    "        random_state=random_state\n",
    "    )\n",
    "    ica.fit(filtered_raw_ica)\n",
    "    \n",
    "    ica.exclude = []\n",
    "    \n",
    "    # find which ICs match the VEOG pattern\n",
    "    veog_indices, veog_scores = ica.find_bads_eog(\n",
    "        filtered_raw_ica,\n",
    "        ch_name=veog,\n",
    "        threshold=0.9,\n",
    "        measure='correlation'\n",
    "    )\n",
    "\n",
    "    # find which ICs match the HEOG pattern\n",
    "    heog_indices, heog_scores = ica.find_bads_eog(\n",
    "        filtered_raw_ica,\n",
    "        ch_name=heog,\n",
    "        threshold=0.7,\n",
    "        measure='correlation'\n",
    "    )   \n",
    "    \n",
    "    if info:\n",
    "        print('ICA components')\n",
    "        fig = ica.plot_components()\n",
    "        print(f\"VEOG indices: {veog_indices}\\nVEOG scores: {veog_scores}\\n\")\n",
    "        print(f\"HEOG indices: {heog_indices}\\nHEOG scores: {heog_scores}\\n\")\n",
    "  \n",
    "    if (len(veog_indices + heog_indices) == 0 and from_template == 'auto') or from_template == True:\n",
    "        print('Using templates...')\n",
    "        templates_ica = pd.read_pickle('data/eog_templates.pkl')\n",
    "\n",
    "        template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "        template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "        mne.preprocessing.corrmap(\n",
    "            [ica], \n",
    "            template=template_veog_component, \n",
    "            threshold=0.9, \n",
    "            label=\"veog blink\", \n",
    "            plot=False\n",
    "        )\n",
    "        mne.preprocessing.corrmap(\n",
    "            [ica], \n",
    "            template=template_heog_component, \n",
    "            threshold=0.8, \n",
    "            label=\"heog blink\", \n",
    "            plot=False\n",
    "        )\n",
    "\n",
    "        veog_indices = ica.labels_['veog blink']\n",
    "        heog_indices = ica.labels_['heog blink']\n",
    "\n",
    "    print(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "    fig = ica.plot_components(veog_indices + heog_indices)\n",
    "    \n",
    "    ica.exclude = veog_indices + heog_indices\n",
    "    \n",
    "    reconstructed_raw = raw.copy()\n",
    "    ica.apply(reconstructed_raw)\n",
    "    \n",
    "    del filtered_raw_ica\n",
    "\n",
    "    return reconstructed_raw\n",
    "\n",
    "\n",
    "def get_k_nearest_neighbors(target_ch_name, epochs, k=6):\n",
    "    \"\"\"\n",
    "    Finds k nearest neighbors of given channel according to the 3D channels positions from the Epoch INFO\n",
    "    :param target_ch_name: String\n",
    "        Name of the target channel.\n",
    "    :param epochs: mne Epochs\n",
    "        Epochs with info attribute that consists of channels positions.\n",
    "    :param k: int\n",
    "        Number of neighbors to use by default for kneighbors queries. \n",
    "    :return: \n",
    "        indices: ndarray of shape (n_neighbors)\n",
    "            Indices of the nearest channels.\n",
    "        neighbor_ch_names: ndarray of shape (n_neighbors)\n",
    "            Names of the nearest channels.   \n",
    "    \"\"\"\n",
    "    epochs_copy_eeg_channels = epochs.copy().pick('eeg')\n",
    "    info = epochs_copy_eeg_channels.info\n",
    "    ch_names = epochs_copy_eeg_channels.info['ch_names']\n",
    "    \n",
    "    chs = [info[\"chs\"][pick] for pick in np.arange(0,len(ch_names))]\n",
    "    electrode_positions_3d =[]\n",
    "    \n",
    "    for ch in chs:\n",
    "        electrode_positions_3d.append((ch['ch_name'], ch[\"loc\"][:3]))\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=k+1, algorithm='auto')\n",
    "    ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d])\n",
    "\n",
    "    neighbors_model.fit(ch_coordinates)\n",
    "\n",
    "    target_ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d if ch_name_coordinates[0] == target_ch_name])\n",
    "    \n",
    "    target_ch_coordinates = target_ch_coordinates.reshape(1,-1)\n",
    "\n",
    "    distances, indices = neighbors_model.kneighbors(target_ch_coordinates)\n",
    "    neighbor_ch_names = []\n",
    "    \n",
    "    # Print the nearest neighbors without the first (self) neighbor\n",
    "    # print(f\"{k} Nearest Neighbors of {target_ch_name}:\")\n",
    "    for i, (distance, index) in enumerate(zip(distances.flatten(), indices.flatten())):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            neighbor_point = electrode_positions_3d[index]\n",
    "            # print(f\"Neighbor {i + 1}: Index {index}, Distance {distance:.2f}, Coordinates {neighbor_point}\")\n",
    "            neighbor_ch_names.append(neighbor_point[0])\n",
    "            \n",
    "    return indices.flatten()[1:], np.array(neighbor_ch_names)\n",
    "\n",
    "\n",
    "def find_bad_trails(epochs):\n",
    "    \"\"\"\n",
    "    Channels that meet following conditions will be marked as bad for the trail:\n",
    "        (1) Channels with a voltage difference of 100 μV through the duration of the epoch;\n",
    "        (2) Channels that were flat;\n",
    "        (3) Channels with more than a 30 μV difference with the nearest six neighbors; \n",
    "    :param mne Epochs \n",
    "        Epochs to find bad channels per trial. \n",
    "    :return: drop_log: tuple of n_trials length\n",
    "        Tuple representing bad channels names per trial.\n",
    "    \"\"\"\n",
    "    epochs_copy = epochs.copy()\n",
    "    \n",
    "    # channels with a voltage difference of 100 μV through the duration of the epoch\n",
    "    reject_criteria = dict(eeg=100e-6)\n",
    "    # flat channels (less than 1 µV of peak-to-peak difference)\n",
    "    flat_criteria = dict(eeg=1e-6)\n",
    "    \n",
    "    epochs_copy.drop_bad(reject=reject_criteria, flat=flat_criteria)\n",
    "    drop_log = epochs_copy.drop_log\n",
    "\n",
    "    # channels with more than a 30 μV difference with the nearest six neighbors\n",
    "    for idx, _ in enumerate(epochs_copy):\n",
    "        epoch = epochs[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "        for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "            mean_channel_data = np.array(np.mean(epoch_data[0,ch_idx,:]))\n",
    "    \n",
    "            ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "                target_ch_name = ch_name,\n",
    "                epochs = epoch,\n",
    "                k=6\n",
    "            )\n",
    "    \n",
    "            mean_neighbors_data = np.array([np.mean(epoch_data[0, ch_neighbor_index, :]) \n",
    "                                            for ch_neighbor_index in ch_neighbors_indices])\n",
    "    \n",
    "            # # if channels has more than a 30 μV difference with the nearest six neighbors\n",
    "            if (abs(mean_neighbors_data - mean_channel_data) > 30e-6).all():\n",
    "                print(f'BAD------ trail index {idx}, channel: {ch_name}')\n",
    "                print(mean_channel_data)\n",
    "                print(mean_neighbors_data)\n",
    "    \n",
    "                new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx]\n",
    "                drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))\n",
    "\n",
    "    del epochs_copy\n",
    "    \n",
    "    return drop_log\n",
    "\n",
    "def calculate_percentage(tuple_of_tuples, element):\n",
    "    total_tuples = len(tuple_of_tuples)\n",
    "    # Avoid division by zero\n",
    "    if total_tuples == 0:\n",
    "        return 0 \n",
    "\n",
    "    tuples_with_element = sum(1 for inner_tuple in tuple_of_tuples if element in inner_tuple)\n",
    "    percentage = (tuples_with_element / total_tuples)\n",
    "    return percentage\n",
    "\n",
    "def find_global_bad_channels(epochs, drop_log):\n",
    "    '''\n",
    "    (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4;\n",
    "    (2) Channels that were marked as bad for more than 20% of epochs;\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :return: \n",
    "    '''\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True)\n",
    "    concatenated_epochs_data = np.concatenate(epochs_data, axis=1)\n",
    "    \n",
    "    global_bad_channels_drop_log = {}\n",
    "    \n",
    "    # (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4\n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        channel_data = concatenated_epochs_data[ch_idx]\n",
    "    \n",
    "        ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "            target_ch_name = ch_name,\n",
    "            epochs = epochs_copy,\n",
    "            k=6\n",
    "        )\n",
    "        channel_neighbors_data = np.array([concatenated_epochs_data[ch_neighbor_index]\n",
    "                                           for ch_neighbor_index in ch_neighbors_indices])\n",
    "        channels_corr = np.tril(np.corrcoef(channel_neighbors_data, channel_data), k=-1)\n",
    "    \n",
    "        if (abs(channels_corr[-1][:-1]) < .4).all():\n",
    "            print(f'BAD------ global channel: {ch_name}, low corr')\n",
    "            print(channels_corr[-1][:-1])\n",
    "    \n",
    "            # mark channel as globally bad\n",
    "            global_bad_channels_drop_log[ch_name] = ['LOW CORR NEIGH']\n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "    \n",
    "    # (2) Channels that were marked as bad for more than 20% of epochs   \n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        percentage = calculate_percentage(drop_log, ch_name)\n",
    "    \n",
    "        if percentage > 0.2:\n",
    "            print(f'BAD------ global channel: {ch_name}, percentage: {percentage}')\n",
    "    \n",
    "            if ch_name in global_bad_channels_drop_log:\n",
    "                global_bad_channels_drop_log[ch_name].append('BAD FOR MORE THAN 20%')\n",
    "            else:\n",
    "                global_bad_channels_drop_log[ch_name] = ['BAD FOR MORE THAN 20%']\n",
    "    \n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "            \n",
    "    del epochs_copy\n",
    "            \n",
    "    return drop_log, global_bad_channels_drop_log\n",
    "\n",
    "def mark_bad_trials(epochs, drop_log, threshold=0.1):\n",
    "    \"\"\"\n",
    "    If more than 10% of channels were marked bad for an epoch (trial), the entire epoch was rejected\n",
    "    :param drop_log: \n",
    "    :return: trials_to_drop: ndarray\n",
    "    '\"\"\"\n",
    "    trials_to_drop_indices = []\n",
    "    epochs_copy = epochs.copy()\n",
    "\n",
    "    if len(epochs_copy) == len(drop_log):\n",
    "        threshold_items = int(threshold * len(epochs_copy.info['ch_names']))\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if len(item) > threshold_items:\n",
    "                print(f'BAD TRAIL (exceeded threshold for bad channels): {idx}')\n",
    "                trials_to_drop_indices.append(idx)\n",
    "        \n",
    "        # update drop_log\n",
    "        for trial_idx in trials_to_drop_indices:\n",
    "            drop_log = tuple(drop_log[i] + ('TO DROP',) if i == trial_idx else item for i, item in enumerate(drop_log))\n",
    "\n",
    "    else:\n",
    "        print(f'Epochs length is not equal drop_log length:\\nepochs: {len(epochs)}\\ndrop_log: {len(drop_log)}')\n",
    "    \n",
    "    return trials_to_drop_indices, drop_log\n",
    "\n",
    "def reject_bad_trials(epochs, drop_log, trials_to_drop_indices=None):\n",
    "    clean_epochs = epochs.copy()\n",
    "    \n",
    "    clean_epochs = clean_epochs.drop(\n",
    "        indices = trials_to_drop_indices,\n",
    "        reason = 'MORE THAN 10% CHANNELS MARKED AS BAD',\n",
    "    )\n",
    "    \n",
    "    # update drop_log\n",
    "    for trial_idx in trials_to_drop_indices:\n",
    "        drop_log = tuple(('REJECTED',) if i == trial_idx else element for i, element in enumerate(drop_log))\n",
    "    \n",
    "    return clean_epochs, drop_log\n",
    "\n",
    "def interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log):\n",
    "    \"\"\"\n",
    "    Bad channels were interpolated using spherical splines\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :param global_bad_channels_drop_log: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_copy.info['bads'] = list(global_bad_channels_drop_log.keys())\n",
    "    epochs_interpolated_bad_channels = epochs_copy.interpolate_bads(method='spline')\n",
    "\n",
    "    # update drop log to remove interpolated channels\n",
    "    updated_drop_log = drop_log\n",
    "    for ch_name in list(global_bad_channels_drop_log.keys()):\n",
    "        updated_drop_log = tuple(tuple(element for element in drop_log_item if element != ch_name) for drop_log_item in updated_drop_log)\n",
    "    \n",
    "    return epochs_interpolated_bad_channels, updated_drop_log\n",
    "\n",
    "def create_erps_waves(epochs, drop_log, type = 'error_response', tmin=0, tmax=0.1, picks=['FCz']):\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True, picks=picks, tmin=tmin, tmax=tmax)\n",
    "    events = epochs_copy.events\n",
    "\n",
    "    eeg_data = []\n",
    "\n",
    "    if (len(epochs_data) == len(drop_log)) & (len(events) == len(drop_log)):\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if events[idx][-1] == new_response_event_dict[type]:\n",
    "                if ('TO DROP' in item) or any(element in item for element in picks):\n",
    "                    pass\n",
    "                else:\n",
    "                    eeg_data.append(epochs_data[idx])\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(eeg_data)\n",
    "    else:\n",
    "        print(f'Epochs length is not equal to drop_log length:\\nepochs: {len(epochs_data)}\\ndrop_log: {len(drop_log)}')\n",
    "        return None\n",
    "\n",
    "def create_erps(epochs, drop_log, type = 'error_response', tmin=0, tmax=0.1, picks=['FCz']):\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True, picks=picks, tmin=tmin, tmax=tmax)\n",
    "    events = epochs_copy.events\n",
    "\n",
    "    erps_data = []\n",
    "\n",
    "    if (len(epochs_data) == len(drop_log)) & (len(events) == len(drop_log)):\n",
    "        for idx, item in enumerate(drop_log):\n",
    "            if events[idx][-1] == new_response_event_dict[type]:\n",
    "                if ('TO DROP' in item) or any(element in item for element in picks):\n",
    "                    erps_data.append([None])\n",
    "                else:\n",
    "                    erps_data.append(np.mean(epochs_data[idx], axis=-1))\n",
    "            else:\n",
    "                pass\n",
    "        return np.array(erps_data)\n",
    "    else:\n",
    "        print(f'Epochs length is not equal to drop_log length:\\nepochs: {len(epochs_data)}\\ndrop_log: {len(drop_log)}')\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc62ce10ab154b16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pre_process_eeg(input_fname, trigger_fname=None, parameters=None):\n",
    "    # 0. read bdf\n",
    "    raw = mne.io.read_raw_bdf(\n",
    "        input_fname,\n",
    "        eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "        exclude=['EXG5', 'EXG6'],\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        raw = raw.set_montage('biosemi64')\n",
    "    except ValueError as e:\n",
    "        if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "            raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "            print('On missing')\n",
    "        else:\n",
    "            print('Lacks important channels!')\n",
    "    \n",
    "    # 1. replace trigger names\n",
    "    trigger_map = read_trigger_map(trigger_fname)\n",
    "    raw_new_triggers = replace_trigger_names(raw, trigger_map)\n",
    "\n",
    "    # 2. re-reference: to mastoids\n",
    "    raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "\n",
    "    # 3. 4-th order Butterworth filters\n",
    "    raw_filtered = raw_ref.copy().filter(\n",
    "        l_freq=.1,\n",
    "        h_freq=30.0,\n",
    "        n_jobs=10,\n",
    "        method='iir',\n",
    "        iir_params=None,\n",
    "    )\n",
    "\n",
    "    # 4. Notch filter at 50 Hz\n",
    "    raw_filtered = raw_filtered.notch_filter(\n",
    "        freqs=np.arange(50, (raw_filtered.info['sfreq'] / 2), 50),\n",
    "        n_jobs=10,\n",
    "    )\n",
    "    \n",
    "    # 5. ocular correction with ICA\n",
    "    raw_corrected_eogs = ocular_correction_ica(\n",
    "        raw_filtered, \n",
    "        raw_ref, \n",
    "        heog=['EXG3', 'EXG4'], \n",
    "        veog=['EXG1', 'EXG2'], \n",
    "        from_template=True, \n",
    "        info=True,\n",
    "    )\n",
    "\n",
    "    # 6. segmentation -400 to 800 ms around the response\n",
    "    raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "    epochs = create_epochs(\n",
    "        raw_corrected_eogs_drop_ref,\n",
    "        tmin=-.4,\n",
    "        tmax=.8,\n",
    "        events_to_select=response_event_dict,  # response_event_dict\n",
    "        new_events_dict=new_response_event_dict,  # new_response_event_dict\n",
    "        events_mapping=response_events_mapping,  # events_mapping\n",
    "        reject=None,\n",
    "        reject_by_annotation=False,\n",
    "    )\n",
    "\n",
    "    # 7. Trial-wise Bad Channels Identification\n",
    "    drop_log = find_bad_trails(epochs)\n",
    "\n",
    "    # 8. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "    drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)\n",
    "\n",
    "    # 9. calculate trails to remove\n",
    "    trials_to_drop_indices, drop_log = mark_bad_trials(epochs, drop_log, threshold=0.1)\n",
    "\n",
    "    # 10. Interpolate bad channels (and thus update drop log)\n",
    "    interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)\n",
    "\n",
    "    # 11. Remove participants that have less then 6 trials\n",
    "    clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "    if len(clean_epochs) < 6:\n",
    "        print(f\"Participant ID: {id} has not enough clean trials\")\n",
    "        return []\n",
    "\n",
    "    # 13. Baseline correction\n",
    "    interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)\n",
    "\n",
    "    return interpolated_epochs, drop_log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edea2632f8b5d423"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log_separately(epochs, drop_log, id):\n",
    "    # save drop_log\n",
    "    with open(f'{preprocessed_data_dir_path}drop_log_{id}.json', 'w') as fjson:\n",
    "        json.dump(drop_log, fjson)\n",
    "\n",
    "    # save Epoch object\n",
    "    epochs.save(f'{preprocessed_data_dir_path}preprocessed_{id}-epo.fif', overwrite=True)\n",
    "\n",
    "    return print('Epochs saved to fif. Drop log saved to json.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be2082a55bf73ab7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log(epochs, drop_log, id):\n",
    "    item = pd.DataFrame({\n",
    "        'epochs': [epochs],\n",
    "        'drop_log': [drop_log],\n",
    "    })\n",
    "    \n",
    "    item.to_pickle(f'{preprocessed_data_dir_path}preprocessed_{id}.pkl')\n",
    "    \n",
    "    return print('Epochs saved to pickle.')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2431faf1c3221a26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_behavioral_file(id):\n",
    "    \n",
    "    behavioral_data_df = pd.read_csv(f'{behavioral_dir_path}beh_{id}.csv')\n",
    "\n",
    "    trial_numerator = 1\n",
    "    trial_numbers = []\n",
    "    for i in range(0, len(behavioral_data_df)):\n",
    "        if behavioral_data_df.iloc[i]['block type'] != 'experiment':\n",
    "            trial_numbers.append(0)\n",
    "        else:\n",
    "            trial_numbers.append(trial_numerator)\n",
    "            trial_numerator+=1\n",
    "    \n",
    "    behavioral_data_df['trial number'] = trial_numbers\n",
    "    return behavioral_data_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6002388a62c3c418"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_epochs_with_behavioral_data_long(epochs, drop_log, id, case='RE'):\n",
    "    \n",
    "    # read behavioral file\n",
    "    behavioral_data_df = read_behavioral_file(id)\n",
    "\n",
    "    beh_data_uninhibited_nogo_responses_df = behavioral_data_df[\n",
    "        (behavioral_data_df['block type'] == 'experiment') &\n",
    "        (behavioral_data_df['trial type'] != 'go') &\n",
    "        (behavioral_data_df['reaction'] == False)\n",
    "        ]\n",
    "    print(f'Number of uninhibited NOGO trials: {len(beh_data_uninhibited_nogo_responses_df)}')\n",
    "    \n",
    "    beh_data_inhibited_nogo_responses_df = behavioral_data_df[\n",
    "        (behavioral_data_df['block type'] == 'experiment') &\n",
    "        (behavioral_data_df['trial type'] != 'go') &\n",
    "        (behavioral_data_df['reaction'] == True)\n",
    "        ]\n",
    "    print(f'Number of inhibited NOGO trials: {len(beh_data_inhibited_nogo_responses_df)}')\n",
    "    \n",
    "    beh_data_correct_go_responses_df = behavioral_data_df[\n",
    "        (behavioral_data_df['block type'] == 'experiment') &\n",
    "        (behavioral_data_df['trial type'] == 'go') &\n",
    "        (behavioral_data_df['response'] == 'num_separator')\n",
    "        ]\n",
    "    print(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "    results_df = pd.DataFrame()\n",
    "    epochs_df = pd.DataFrame()\n",
    "    behavioral_df = pd.DataFrame()\n",
    "    if case == 'RE':\n",
    "        behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "        \n",
    "        print(f'Len drop log: {len(drop_log)}')\n",
    "        print(f'Len behavioral df: {len(behavioral_df)}')\n",
    "        assert len(behavioral_df) == len(drop_log)\n",
    "                \n",
    "        for idx, _ in enumerate(epochs):\n",
    "            epoch = epochs[idx]\n",
    "            epoch_type = list(epoch.event_id.keys())\n",
    "            assert len(epoch_type) == 1\n",
    "            drop_log_item = drop_log[idx]\n",
    "            \n",
    "            this_df = pd.DataFrame({\n",
    "                'epoch': [epoch],\n",
    "                'event': epoch_type,\n",
    "                'drop_log': [drop_log_item],\n",
    "            })\n",
    "\n",
    "            epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "        # epochs_df.reset_index(drop=True, inplace=True)\n",
    "        indexes = behavioral_df.index\n",
    "        # Set the indexes of epochs to match reactions\n",
    "        epochs_df.set_index(indexes, inplace=True)\n",
    "        results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "        \n",
    "    else:\n",
    "        print('Not implemented')\n",
    "\n",
    "    assert len(results_df) == len(behavioral_df) == len(epochs_df)\n",
    "\n",
    "    results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{id}.pkl')\n",
    "    print('Epochs and behavioral data in long format saved to pickle.')\n",
    "    \n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e8e0c957e9e500d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a429187fd12024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id = 'B-GNG-002'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2015d4a86f612fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bdf_fname = f'{bdf_dir_path}{id}.bdf' \n",
    "trigger_fname = f'{trigger_dir_path}triggerMap_{id}.txt'\n",
    "\n",
    "trigger_map = read_trigger_map(trigger_fname)\n",
    "response_event_dict, response_events_mapping, new_response_event_dict = create_events_mappings(trigger_map)\n",
    "\n",
    "print(response_event_dict)\n",
    "print(response_events_mapping)\n",
    "print(new_response_event_dict)\n",
    "\n",
    "epochs_preprocessed, drop_log = pre_process_eeg(\n",
    "    input_fname=bdf_fname,\n",
    "    trigger_fname=trigger_fname\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a3c545bf3cad5b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_epochs_with_drop_log(epochs_preprocessed, drop_log, id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "209e083dc6d43b68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_epochs_with_drop_log_separately(epochs_preprocessed, drop_log, id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710fde14cb77663d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = save_epochs_with_behavioral_data_long(\n",
    "    epochs_preprocessed, \n",
    "    drop_log, \n",
    "    id,\n",
    "    case='RE',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ecccbfc4463e5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b6ad8c6e166a8a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id = 'A-GNG-219'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41121cf44e8b472a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs_df = pd.read_pickle(f'{preprocessed_data_dir_path}preprocessed_{id}.pkl')\n",
    "epochs = epochs_df['epochs'].to_numpy().flatten()[0]\n",
    "drop_log = epochs_df['drop_log'].to_numpy().flatten()[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65258cf1cbc6fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_wave = create_erps_waves(epochs, drop_log, type='error_response', tmin=-0.1, tmax=0.6, picks=['FCz'])\n",
    "correct_wave = create_erps_waves(epochs, drop_log, type='correct_response', tmin=-0.1, tmax=0.6, picks=['FCz'])\n",
    "\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten())\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(correct_wave, axis=0).flatten())), np.mean(correct_wave, axis=0).flatten())\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a4bd4a3b107d0b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_wave.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b86a0d7c352570e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ERPs scoring\n",
    "ern_single_trials = create_erps(epochs_preprocessed, drop_log, type='error_response', tmin=0, tmax=0.1, picks=['FCz'])\n",
    "pe_single_trials = create_erps(epochs_preprocessed, drop_log, type='error_response', tmin=0.2, tmax=0.4, picks=['Pz'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4294fd3733c3d47e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## For testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb751664fc106984"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_fname = 'data/raw/A-GNG-000.bdf'\n",
    "raw = mne.io.read_raw_bdf(\n",
    "    input_fname, \n",
    "    eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'], \n",
    "    exclude=['EXG5', 'EXG6'], \n",
    "    preload=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    raw = raw.set_montage('biosemi64')\n",
    "except ValueError as e:\n",
    "    if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "        raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "        print('On missing')\n",
    "    else:\n",
    "        print('Lacks important channels!')\n",
    "\n",
    "\n",
    "file_path = 'data/raw/triggerMap_A-GNG-000.txt'\n",
    "trigger_map = read_trigger_map(file_path)\n",
    "raw_new_triggers = replace_trigger_names(raw, trigger_map)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24a0c51998a3561b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. re-reference: to mastoids\n",
    "raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "# fig = raw_ref.plot(start=60, duration=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80136436d945d215"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. 4-th order Butterworth filters\n",
    "raw_filtered = raw_ref.copy().filter(\n",
    "    l_freq=.1,\n",
    "    h_freq=30.0,\n",
    "    n_jobs=10,\n",
    "    method='iir',\n",
    "    iir_params=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bc89381c09684f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Notch filter at 50 Hz\n",
    "raw_filtered = raw_filtered.notch_filter(\n",
    "    freqs=np.arange(50, 251, 50),\n",
    "    n_jobs=10,\n",
    "    # method='iir',\n",
    "    # iir_params=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "797466c92a2ed62f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. ocular artifact correction with ICA\n",
    "# raw_corrected_eogs = ocular_correction_ica(raw_filtered, raw_ref, heog=['EXG3', 'EXG4'], veog=['EXG1', 'EXG2'])\n",
    "heog=['EXG3', 'EXG4']\n",
    "veog=['EXG1', 'EXG2']\n",
    "filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "ica = mne.preprocessing.ICA(\n",
    "    n_components=12,\n",
    "    method='infomax',\n",
    "    max_iter=\"auto\",\n",
    "    random_state=random_state\n",
    ")\n",
    "ica.fit(filtered_raw_ica)\n",
    "\n",
    "ica.exclude = []\n",
    "\n",
    "# find which ICs match the VEOG pattern\n",
    "veog_indices, veog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=veog,\n",
    "    threshold=0.9,\n",
    "    measure='correlation'\n",
    ")\n",
    "\n",
    "# find which ICs match the HEOG pattern\n",
    "heog_indices, heog_scores = ica.find_bads_eog(\n",
    "    filtered_raw_ica,\n",
    "    ch_name=heog,\n",
    "    threshold=0.7,\n",
    "    measure='correlation'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1b9034b1d9f83e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('ICA components')\n",
    "fig = ica.plot_components()\n",
    "\n",
    "print(f\"VEOG indices: {veog_indices}\\nVEOG scores: {veog_scores}\\n\")\n",
    "print(f\"HEOG indices: {heog_indices}\\nHEOG scores: {heog_scores}\\n\")\n",
    "\n",
    "\n",
    "print(f'Excluded ICA components:\\nVEOG: {veog_indices}\\nHEOG: {heog_indices}')\n",
    "fig = ica.plot_components(veog_indices + heog_indices)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598e52e31553fb12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if len(veog_indices + heog_indices) == 0 and from_template == 'auto':\n",
    "# print('No ICA component correlates with EOG channels. Using templates...')\n",
    "# ica_template = pd.read_pickle('data/eog_templates.pkl')\n",
    "# veog_template = ica_template[['VEOG']].to_numpy()\n",
    "# heog_template = ica_template[['HEOG']].to_numpy()\n",
    "# \n",
    "# mne.preprocessing.corrmap([ica], template=veog_template, threshold=0.9)\n",
    "# mne.preprocessing.corrmap([ica], template=heog_template, threshold=0.8)\n",
    "\n",
    "\n",
    "# elif from_template is True:\n",
    "#     print('Using templates...')\n",
    "\n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw.copy()\n",
    "# ica.apply(reconstructed_raw)\n",
    "\n",
    "template_veog_component = ica.get_components()[:, veog_indices[0]]\n",
    "template_heog_component = ica.get_components()[:, heog_indices[0]]\n",
    "\n",
    "templates_ica = pd.DataFrame({\n",
    "    'VEOG': [template_veog_component],\n",
    "    'HEOG': [template_heog_component],\n",
    "})\n",
    "templates_ica.to_pickle('data/eog_templates.pkl')\n",
    "#########\n",
    "templates_ica = pd.read_pickle('data/eog_templates.pkl')\n",
    "\n",
    "template_veog_component = templates_ica['VEOG'].to_numpy()[0].flatten()\n",
    "template_heog_component = templates_ica['HEOG'].to_numpy()[0].flatten()\n",
    "\n",
    "\n",
    "mne.preprocessing.corrmap([ica], template=template_veog_component, threshold=0.9, label=\"veog blink\", plot=False)\n",
    "mne.preprocessing.corrmap([ica], template=template_heog_component, threshold=0.8, label=\"heog blink\", plot=False)\n",
    "\n",
    "to_exclude = ica.labels_['veog blink'] + ica.labels_['heog blink']\n",
    "print(to_exclude)\n",
    "\n",
    "\n",
    "\n",
    "# del filtered_raw_ica\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b9a8dc7e7e4174"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ica.labels_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c13a762401f4206c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_corrected_eogs_drop_ref = raw_corrected_eogs.copy().drop_channels(['EXG7', 'EXG8']).pick('eeg')\n",
    "\n",
    "epochs = create_epochs(\n",
    "    raw_corrected_eogs_drop_ref, \n",
    "    tmin=-.4, \n",
    "    tmax=.8,\n",
    "    events_to_select=response_event_dict,  # response_event_dict\n",
    "    new_events_dict=new_response_event_dict,  # new_response_event_dict\n",
    "    events_mapping=events_mapping,  # events_mapping\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    ")\n",
    "\n",
    "fig = epochs.plot(n_epochs=20, n_channels=68)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "173d5e9b55631419"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. Trial-wise Bad Channels Identification\n",
    "drop_log = find_bad_trails(epochs) \n",
    "drop_log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da372e30630abee1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a08ff9538219b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8. calculate trails to remove\n",
    "trials_to_drop_indices, drop_log = calculate_bad_trials(drop_log, threshold=0.1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29521e825f342e62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10. Interpolate bad channels (and thus update drop log)\n",
    "interpolated_epochs, drop_log = interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28fbd355474b393"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 12. Remove participants that have less then 6 trials\n",
    "clean_epochs, _ = reject_bad_trials(interpolated_epochs.copy(), drop_log, trials_to_drop_indices)\n",
    "if len(clean_epochs) < 6:\n",
    "      print(f\"Participant ID: {id} has not enough clean trials\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c235a759bbd5c9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 13. Baseline correction\n",
    "interpolated_epochs.apply_baseline(baseline=(-0.4, -0.2),)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1589b68b16f1edd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drop_log"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3af257db7dda387d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 13. ERPs scoring\n",
    "ern_single_trials = create_erps(interpolated_epochs, drop_log, type='error_response', tmin=0, tmax=0.1, picks=['FCz'])\n",
    "pe_single_trials = create_erps(interpolated_epochs, drop_log, type='error_response', tmin=0.2, tmax=0.4, picks=['Pz'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec31ac2c89fe2836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error_wave = create_erps_waves(interpolated_epochs, drop_log, type='error_response', tmin=-0.1, tmax=0.6, picks=['FCz'])\n",
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten(), )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "923162a9bc466d5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-0.1, 0.6, len(np.mean(error_wave, axis=0).flatten())), np.mean(error_wave, axis=0).flatten(), )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0efeff532643e93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filtered_raw_ica = raw_ref.copy().drop_channels(['EXG7', 'EXG8']).filter(l_freq=1.0, h_freq=None)\n",
    "# \n",
    "# ica = mne.preprocessing.ICA(\n",
    "#     n_components=12,\n",
    "#     method='infomax',\n",
    "#     max_iter=\"auto\",\n",
    "#     random_state=random_state\n",
    "# )\n",
    "# ica.fit(filtered_raw_ica)\n",
    "# \n",
    "# ica.exclude = []\n",
    "# heog=['EXG3', 'EXG4']\n",
    "# veog=['EXG1', 'EXG2']\n",
    "# \n",
    "# # find which ICs match the VEOG pattern\n",
    "# veog_indices, veog_scores = ica.find_bads_eog(\n",
    "#     filtered_raw_ica,\n",
    "#     ch_name=veog,\n",
    "#     threshold=0.9,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# # \n",
    "# print(f'{veog_indices}')\n",
    "# \n",
    "# heog_indices, heog_scores = ica.find_bads_eog(\n",
    "#     filtered_raw_ica,\n",
    "#     ch_name=heog,\n",
    "#     threshold=0.7,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# print(f'{heog_indices}')\n",
    "# \n",
    "# \n",
    "# print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "# \n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw_filtered.copy()\n",
    "# ica.apply(reconstructed_raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "643b4bb1037c5b3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ica.exclude = []\n",
    "# heog=['EXG3', 'EXG4']\n",
    "# veog=['EXG1', 'EXG2']\n",
    "# \n",
    "# # find which ICs match the VEOG pattern\n",
    "# veog_indices, veog_scores = ica.find_bads_eog(\n",
    "#     raw_ref,\n",
    "#     ch_name=veog,\n",
    "#     threshold=0.9,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# # \n",
    "# print(f'{veog_indices}')\n",
    "# \n",
    "# heog_indices, heog_scores = ica.find_bads_eog(\n",
    "#     raw_ref,\n",
    "#     ch_name=heog,\n",
    "#     threshold=0.7,\n",
    "#     measure='correlation'\n",
    "# )\n",
    "# print(f'{heog_indices}')\n",
    "# \n",
    "#  \n",
    "# print(f'Excluded ICA components: {veog_indices + heog_indices}')\n",
    "# fig = ica.plot_components(veog_indices + heog_indices)\n",
    "# \n",
    "# ica.exclude = veog_indices + heog_indices\n",
    "# # \n",
    "# reconstructed_raw = raw_filtered.copy()\n",
    "# ica.apply(reconstructed_raw)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45b5b98ffe961431"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(heog_scores)\n",
    "print(veog_scores)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3e3f1c5698c88bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
