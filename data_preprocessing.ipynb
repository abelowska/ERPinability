{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-06T10:59:57.058431Z",
     "start_time": "2023-12-06T10:59:57.054384Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "class DifferentLengthsError(Exception):\n",
    "    \"\"\"Exception raised for errors when lengths of trigger map and events are different.\n",
    "\n",
    "    Attributes:\n",
    "        trigger_map_len -- length of the trigger map\n",
    "        events_len -- length of the events\n",
    "        message -- explanation of the error\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trigger_map_len, events_len, message=\"Trigger map and events have different lengths.\"):\n",
    "        self.trigger_map_len = trigger_map_len\n",
    "        self.events_len = events_len\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.message} Trigger map length: {self.trigger_map_len}, Events length: {self.events_len}\"\n",
    "\n",
    "\n",
    "def replace_trigger_names(raw, trigger_map=None):\n",
    "    # Replace event IDs in the Raw object\n",
    "    events = raw.annotations.description\n",
    "    if len(events) != len(trigger_map):\n",
    "        raise DifferentLengthsError(len(trigger_map), len(events))\n",
    "    else:\n",
    "        for idx, event in enumerate(events):\n",
    "            raw.annotations.description[idx] = trigger_map[idx]\n",
    "        return raw\n",
    "\n",
    "\n",
    "def create_epochs(\n",
    "        raw,\n",
    "        tmin=-.1,\n",
    "        tmax=.6,\n",
    "        events_to_select=None,  # response_event_dict\n",
    "        new_events_dict=None,  # new_response_event_dict\n",
    "        events_mapping=None,  # events_mapping\n",
    "        reject=None,\n",
    "        reject_by_annotation=False\n",
    "):\n",
    "    # select specific events\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=events_to_select)\n",
    "\n",
    "    # Merge different events of one kind\n",
    "    for mapping in events_mapping:\n",
    "        events = mne.merge_events(\n",
    "            events=events,\n",
    "            ids=events_mapping[mapping],\n",
    "            new_id=new_events_dict[mapping],\n",
    "            replace_events=True,\n",
    "        )\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=events,\n",
    "        event_id=new_events_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=reject_by_annotation,\n",
    "        preload=True,\n",
    "        reject=reject,\n",
    "    )\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "\n",
    "def ocular_correction_ica(epochs, raw, veog=[], heog=[]):\n",
    "    filtered_raw = raw.copy().filter(l_freq=1.0, h_freq=None)\n",
    "\n",
    "    # peak-to-peak amplitude rejection parameters to exclude very noisy epochs\n",
    "    epochs_ica = create_epochs(\n",
    "        filtered_raw,\n",
    "        tmin=-.4,\n",
    "        tmax=.8,\n",
    "        reject=dict(mag=4e-12),\n",
    "    )\n",
    "\n",
    "    ica = mne.preprocessing.ICA(\n",
    "        n_components=36,\n",
    "        method='infomax',\n",
    "        max_iter=\"auto\",\n",
    "        random_state=97\n",
    "    )\n",
    "    ica.fit(epochs_ica)\n",
    "\n",
    "    ica.exclude = []\n",
    "    # find which ICs match the VEOG pattern\n",
    "    veog_indices, veog_scores = ica.find_bads_eog(\n",
    "        epochs,\n",
    "        ch_name=veog,\n",
    "        threshold=0.9,\n",
    "        measure='correlation'\n",
    "    )\n",
    "    heog_indices, heog_scores = ica.find_bads_eog(\n",
    "        epochs,\n",
    "        ch_name=heog,\n",
    "        threshold=0.8,\n",
    "        measure='correlation'\n",
    "    )\n",
    "\n",
    "    ica.exclude = veog_indices + heog_indices\n",
    "\n",
    "    reconstructed_epochs = epochs.copy()\n",
    "    ica.apply(reconstructed_epochs)\n",
    "\n",
    "    del filtered_raw, epochs_ica\n",
    "\n",
    "    return reconstructed_epochs\n",
    "\n",
    "\n",
    "def get_k_nearest_neighbors(target_ch_name, epochs, k=6):\n",
    "    \"\"\"\n",
    "    Finds k nearest neighbors of given channel according to the 3D channels positions from the Epoch INFO\n",
    "    :param target_ch_name: String\n",
    "        Name of the target channel.\n",
    "    :param epochs: mne Epochs\n",
    "        Epochs with info attribute that consists of channels positions.\n",
    "    :param k: int\n",
    "        Number of neighbors to use by default for kneighbors queries. \n",
    "    :return: \n",
    "        indices: ndarray of shape (n_neighbors)\n",
    "            Indices of the nearest channels.\n",
    "        neighbor_ch_names: ndarray of shape (n_neighbors)\n",
    "            Names of the nearest channels.   \n",
    "    \"\"\"\n",
    "    info = epochs.info\n",
    "    ch_names = epochs.info['ch_names']\n",
    "    \n",
    "    chs = [info[\"chs\"][pick] for pick in np.arange(0,len(ch_names))]\n",
    "    electrode_positions_3d =[]\n",
    "    \n",
    "    for ch in chs:\n",
    "        electrode_positions_3d.append((ch['ch_name'], ch[\"loc\"][:3]))\n",
    "    \n",
    "    neighbors_model = NearestNeighbors(n_neighbors=k+1, algorithm='auto')\n",
    "    ch_coordinates = np.array([ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d])\n",
    "\n",
    "    neighbors_model.fit(ch_coordinates)\n",
    "\n",
    "    target_ch_coordinates = [ch_name_coordinates[1] for ch_name_coordinates in electrode_positions_3d if ch_name_coordinates[0] == target_ch_name]\n",
    "\n",
    "    distances, indices = neighbors_model.kneighbors(target_ch_coordinates)\n",
    "    neighbor_ch_names = []\n",
    "    \n",
    "    # Print the nearest neighbors without the first (self) neighbor\n",
    "    # print(f\"{k} Nearest Neighbors of {target_ch_name}:\")\n",
    "    for i, (distance, index) in enumerate(zip(distances.flatten(), indices.flatten())):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            neighbor_point = electrode_positions_3d[index]\n",
    "            # print(f\"Neighbor {i + 1}: Index {index}, Distance {distance:.2f}, Coordinates {neighbor_point}\")\n",
    "            neighbor_ch_names.append(neighbor_point[0])\n",
    "            \n",
    "    return indices.flatten()[1:], np.array(neighbor_ch_names)\n",
    "\n",
    "\n",
    "def find_bad_trails(epochs):\n",
    "    \"\"\"\n",
    "    Channels that meet following conditions will be marked as bad for the trail:\n",
    "        (1) Channels with a voltage difference of 100 μV through the duration of the epoch;\n",
    "        (2) Channels that were flat;\n",
    "        (3) Channels with more than a 30 μV difference with the nearest six neighbors; \n",
    "    :param mne Epochs \n",
    "        Epochs to find bad channels per trial. \n",
    "    :return: drop_log: tuple of n_trials length\n",
    "        Tuple representing bad channels names per trial.\n",
    "    \"\"\"\n",
    "    epochs_copy = epochs.copy()\n",
    "    \n",
    "    # channels with a voltage difference of 100 μV through the duration of the epoch\n",
    "    reject_criteria = dict(eeg=100e-6)\n",
    "    # flat channels (less than 1 µV of peak-to-peak difference)\n",
    "    flat_criteria = dict(eeg=1e-6)\n",
    "    \n",
    "    epochs_copy.drop_bad(reject=reject_criteria, flat=flat_criteria)\n",
    "    drop_log = epochs_copy.drop_log\n",
    "\n",
    "    # channels with more than a 30 μV difference with the nearest six neighbors\n",
    "    for idx, _ in enumerate(epochs_copy):\n",
    "        epoch = epochs[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "        for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "            mean_channel_data = np.array(np.mean(epoch_data[0,ch_idx,:]))\n",
    "    \n",
    "            ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "                target_ch_name = ch_name,\n",
    "                epochs = epoch,\n",
    "                k=6\n",
    "            )\n",
    "    \n",
    "            mean_neighbors_data = np.array([np.mean(epoch_data[0, ch_neighbor_index, :]) \n",
    "                                            for ch_neighbor_index in ch_neighbors_indices])\n",
    "    \n",
    "            # # if channels has more than a 30 μV difference with the nearest six neighbors\n",
    "            if (abs(mean_neighbors_data - mean_channel_data) > 30e-6).all():\n",
    "                print(f'BAD------ trail index {idx}, channel: {ch_name}')\n",
    "                print(mean_channel_data)\n",
    "                print(mean_neighbors_data)\n",
    "    \n",
    "                new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx]\n",
    "                drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))\n",
    "\n",
    "    del epochs_copy\n",
    "    \n",
    "    return drop_log\n",
    "\n",
    "def calculate_percentage(tuple_of_tuples, element):\n",
    "    total_tuples = len(tuple_of_tuples)\n",
    "    # Avoid division by zero\n",
    "    if total_tuples == 0:\n",
    "        return 0 \n",
    "\n",
    "    tuples_with_element = sum(1 for inner_tuple in tuple_of_tuples if element in inner_tuple)\n",
    "    percentage = (tuples_with_element / total_tuples)\n",
    "    return percentage\n",
    "\n",
    "def find_global_bad_channels(epochs, drop_log):\n",
    "    '''\n",
    "    (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4;\n",
    "    (2) Channels that were marked as bad for more than 20% of epochs;\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :return: \n",
    "    '''\n",
    "    epochs_copy = epochs.copy()\n",
    "    epochs_data = epochs_copy.get_data(copy=True)\n",
    "    concatenated_epochs_data = np.concatenate(epochs_data, axis=1)\n",
    "    \n",
    "    global_bad_channels_drop_log = {}\n",
    "    \n",
    "    # (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4\n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        channel_data = concatenated_epochs_data[ch_idx]\n",
    "    \n",
    "        ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "            target_ch_name = ch_name,\n",
    "            epochs = epochs_copy,\n",
    "            k=6\n",
    "        )\n",
    "        channel_neighbors_data = np.array([concatenated_epochs_data[ch_neighbor_index]\n",
    "                                           for ch_neighbor_index in ch_neighbors_indices])\n",
    "        channels_corr = np.tril(np.corrcoef(channel_neighbors_data, channel_data), k=-1)\n",
    "    \n",
    "        if (abs(channels_corr[-1][:-1]) < .4).all():\n",
    "            print(f'BAD------ global channel: {ch_name}, low corr')\n",
    "            print(channels_corr[-1][:-1])\n",
    "    \n",
    "            # mark channel as globally bad\n",
    "            global_bad_channels_drop_log[ch_name] = ['LOW CORR NEIGH']\n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "    \n",
    "    # (2) Channels that were marked as bad for more than 20% of epochs   \n",
    "    for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "        percentage = calculate_percentage(drop_log, ch_name)\n",
    "    \n",
    "        if percentage > 0.2:\n",
    "            print(f'BAD------ global channel: {ch_name}, percentage: {percentage}')\n",
    "    \n",
    "            if ch_name in global_bad_channels_drop_log:\n",
    "                global_bad_channels_drop_log[ch_name].append('BAD FOR MORE THAN 20%')\n",
    "            else:\n",
    "                global_bad_channels_drop_log[ch_name] = ['BAD FOR MORE THAN 20%']\n",
    "    \n",
    "            # update drop_log\n",
    "            drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "                             if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "            \n",
    "    del epochs_copy\n",
    "            \n",
    "    return drop_log, global_bad_channels_drop_log\n",
    "\n",
    "def get_bad_trials(drop_log):\n",
    "    \"\"\"\n",
    "    If more than 10% of channels were marked bad for an epoch (trial), the entire epoch was rejected\n",
    "    :param drop_log: \n",
    "    :return: trials_to_drop: ndarray\n",
    "    '\"\"\"\n",
    "    trials_to_drop = []\n",
    "    return trials_to_drop\n",
    "\n",
    "def interpolate_bad_channels(epochs, drop_log, global_bad_channels_drop_log):\n",
    "    '''\n",
    "    Bad channels were interpolated using spherical splines\n",
    "    :param epochs: \n",
    "    :param drop_log: \n",
    "    :param global_bad_channels_drop_log: \n",
    "    :return: \n",
    "    '''\n",
    "    return epochs, drop_log\n",
    "    \n",
    "# def pre_process_eeg(input_fname, trigger_map=None, parameters=None):\n",
    "#     raw = mne.io.read_raw_bdf(input_fname, eog=['EX7', 'EX8'])\n",
    "# \n",
    "#     # 1. re-reference: to mastoids\n",
    "#     raw.set_eeg_reference(ref_channels=['M1', 'M2'])\n",
    "# \n",
    "#     # 2. 4-th order Butterworth filters\n",
    "#     raw_filtered = raw.copy().filter(\n",
    "#         l_freq=.1,\n",
    "#         h_freq=30.0,\n",
    "#         n_jobs=10,\n",
    "#         method='iir',\n",
    "#         iir_params=None,\n",
    "#     )\n",
    "# \n",
    "#     # 3. Notch filter at 50 Hz\n",
    "#     raw_filtered = raw_filtered.filter(\n",
    "#         freqs=np.arange(50, 251, 50),\n",
    "#         n_jobs=10,\n",
    "#         method='iir',\n",
    "#         iir_params=None,\n",
    "#     )\n",
    "# \n",
    "#     # 4. segmentation -400 to 800 ms around the response\n",
    "#     epochs = create_epochs(raw_filtered, tmin=-.4, tmax=.8)\n",
    "# \n",
    "#     # 5. ocular artifact correction with ICA\n",
    "#     refined_epochs = ocular_correction_ica(epochs, raw, heog=['EX3', 'EX4'], veog=['EX1', 'EX2'])\n",
    "# \n",
    "#     # 6. Trial-wise Bad Channels Identification\n",
    "#     drop_log = find_bad_trails(epochs) \n",
    "#\n",
    "#     # 7. Global Bad Channel Identification - <.4 corr with 6 neigh. and channels marked as bad for more than 20% trials\n",
    "#     drop_log, global_bad_channels_drop_log = find_global_bad_channels(epochs, drop_log)\n",
    "#\n",
    "#     # 8. Drop bad trails\n",
    "#     get_bad_trials(drop_log) -> if more than 10% of channels were marked bad for an epoch, the entire epoch was rejected todo\n",
    "#    \n",
    "#     # 9. Interpolate bad channels (and thus update drop log)\n",
    "#     interpolate_bad_channels(epochs, ) -> changed epochs and changes updated_drop_log todo\n",
    "# "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:21:38.788972Z",
     "start_time": "2023-12-06T18:21:38.774658Z"
    }
   },
   "id": "cc62ce10ab154b16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24a0c51998a3561b"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "response_event_dict = {\n",
    "    'Stimulus/RE*ex*1_n*1_c_1*R*FB': 10003,\n",
    "    'Stimulus/RE*ex*1_n*1_c_1*R*FG': 10004,\n",
    "    'Stimulus/RE*ex*1_n*1_c_2*R': 10005,\n",
    "    'Stimulus/RE*ex*1_n*2_c_1*R': 10006,\n",
    "    'Stimulus/RE*ex*2_n*1_c_1*R': 10007,\n",
    "    'Stimulus/RE*ex*2_n*2_c_1*R*FB': 10008,\n",
    "    'Stimulus/RE*ex*2_n*2_c_1*R*FG': 10009,\n",
    "    'Stimulus/RE*ex*2_n*2_c_2*R': 10010,\n",
    "}\n",
    "\n",
    "new_response_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "events_mapping = {\n",
    "    'correct_response': [10003, 10004, 10008, 10009],\n",
    "    'error_response': [10005, 10006, 10007, 10010],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:15:05.605025Z",
     "start_time": "2023-12-06T18:15:05.601203Z"
    }
   },
   "id": "cb79f60ed143d32c"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from data/GNG_AA0303-64 el.vhdr...\n",
      "Setting channel info structure...\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_brainvision(\n",
    "    vhdr_fname = 'data/GNG_AA0303-64 el.vhdr',   \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:15:06.240608Z",
     "start_time": "2023-12-06T18:15:06.198585Z"
    }
   },
   "id": "9fbed82b8b95139f"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Stimulus/RE*ex*1_n*1_c_1*R*FB', 'Stimulus/RE*ex*1_n*1_c_1*R*FG', 'Stimulus/RE*ex*1_n*1_c_2*R', 'Stimulus/RE*ex*1_n*2_c_1*R', 'Stimulus/RE*ex*2_n*1_c_1*R', 'Stimulus/RE*ex*2_n*2_c_1*R*FB', 'Stimulus/RE*ex*2_n*2_c_1*R*FG', 'Stimulus/RE*ex*2_n*2_c_2*R']\n",
      "Not setting metadata\n",
      "311 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 311 events and 181 original time points ...\n",
      "1 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "epochs = create_epochs(\n",
    "    raw, \n",
    "    tmin=-.1, \n",
    "    tmax=.6,\n",
    "    events_to_select=response_event_dict,  # response_event_dict\n",
    "    new_events_dict=new_response_event_dict,  # new_response_event_dict\n",
    "    events_mapping=events_mapping,  # events_mapping\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:15:07.363377Z",
     "start_time": "2023-12-06T18:15:06.823459Z"
    }
   },
   "id": "173d5e9b55631419"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F8']\n",
      "    Rejecting  epoch based on EEG : ['P9']\n",
      "    Rejecting  epoch based on EEG : ['F4', 'PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['PO7']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F4', 'PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['AF8', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'C4', 'CP4', 'CP2', 'P4', 'PO4']\n",
      "    Rejecting  epoch based on EEG : ['F3', 'F5', 'F8', 'PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['F4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "    Rejecting  epoch based on EEG : ['PO4']\n",
      "53 bad epochs dropped\n",
      "BAD------ trail index 24, channel: F4\n",
      "-3.479290055248618e-05\n",
      "[-8.80386740e-08  1.40742320e-05  5.11154696e-06  7.07038674e-06\n",
      "  4.33318785e-06 -2.42235912e-06]\n",
      "BAD------ trail index 236, channel: PO7\n",
      "8.843108839779003e-05\n",
      "[7.97972376e-07 8.81430387e-06 1.21351768e-05 1.47831768e-05\n",
      " 7.57413260e-06 1.45127514e-05]\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "#     Channels that meet following conditions were marked as bad for the trail:\n",
    "#         (1) Channels with a voltage difference of 100 μV through the duration of the epoch;\n",
    "#         (2) Channels that were flat;\n",
    "#         (3) Channels with more than a 30 μV difference with the nearest six neighbors; \n",
    "#     :param epochs with globally setted bad channels that have < .04 correlation with the neighbors: \n",
    "#     :return: epochs with channels marked as bad\n",
    "#     \"\"\"\n",
    "# \n",
    "# epochs_copy = epochs.copy()\n",
    "# drop_log = None\n",
    "# \n",
    "# # channels with a voltage difference of 100 μV through the duration of the epoch\n",
    "# reject_criteria = dict(eeg=100e-6)\n",
    "# # flat channels (less than 1 µV of peak-to-peak difference)\n",
    "# flat_criteria = dict(eeg=1e-6)\n",
    "# \n",
    "# epochs_copy.drop_bad(reject=reject_criteria, flat=flat_criteria)\n",
    "# drop_log = epochs_copy.drop_log # drop_log - bo bedziemy tylko odrzucac te epoki, dla których ilość złych kanałów pzekroczy 10%\n",
    "# \n",
    "# for idx, _ in enumerate(epochs_copy):\n",
    "#     epoch = epochs[idx]\n",
    "#     epoch_data = epoch.get_data(copy=True)\n",
    "#     for ch_name, ch_idx in zip(epoch.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "#         mean_channel_data = np.array(np.mean(epoch_data[0,ch_idx,:]))    \n",
    "# \n",
    "#         ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "#             target_ch_name = ch_name, \n",
    "#             epochs = epoch, \n",
    "#             k=6\n",
    "#         )\n",
    "# \n",
    "#         mean_neighbors_data = np.array([np.mean(epoch_data[0, ch_neighbor_index, :]) for ch_neighbor_index in ch_neighbors_indices])\n",
    "# \n",
    "#         # # if channels has more than a 30 μV difference with the nearest six neighbors\n",
    "#         if (abs(mean_neighbors_data - mean_channel_data) > 30e-6).all():\n",
    "#             print(f'BAD------ trail index {idx}, channel: {ch_name}')\n",
    "#             print(mean_channel_data)\n",
    "#             print(mean_neighbors_data)\n",
    "# \n",
    "#             new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx] \n",
    "#             drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:15:11.814122Z",
     "start_time": "2023-12-06T18:15:07.379868Z"
    }
   },
   "id": "d2edad5badbd8442"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 64, 181)\n",
      "BAD------ global channel: Fp1, low corr\n",
      "[ 0.00038896 -0.00124657 -0.0016246   0.00173066  0.00396127 -0.00232104]\n",
      "BAD------ global channel: Fp1, percentage: 1.0\n"
     ]
    }
   ],
   "source": [
    "# epochs_copy = epochs.copy()\n",
    "# epochs_data = epochs_copy.get_data(copy=True)\n",
    "# print(epochs_data.shape)\n",
    "# concatenated_epochs_data = np.concatenate(epochs_data, axis=1)\n",
    "# \n",
    "# # zepsuj do testow\n",
    "# concatenated_epochs_data[0] = np.random.normal(0, 5, concatenated_epochs_data[0].shape)\n",
    "# \n",
    "# global_bad_channels_drop_log = {}\n",
    "# \n",
    "# # (1) Channels with an absolute correlation with the nearest six neighboring channels that fell below .4\n",
    "# for ch_name, ch_idx in zip(epochs.info['ch_names'], np.arange(0, len(epochs.info['ch_names']))):\n",
    "#     channel_data = concatenated_epochs_data[ch_idx]\n",
    "# \n",
    "#     ch_neighbors_indices, ch_neighbors_names = get_k_nearest_neighbors(\n",
    "#         target_ch_name = ch_name,\n",
    "#         epochs = epochs.copy(),\n",
    "#         k=6\n",
    "#     )\n",
    "#     channel_neighbors_data = np.array([concatenated_epochs_data[ch_neighbor_index]\n",
    "#                                        for ch_neighbor_index in ch_neighbors_indices])\n",
    "#     channels_corr = np.tril(np.corrcoef(channel_neighbors_data, channel_data), k=-1)\n",
    "# \n",
    "#     if (abs(channels_corr[-1][:-1]) < .4).all():\n",
    "#         print(f'BAD------ global channel: {ch_name}, low corr')\n",
    "#         print(channels_corr[-1][:-1])\n",
    "# \n",
    "#         # mark channel as globally bad\n",
    "#         global_bad_channels_drop_log[ch_name] = ['LOW CORR NEIGH']\n",
    "#         # update drop_log\n",
    "#         drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "#                          if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))\n",
    "# \n",
    "# # (2) Channels that were marked as bad for more than 20% of epochs   \n",
    "# for ch_name, ch_idx in zip(epochs_copy.info['ch_names'], np.arange(0, len(epochs_copy.info['ch_names']))):\n",
    "#     percentage = calculate_percentage(drop_log, ch_name)\n",
    "# \n",
    "#     if percentage > 0.2:\n",
    "#         print(f'BAD------ global channel: {ch_name}, percentage: {percentage}')\n",
    "# \n",
    "#         if ch_name in global_bad_channels_drop_log:\n",
    "#             global_bad_channels_drop_log[ch_name].append('BAD FOR MORE THAN 20%')\n",
    "#         else:\n",
    "#             global_bad_channels_drop_log[ch_name] = ['BAD FOR MORE THAN 20%']\n",
    "# \n",
    "#         # update drop_log\n",
    "#         drop_log = tuple(drop_log[i] + (ch_name,)\n",
    "#                          if ch_name not in drop_log[i] else drop_log[i] for i, item in enumerate(drop_log))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:15:41.712537Z",
     "start_time": "2023-12-06T18:15:41.455444Z"
    }
   },
   "id": "bb79d5c56aa2a3e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d52225cbddc8f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
