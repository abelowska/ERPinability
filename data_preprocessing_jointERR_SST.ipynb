{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:00:50.487542Z",
     "start_time": "2023-12-17T11:00:50.442256Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import mne\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mne.utils import set_log_file\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from ssqueezepy import Wavelet, cwt, icwt\n",
    "from lifelines import KaplanMeierFitter\n",
    "from ssqueezepy.experimental import scale_to_freq\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# matplotlib.use('Qt5Agg')\n",
    "# plt.switch_backend('QtAgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45112740-0fe5-49ac-9498-d68c4f9ccd91",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc2816dd38f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:00:59.503095Z",
     "start_time": "2023-12-17T11:00:59.455840Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8e45f-7668-4b0e-b5d9-a3deb64611c1",
   "metadata": {},
   "source": [
    "Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f9943ed75eefd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:00:52.271529Z",
     "start_time": "2023-12-17T11:00:52.254468Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a custom logger for preprocessing INFO\n",
    "logger_preprocessing_info = logging.getLogger('preprocessing_info')\n",
    "logger_preprocessing_info.setLevel(logging.INFO)\n",
    "logger_preprocessing_info.propagate = False\n",
    "\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a custom logger for errors\n",
    "logger_errors_info = logging.getLogger('errors')\n",
    "logger_errors_info.setLevel(logging.INFO)\n",
    "logger_preprocessing_info.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cb486bce59f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:25:40.265260Z",
     "start_time": "2023-12-17T11:25:40.211082Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def read_trigger_map(file_name):\n",
    "    line_count = 0\n",
    "    trigger_map = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        # Read each line and increment the counter\n",
    "        line = file.readline()\n",
    "        try:\n",
    "            match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "            trigger = (match.group(1), match.group(2), )\n",
    "            trigger_map.append(trigger)\n",
    "        except:\n",
    "            pass\n",
    "        while line:\n",
    "            line_count += 1\n",
    "            line = file.readline()\n",
    "            try:\n",
    "                match = re.search(\"(.*):(.*)(\\\\n)\", line)\n",
    "                trigger = (match.group(1), match.group(2), )\n",
    "                trigger_map.append(trigger)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    assert len(trigger_map) == line_count, \\\n",
    "        f'The length of trigger file ({line_count}) not equals length of created trigger_map ({len(trigger_map)})'\n",
    "\n",
    "    return trigger_map\n",
    "\n",
    "def create_triggers_dict(trigger_map):\n",
    "    triggers_codes = [item[1] for item in trigger_map]\n",
    "    # Create an ordered dictionary to maintain order and remove duplicates\n",
    "    unique_ordered_dict = OrderedDict.fromkeys(triggers_codes)\n",
    "    numbered_dict = {key: 1000 + number for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    reversed_numbered_dict = {1000 + number: key for number, key in enumerate(unique_ordered_dict.keys())}\n",
    "    return numbered_dict, reversed_numbered_dict\n",
    "\n",
    "def replace_trigger_names(raw, participant_id, trigger_map, new_response_event_dict=None, replace=False, search='RE'):\n",
    "    # Replace event IDs in the Raw object\n",
    "    events = mne.find_events(raw, stim_channel='Status')\n",
    "    new_events_list = events.copy()\n",
    "    \n",
    "    # add trigger to corrupted bdf files - too short reaction \n",
    "    if paradigm == 'GNG' and (participant_id == 'B-GNG-199' or participant_id == 'B-GNG-208'):\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "    # add missing RE triggers to bdf file - to short time between stop trigger and reaction trigger\n",
    "    ids = ['SST-165', 'SST-211', 'SST-122', 'SST-088','SST-045','SST-012','SST-083','SST-136','SST-125']\n",
    "    if paradigm == 'SST' and participant_id in ids:\n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "    \n",
    "    # delete ghost event 65312\n",
    "    if paradigm == 'SST' and (participant_id == 'SST-181'):\n",
    "        for idx, event in enumerate(events):\n",
    "            if event[2] == 65312:\n",
    "                new_events_list = np.concatenate([events[:idx, :], events[idx+1:, :]]) \n",
    "                break  \n",
    "    \n",
    "    # delete ghost event 0: 130816\n",
    "    if paradigm == 'SST' and (participant_id == 'SST-075'):\n",
    "        for idx, event in enumerate(events):\n",
    "            if event[2] == 130816:\n",
    "                new_events_list = np.concatenate([events[:idx, :], events[idx+1:, :]]) \n",
    "                break   \n",
    "    \n",
    "    if paradigm == 'SST' and participant_id == 'SST-130':\n",
    "        \n",
    "        delta_time = 3\n",
    "        for idx, event in enumerate(events):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([events[:idx, :], [[events[idx-1][0] + delta_time, 0, 65281]], events[idx:, :]]) \n",
    "                break\n",
    "\n",
    "        for idx, event in enumerate(new_events_list):\n",
    "            event_id = str(event[2])[-1]\n",
    "            trigger_id = trigger_map[idx][0]\n",
    "            trigger_new_code = trigger_map[idx][1]\n",
    "\n",
    "            if event_id != trigger_id:\n",
    "                new_events_list = np.concatenate([new_events_list[:idx, :], [[new_events_list[idx-1][0] + delta_time, 0, 65281]], new_events_list[idx:, :]]) \n",
    "                break\n",
    "                           \n",
    "    logger_preprocessing_info.info(f'EVENTS: {new_events_list}')\n",
    "\n",
    "    assert len(new_events_list) == len(trigger_map), \\\n",
    "            f'The length of trigger map ({len(trigger_map)}) not equals length of events in eeg recording ({len(new_events_list)})'\n",
    "\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    for idx, event in enumerate(new_events_list):\n",
    "        event_id = str(event[2])[-1]\n",
    "        trigger_id = trigger_map[idx][0]\n",
    "        trigger_new_code = trigger_map[idx][1]\n",
    "        \n",
    "        if event_id != trigger_id:\n",
    "            logger_errors_info.info(f'An event {idx} has different number than in provided file. {trigger_id} expected, {str(event[2])} found. Triggers may need to be checked.')\n",
    "\n",
    "        trigger_new_code_int = trigger_map_codes[trigger_new_code]\n",
    "        new_events_list[idx][2] = trigger_new_code_int\n",
    "\n",
    "    annot_from_events = mne.annotations_from_events(\n",
    "        events=new_events_list,\n",
    "        event_desc=mapping,\n",
    "        sfreq=raw.info[\"sfreq\"],\n",
    "        orig_time=raw.info[\"meas_date\"],\n",
    "    )\n",
    "    raw_copy = raw.copy()\n",
    "    raw_copy.set_annotations(annot_from_events)\n",
    "\n",
    "    return raw_copy\n",
    "\n",
    "def find_items_matching_regex(dictionary, regex_list):\n",
    "    matching_items = {}\n",
    "    for regex in regex_list:\n",
    "        pattern = re.compile(regex)\n",
    "        matching_items.update({key: value for key, value in dictionary.items() if pattern.match(key)})\n",
    "    return matching_items\n",
    "\n",
    "@dataclass\n",
    "class ParticipantTriggerMappingContext:\n",
    "    event_dict: dict\n",
    "    events_mapping: dict\n",
    "    new_event_dict: dict\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.event_dict}\\n{self.events_mapping}\\n{self.new_event_dict}\"\n",
    "\n",
    "def create_events_mappings(trigger_map, case='RE') -> ParticipantTriggerMappingContext:\n",
    "    trigger_map_codes, mapping = create_triggers_dict(trigger_map)\n",
    "\n",
    "    if case == 'RE':\n",
    "        new_event_dict = {\"correct_response\": 0, \"error_response\": 1, \"incorrect_go_response\": 2}\n",
    "        events_mapping = {\n",
    "            'correct_response': [],\n",
    "            'error_response': [],\n",
    "            'incorrect_go_response' : []\n",
    "        }\n",
    "        \n",
    "        # find response events from experimental blocks\n",
    "        regex_pattern = [r'RE\\*image\\*.*\\*0\\*.*', r'RE\\*image\\*.*\\*-\\*.*']\n",
    "        event_dict = find_items_matching_regex(trigger_map_codes, regex_pattern)\n",
    "    \n",
    "        for event_id in event_dict.keys():\n",
    "            event_id_splitted = event_id.split('*')\n",
    "    \n",
    "            if (event_id_splitted[3] == '-') and (event_id_splitted[-1] == event_id_splitted[-2]):\n",
    "                events_mapping['correct_response'].append(event_dict[event_id])\n",
    "            elif (event_id_splitted[3] == '-') and (event_id_splitted[-1] != event_id_splitted[-2]):\n",
    "                events_mapping['incorrect_go_response'].append(event_dict[event_id])\n",
    "            elif (str(event_id_splitted[3]) == '0') and (event_id_splitted[-1] != '-'):\n",
    "                events_mapping['error_response'].append(event_dict[event_id])\n",
    "    \n",
    "        \n",
    "    elif case == 'STIM':\n",
    "        new_event_dict = {\"inhibited_stop\": 0, \"uninhibited_stop\": 1}\n",
    "        events_mapping = {\n",
    "            'inhibited_stop': [],\n",
    "            'uninhibited_stop': [],\n",
    "        }\n",
    "        # find all target stimuli events from experimental blocks\n",
    "        regex_pattern = [r'ST.*']\n",
    "        event_dict = find_items_matching_regex(trigger_map_codes, regex_pattern)\n",
    "\n",
    "        for event_id in event_dict.keys():\n",
    "            event_id_splitted = event_id.split('*')\n",
    "\n",
    "            if (event_id_splitted[-1] == '-') and (event_id_splitted[3] == '0'):\n",
    "                events_mapping['inhibited_stop'].append(event_dict[event_id])\n",
    "            elif (event_id_splitted[-1] != '-') and (event_id_splitted[3] == '0'):\n",
    "                events_mapping['uninhibited_stop'].append(event_dict[event_id])\n",
    "\n",
    "    else:\n",
    "        logger_errors_info('Not known case. Possible cases: \\'RE\\' for response, \\'STIM\\` for stimuli, and \\`FBCK\\` for feedback-locked events extraction.')\n",
    "        # todo raise an Error\n",
    "        event_dict = {}\n",
    "        events_mapping = {}\n",
    "        new_event_dict = {}\n",
    "        \n",
    "    return ParticipantTriggerMappingContext(event_dict=event_dict,\n",
    "                                            events_mapping=events_mapping,\n",
    "                                            new_event_dict=new_event_dict)\n",
    "\n",
    "def create_epochs(\n",
    "    raw,\n",
    "    context: ParticipantTriggerMappingContext,\n",
    "    tmin=-.1,\n",
    "    tmax=.6,\n",
    "    baseline=None,\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    "    detrend=None\n",
    "):\n",
    "    # select specific events\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=context.event_dict)\n",
    "\n",
    "    # Merge different events of one kind\n",
    "    for mapping in context.events_mapping:\n",
    "        events = mne.merge_events(\n",
    "            events=events,\n",
    "            ids=context.events_mapping[mapping],\n",
    "            new_id=context.new_event_dict[mapping],\n",
    "            replace_events=True,\n",
    "        )\n",
    "    \n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=events,\n",
    "        event_id=context.new_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=baseline,\n",
    "        reject_by_annotation=reject_by_annotation,\n",
    "        preload=True,\n",
    "        reject=reject,\n",
    "        picks=['eeg', 'eog'],\n",
    "        detrend=detrend,\n",
    "        on_missing = 'warn',\n",
    "    )\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "\n",
    "def ocular_correction_gratton(epochs, subtract_evoked=False):\n",
    "\n",
    "    if subtract_evoked:\n",
    "        epochs_sub = epochs.copy().subtract_evoked()\n",
    "\n",
    "        eog_model = mne.preprocessing.EOGRegression(\n",
    "            picks=\"eeg\",\n",
    "            picks_artifact=\"eog\"\n",
    "        ).fit(epochs_sub)\n",
    "    else:\n",
    "        eog_model = mne.preprocessing.EOGRegression(\n",
    "            picks=\"eeg\",\n",
    "            picks_artifact=\"eog\"\n",
    "        ).fit(epochs)\n",
    "\n",
    "    epochs_clean_plain = eog_model.apply(epochs)\n",
    "\n",
    "    return epochs_clean_plain\n",
    "\n",
    "def find_bad_trials(epochs, picks=['FCz','Cz']):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    epochs_picked_channels = epochs.copy().pick(picks=picks)\n",
    "\n",
    "    epochs_picked_channels.drop_bad()\n",
    "    drop_log = epochs_picked_channels.drop_log\n",
    "\n",
    "    for idx, _ in enumerate(epochs_picked_channels):\n",
    "        epoch = epochs_picked_channels[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "        \n",
    "        for ch_name, ch_idx in zip(epochs_picked_channels.info['ch_names'], \n",
    "                                   np.arange(0, len(epochs_picked_channels.info['ch_names']))):\n",
    "            channel_data = epoch_data[0,ch_idx,:]\n",
    "\n",
    "            # EEG signal at the FCz or Cz site was greater than ± 150 μV were removed\n",
    "            if(abs(channel_data) > 150e-6).any():\n",
    "                logger_preprocessing_info.info(f'Channel {ch_name} exceeded +- 150 μV threshold at {idx} trail')\n",
    "                new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx]\n",
    "                drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))\n",
    "\n",
    "    del epochs_picked_channels\n",
    "\n",
    "    return drop_log\n",
    "\n",
    "\n",
    "def reject_bad_trials(epochs, drop_log, picks=['FCz', 'Cz']):\n",
    "\n",
    "    epochs_to_drop_indices = []\n",
    "    clean_epochs = epochs.copy()\n",
    "\n",
    "    assert len(clean_epochs) == len(drop_log), f'Length of epochs ({len(clean_epochs)}) not equals length of drop_log ({len(drop_log)}). Cannot mark trials as BAD.'\n",
    "            \n",
    "    for idx, item in enumerate(drop_log):\n",
    "        if ('FCz' in item) or ('Cz' in item):\n",
    "            logger_preprocessing_info.info(f'Rejecting trial {idx}. Artifacts at Fz or FCz')\n",
    "            epochs_to_drop_indices.append(idx)\n",
    "\n",
    "    clean_epochs = clean_epochs.drop(\n",
    "        indices = epochs_to_drop_indices,\n",
    "        reason = 'EXCEED 150uV',\n",
    "    )\n",
    "\n",
    "     # update drop_log\n",
    "    for trial_idx in epochs_to_drop_indices:\n",
    "        drop_log = tuple(('REJECTED',) if i == trial_idx else element for i, element in enumerate(drop_log))\n",
    "    \n",
    "    return clean_epochs, drop_log    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6ee7ddf4c2955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:25:56.234208Z",
     "start_time": "2023-12-17T11:25:56.197874Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pre_process_eeg(input_fname, participant_id, context, trigger_fname=None, tmin=-0.1, tmax=0.9):\n",
    "    # 0. read bdf\n",
    "    raw = mne.io.read_raw_bdf(\n",
    "        input_fname,\n",
    "        eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "        exclude=['EXG5', 'EXG6'],\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        raw = raw.set_montage('biosemi64')\n",
    "    except ValueError as e:\n",
    "        if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "            raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "            logger_preprocessing_info.info('On missing')\n",
    "        else:\n",
    "            logger_errors_info.info('Lacks important channels!')\n",
    "\n",
    "    # 1. replace trigger names\n",
    "    trigger_map = read_trigger_map(trigger_fname)\n",
    "    raw_new_triggers = replace_trigger_names(raw, participant_id, trigger_map)\n",
    "\n",
    "    # 2. re-reference: to mastoids\n",
    "    if '005' in participant_id:\n",
    "        raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7'])\n",
    "        logger_errors_info.info('Referencing to EX7')\n",
    "    elif '044' in participant_id:\n",
    "        raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG8'])\n",
    "        logger_errors_info.info('Referencing to EX8')\n",
    "    else:\n",
    "        raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])\n",
    "\n",
    "    # 3. Resampling\n",
    "    raw_resampled = raw_ref.copy().resample(sfreq=500)\n",
    "\n",
    "    # 4. Detrending, Segmentation, and first baseline correction\n",
    "    epochs = create_epochs(\n",
    "        # raw_resampled,\n",
    "        raw_resampled,\n",
    "        tmin = tmin,\n",
    "        tmax = tmax,\n",
    "        baseline = (-0.1, 0),\n",
    "        detrend = 1,\n",
    "        context=context,\n",
    "        reject = None,\n",
    "        reject_by_annotation = False,\n",
    "    )\n",
    "\n",
    "    # 5. ocular artifact correction with Gratton\n",
    "    epochs_eog_corrected = ocular_correction_gratton(epochs)\n",
    "    \n",
    "    # 6. Second re-baseline\n",
    "    epochs_eog_corrected.apply_baseline()\n",
    "    \n",
    "    # 7. Mark bad trials\n",
    "    drop_log = find_bad_trials(epochs_eog_corrected, picks='eeg')\n",
    "\n",
    "    # 8. Reject bad trials\n",
    "    clean_epochs, _ = reject_bad_trials(epochs_eog_corrected, drop_log)\n",
    "    if len(clean_epochs) < 6:\n",
    "        logger_errors_info.info(f'Participant has only {len(clean_epochs)} artifact-free trials')\n",
    "        \n",
    "        \n",
    "    return epochs_eog_corrected, drop_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4804744bae52053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:26:08.441044Z",
     "start_time": "2023-12-17T11:26:08.398709Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def save_epochs_with_drop_log(epochs, drop_log, participant_id):\n",
    "    item = pd.DataFrame({\n",
    "        'epochs': [epochs],\n",
    "        'drop_log': [drop_log],\n",
    "    })\n",
    "\n",
    "    item.to_pickle(f'{preprocessed_data_dir_path}preprocessed_{participant_id}.pkl')\n",
    "\n",
    "    return logger_preprocessing_info.info('Epochs saved to pickle.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf96ab-4149-4f16-8773-60bc0c8420bc",
   "metadata": {},
   "source": [
    "## Base preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882b097-207b-4587-a419-063bd5394d22",
   "metadata": {},
   "source": [
    "Set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d9128fbf4b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:26:53.363658Z",
     "start_time": "2023-12-17T11:26:53.335052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GNG | SST | Flanker\n",
    "paradigm = 'SST'\n",
    "# RE | STIM | FBCK\n",
    "case = 'RE'\n",
    "# todo think whether move global vars as paradigm and case info some kind of data/case class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e062bba-d542-4f5b-967c-a07e4c7d0804",
   "metadata": {},
   "source": [
    "Set paths base on globals values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d78bc6882ce3a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:28:10.280614Z",
     "start_time": "2023-12-17T11:28:10.230824Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trigger_dir_path = f'data/{paradigm}/raw/triggers/'\n",
    "bdf_dir_path = f'data/{paradigm}/raw/bdfs/'\n",
    "behavioral_dir_path = f'data/{paradigm}/behavioral/'\n",
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "logger_dir_path = f'data/joint/{paradigm}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9dadc7-6b1e-4816-b3e2-6dcf5106c456",
   "metadata": {},
   "source": [
    "Set output files for loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757326b947ffa25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a file handler for preprocessing and set the level to INFO\n",
    "file_handler_preprocessing = logging.FileHandler(f'data/joint/{paradigm}/{case}_preprocessing.txt')\n",
    "file_handler_preprocessing.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for preprocessing\n",
    "formatter_preprocessing = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_preprocessing.setFormatter(formatter_preprocessing)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_preprocessing_info.addHandler(file_handler_preprocessing)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a file handler for errors and set the level to INFO\n",
    "file_handler_errors = logging.FileHandler(f'data/joint/{paradigm}/{case}_errors.txt')\n",
    "file_handler_errors.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for errors\n",
    "formatter_errors = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_errors.setFormatter(formatter_errors)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_errors_info.addHandler(file_handler_errors)\n",
    "\n",
    "##### MNE ###################################################\n",
    "# Create logger for MNE logs\n",
    "logger_f_name = f'data/joint/{paradigm}/{case}_MNE-logs.txt'\n",
    "set_log_file(fname=logger_f_name, output_format=\"%(asctime)s - %(message)s\", overwrite=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3497de-7160-43f8-a732-dd0222d58081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# console_handler = logging.StreamHandler()\n",
    "# console_handler.setLevel(logging.DEBUG)  # Set the desired logging level\n",
    "\n",
    "# logger_errors_info.addHandler(console_handler)\n",
    "# logger_preprocessing_info.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ef0c8-4699-4989-b78b-fe3178f79139",
   "metadata": {},
   "source": [
    "Read participant IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cecb11b0b0a96c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id_list = [item.split('.')[0] for item in os.listdir(bdf_dir_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5152a-147c-456c-aa82-ead7a1ebf739",
   "metadata": {},
   "source": [
    "Perform base preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d32845d8c6e64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for participant_id in id_list:\n",
    "    print(f'{participant_id}\\n')\n",
    "    bdf_fname = f'{bdf_dir_path}{participant_id}.bdf'\n",
    "    trigger_fname = f'{trigger_dir_path}triggerMap_{participant_id}.txt'\n",
    "\n",
    "    logger_preprocessing_info.info(f'#### PARTICIPANT ID: {participant_id} #########')\n",
    "    logger_errors_info.info(f'#### PARTICIPANT ID: {participant_id} #########')\n",
    "\n",
    "    try:\n",
    "        trigger_map = read_trigger_map(trigger_fname)\n",
    "        participant_context = create_events_mappings(trigger_map)\n",
    "        logger_preprocessing_info.info(f'Context: {participant_context}')\n",
    "\n",
    "        epochs_preprocessed, drop_log = pre_process_eeg(\n",
    "            input_fname=bdf_fname,\n",
    "            participant_id=participant_id,\n",
    "            context=participant_context,\n",
    "            trigger_fname=trigger_fname,\n",
    "            tmin=-0.1, \n",
    "            tmax=0.9,\n",
    "        )\n",
    "\n",
    "        save_epochs_with_drop_log(epochs_preprocessed, drop_log, participant_id)\n",
    "\n",
    "    except Exception as e:        \n",
    "        logger_errors_info.info(f\"{e}\")\n",
    "    \n",
    "    logger_preprocessing_info.info(f'\\n')\n",
    "    logger_errors_info.info(f'\\n')\n",
    "\n",
    "print(f'##########\\n DONE\\n')   \n",
    "# Restore MNE logging to std out     \n",
    "set_log_file(fname=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76603d2-540c-4d2d-8df6-7c1959e49691",
   "metadata": {},
   "source": [
    "## Wavelet filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2dbde-c443-4a47-9bc0-7ca70958925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wavelet_filter(grand_average, scales, central_freq = 6, signal_freq=500, threshold_point=0.85):\n",
    "    results_per_channel = []\n",
    "    for channel_grand_average in grand_average:\n",
    "        x = channel_grand_average.flatten()\n",
    "        t = np.linspace(-0.1, 0.9, len(x))\n",
    "        # construct wavelet function\n",
    "        wavelet = Wavelet(('morlet', {'mu': central_freq}))\n",
    "        Wx, _ = cwt(x, wavelet, fs=signal_freq, scales=scales, padtype='wrap', l1_norm=True, nv=None)\n",
    "        \n",
    "        # # baseline\n",
    "        # baseline_mean = np.mean(Wx[:, :50], axis=1, keepdims=True)\n",
    "        # Wx = Wx - baseline_mean\n",
    "        \n",
    "        freq = scale_to_freq(scales, wavelet, N=len(x), fs=signal_freq)\n",
    "        \n",
    "        # Compute and normalize the power spectrum from the CWT coefficients\n",
    "        power_spectrum = np.abs(Wx)**2\n",
    "        normalized_power_spectrum = power_spectrum / np.sum(power_spectrum)\n",
    "\n",
    "        # Flatten the normalized power spectrum for CDF calculation\n",
    "        flattened_spectrum = normalized_power_spectrum.flatten()\n",
    "\n",
    "        # Use the Kaplan–Meier estimator\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=flattened_spectrum, event_observed=np.ones_like(flattened_spectrum))\n",
    "\n",
    "        # Get the CDF values from the Kaplan–Meier estimator\n",
    "        cdf_values = 1 - kmf.survival_function_.KM_estimate\n",
    "\n",
    "        # Calculate the threshold\n",
    "        threshold = threshold_point * (np.max(cdf_values) - np.min(cdf_values)) + np.min(cdf_values)\n",
    "\n",
    "        # Plot the empirical CDF and the filtering model\n",
    "        plt.step(kmf.survival_function_.index, cdf_values, where='post', label='Empirical CDF')\n",
    "        plt.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "        plt.title('Empirical CDF and Filtering Model')\n",
    "        plt.xlabel('Wavelet Coefficient')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Find the value of wavelets coefficient that are above threshold\n",
    "        cutoff_wavelet_index = np.where(cdf_values > threshold)[0][0]\n",
    "        cutoff_wavelet_coef = kmf.survival_function_.index[cutoff_wavelet_index]\n",
    "        print(f'Estimated threshold value for wavelet coefficients: {cutoff_wavelet_coef}')\n",
    "\n",
    "        cwt_result_threshold_mask = np.where(normalized_power_spectrum >= cutoff_wavelet_coef, 1, 0)\n",
    "\n",
    "        # Plot the CWT result\n",
    "        plt.figure(figsize=(12, 16))\n",
    "\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.imshow(np.abs(Wx), extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('CWT Magnitude')\n",
    "\n",
    "        # \n",
    "        plt.subplot(4, 1, 2)\n",
    "        plt.imshow(normalized_power_spectrum, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('Normalized Power Spectrum')\n",
    "\n",
    "        plt.subplot(4, 1, 3)\n",
    "        plt.imshow(cwt_result_threshold_mask, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('Threshold Mask')\n",
    "\n",
    "        plt.subplot(4, 1, 4)\n",
    "        plt.imshow(cwt_result_threshold_mask*np.abs(Wx), extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('Thresholded Grand Average - Examle')\n",
    "        plt.show()\n",
    "        \n",
    "        results_per_channel.append((cwt_result_threshold_mask, wavelet, scales))\n",
    "\n",
    "    return results_per_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8f25d-a2e5-4c96-ac3e-8f31999dbc16",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-14T13:20:21.386590Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def signal_cwt(signal, scales, central_freq = 6, signal_freq=500):\n",
    "    x = signal.flatten()\n",
    "\n",
    "    # construct wavelet function\n",
    "    wavelet = Wavelet(('morlet', {'mu': central_freq}))\n",
    "    Wx, scales = cwt(x, wavelet, fs=signal_freq, scales=scales, padtype='wrap', l1_norm=True, nv=None)\n",
    "    \n",
    "    # baseline\n",
    "    # baseline_mean = np.mean(Wx[:, :50], axis=1, keepdims=True)\n",
    "    # Wx_baselined = Wx - baseline_mean\n",
    "\n",
    "    return Wx\n",
    "\n",
    "def epochs_to_tfr(epochs, scales, picks=['FCz', 'Cz'], events=['error_response']):\n",
    "    '''\n",
    "    \n",
    "    :param epochs: \n",
    "    :param picks: \n",
    "    :param events: \n",
    "    :return: ndarray of shape (n_events, n_channels, n_freqs, n_timepoints)\n",
    "    '''\n",
    "    if events == 'all':\n",
    "        epochs_picked = epochs.copy().pick(picks)\n",
    "    else:    \n",
    "        epochs_picked = epochs.copy()[events].pick(picks)\n",
    "    tfr_epochs = []\n",
    "    for idx, _ in enumerate(epochs_picked):\n",
    "        epoch = epochs_picked[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "\n",
    "        tfr_channel_data = []\n",
    "        for ch_name, ch_idx in zip(epochs_picked.info['ch_names'],\n",
    "                                   np.arange(0, len(epochs_picked.info['ch_names']))):\n",
    "            channel_data = epoch_data[0,ch_idx,:]\n",
    "    \n",
    "            channel_wavelet_data = signal_cwt(channel_data, scales)\n",
    "            tfr_channel_data.append(channel_wavelet_data)\n",
    "        tfr_epochs.append(tfr_channel_data)\n",
    "    \n",
    "    tfr_epochs = np.array(tfr_epochs)\n",
    "    return tfr_epochs\n",
    "\n",
    "def filter_signal(Wx, x, mask, wavelet, scales):\n",
    "    time_domain_signal = icwt(mask * Wx, wavelet, scales, nv=None, padtype='wrap', l1_norm=True, x_mean = np.mean(x))\n",
    "\n",
    "    return time_domain_signal\n",
    "\n",
    "def tfr_filter_epochs(tfr, original_signal, per_channel_cwt_results):\n",
    "    filtered_epochs = []\n",
    "    for tfr_epochs, org_epoch in zip(tfr,original_signal) :\n",
    "        filtered_channel_data = []\n",
    "        for idx, channel_data in enumerate(tfr_epochs):\n",
    "            mask = per_channel_cwt_results[idx][0]\n",
    "            wavelet = per_channel_cwt_results[idx][1]\n",
    "            scales = per_channel_cwt_results[idx][2]\n",
    "            signal = channel_data\n",
    "            reconstructed_signal = filter_signal(signal, org_epoch[idx], mask, wavelet, scales)\n",
    "            filtered_channel_data.append(reconstructed_signal)\n",
    "        filtered_epochs.append(filtered_channel_data)\n",
    "\n",
    "    filtered_epochs = np.array(filtered_epochs)    \n",
    "    return filtered_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036a811-5d7f-4b5d-8fdf-9c5728c71b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grand_average(path_to_dir, picks, event):\n",
    "    id_list = [item.split('.')[0] for item in os.listdir(path_to_dir)]\n",
    "    all_evokeds = []\n",
    "    \n",
    "    for id_ in id_list:\n",
    "        preprocessed_epochs = pd.read_pickle(f'{path_to_dir}{id_}.pkl')\n",
    "        clean_epochs, _ = reject_bad_trials(preprocessed_epochs['epochs'].to_numpy().flatten()[0], preprocessed_epochs['drop_log'].to_numpy().flatten()[0])\n",
    "        if len(clean_epochs) < 6:\n",
    "            logger_errors_info.info(f'Participant has only {len(clean_epochs)} artifact-free trials')\n",
    "        else:\n",
    "            all_evokeds.append(clean_epochs[event].average().get_data(picks=picks))\n",
    "    \n",
    "    all_evokeds = np.array(all_evokeds)    \n",
    "    grand_average = np.mean(all_evokeds, axis=0)\n",
    "    \n",
    "    return grand_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3597f-4e39-4231-98e1-373d6833c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wavelet_filter(path_to_dir, scales, picks=['FCz', 'Cz'], event='error_response', threshold=0.85, central_freq=6):\n",
    "    grand_average = get_grand_average(path_to_dir, picks=picks, event=event)\n",
    "    filter_per_channel = calculate_wavelet_filter(grand_average, scales=scales, central_freq=central_freq, threshold_point=threshold)\n",
    "    \n",
    "    return filter_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00fcd9-3db5-4554-a692-53eea15bc06d",
   "metadata": {},
   "source": [
    "### Test wavelet filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee8283-bb8b-42ff-95f0-e49d8ba95206",
   "metadata": {},
   "source": [
    "#### 1. Test quality of wavelet invers transform\n",
    "\n",
    "See: https://dsp.stackexchange.com/questions/87097/why-is-inverse-cwt-inexact-inaccurate/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d2a46-ba9d-4ffd-b5f4-97230825f154",
   "metadata": {},
   "source": [
    "Perform wavelet deconstruction and inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150556c9-b9ab-4544-bbff-b67c3e95a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm = 'SST'\n",
    "case = 'RE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111da7f3-78a6-4c4f-b57c-6a5913dc6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "id_list = [item.split('.')[0] for item in os.listdir(preprocessed_data_dir_path)]\n",
    "\n",
    "all_epochs_reconstructed = []\n",
    "all_epochs_original = []\n",
    "diffs = []\n",
    "nv=None\n",
    "\n",
    "# create scales\n",
    "# preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_list[0]}.pkl')\n",
    "# epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "# drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "\n",
    "# x = epochs[event].get_data(picks=picks)[0].flatten()\n",
    "# wavelet = Wavelet(('morlet', {'mu': 6}))\n",
    "\n",
    "# Wx, scales = cwt(x, wavelet, fs=500, scales='log-piecewise', padtype='wrap', l1_norm=True, nv=nv)\n",
    "# new_scales = scales[34:]\n",
    "new_scales = np.geomspace(16,500,200)\n",
    "\n",
    "for id_ in id_list:\n",
    "    # read data\n",
    "    preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_}.pkl')\n",
    "    epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "         \n",
    "    drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "    \n",
    "    epochs_data = epochs.copy()['error_response'].pick(['FCz'])\n",
    "    \n",
    "    # transform data into TFR space\n",
    "    wavelet = Wavelet(('morlet', {'mu': 6}))\n",
    "    \n",
    "    participant_epochs_reconstructed = []\n",
    "    participant_epochs_original = []\n",
    "    participant_diffs = []\n",
    "    for epoch in epochs_data:\n",
    "        \n",
    "        Wx, scales = cwt(epoch.flatten(), wavelet, fs=500, scales=new_scales, padtype='wrap', l1_norm=True, nv=None)\n",
    "        time_domain_signal = icwt(Wx, wavelet, scales=new_scales, nv=None, padtype='wrap', l1_norm=True, x_mean=np.mean(epoch.flatten()))\n",
    "\n",
    "        diff = abs(np.mean(epoch.flatten()) - np.mean(time_domain_signal))\n",
    "        \n",
    "        participant_epochs_reconstructed.append(time_domain_signal)\n",
    "        participant_epochs_original.append(epoch.flatten())\n",
    "        participant_diffs.append(diff) \n",
    "        \n",
    "    all_epochs_reconstructed.append(participant_epochs_reconstructed)\n",
    "    all_epochs_original.append(participant_epochs_original)\n",
    "    diffs.append(participant_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d4629-2702-484c-a9bf-7bfe59d17911",
   "metadata": {},
   "source": [
    "Plot per participant grand average similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d47ac-adf7-4459-b4bb-68a69651837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- 30 Hz adjusted with mean od the original signal \n",
    "x = np.linspace(-0.25, 0.9, 501)\n",
    "for i in range(0, len(all_epochs_reconstructed)):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(x, np.mean(all_epochs_original[i], axis=0).flatten())\n",
    "    plt.plot(x, np.mean(all_epochs_reconstructed[i], axis=0).flatten() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523c238-16b8-4912-804c-0da2b0a648e9",
   "metadata": {},
   "source": [
    "Calculate differences between original and reconstructed signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbbd4d9-a44a-44cd-a0fc-72197d42b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7 # 0.1 uV\n",
    "\n",
    "for idx, paricipant in enumerate(diffs):\n",
    "    print(f'IDX: {idx}\\n{paricipant}\\n')\n",
    "    exceed = np.array([True if x > epsilon else False for x in paricipant])\n",
    "    print(exceed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e43fd-bea7-45a0-bcc7-035da029c696",
   "metadata": {},
   "source": [
    "Test wavelet inverse transform quality per-participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6a7aa-c78f-45f3-b0e4-0e1affcb2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, np.array(all_epochs_reconstructed[0]).shape[-1])\n",
    "\n",
    "idx = 100\n",
    "participant_reconstructed = all_epochs_reconstructed[idx]\n",
    "participant_original = all_epochs_original[idx]\n",
    "\n",
    "for i in range(0, len(participant_reconstructed)):\n",
    "    plt.figure()\n",
    "    print(i)\n",
    "    plt.plot(x, participant_original[i])\n",
    "    plt.plot(x, participant_reconstructed[i])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd26b32-ed56-49d4-a186-9bbd6e799418",
   "metadata": {},
   "source": [
    "#### 2. Test thresholds and their impact into amplitude reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead272f2-60af-472f-9014-5b9a7137eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "paradigm = 'SST'\n",
    "case = 'RE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c3c15-d1f0-4a36-b1ad-69bc6978fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "picks = ['FCz']\n",
    "event = 'error_response'\n",
    "\n",
    "id_list = [item.split('.')[0] for item in os.listdir(preprocessed_data_dir_path)]\n",
    "tfr_epochs_participants = []\n",
    "epochs_participants = []\n",
    "\n",
    "# create scales\n",
    "# preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_list[0]}.pkl')\n",
    "# epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "# drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "\n",
    "# x = epochs[event].get_data(picks=picks)[0].flatten()\n",
    "# wavelet = Wavelet(('morlet', {'mu': 6}))\n",
    "new_scales = np.geomspace(16,500,200) # from 1 to 30 Hz\n",
    "\n",
    "for id_ in id_list:\n",
    "    # read data\n",
    "    preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_}.pkl')\n",
    "    epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "    drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "    \n",
    "    # save unfiltered data\n",
    "    epochs_participants.append(epochs[event].get_data(picks=picks))\n",
    "    \n",
    "    # transform data into TFR space\n",
    "    tfr_epochs = epochs_to_tfr(epochs, scales=new_scales, picks=picks, events=event)\n",
    "    # save tfr data\n",
    "    tfr_epochs_participants.append(tfr_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d2283-7013-465d-b200-4842f4942e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_average = get_grand_average(preprocessed_data_dir_path, picks=picks, event=event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc425d70-4c2a-48d2-8f09-135d878d2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_epochs_per_threshold = []\n",
    "filtered_ths = []\n",
    "for threshold in thresholds:\n",
    "    print(f'Threshold: {threshold}')\n",
    "    filter_per_channel = calculate_wavelet_filter(grand_average, scales=new_scales, signal_freq=500, central_freq=6, threshold_point=threshold)\n",
    "    filtered_ths.append(filter_per_channel)\n",
    "    \n",
    "    all_epochs_reconstructed = []\n",
    "    for idx, tfr_epochs in enumerate(tfr_epochs_participants):\n",
    "        reconstructed_epochs = tfr_filter_epochs(\n",
    "            tfr_epochs,\n",
    "            epochs_participants[idx],\n",
    "            per_channel_cwt_results = filter_per_channel\n",
    "        )\n",
    "        \n",
    "    \n",
    "        all_epochs_reconstructed.append(reconstructed_epochs)\n",
    "    reconstructed_epochs_per_threshold.append(all_epochs_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6fac8-0d8b-4c45-a024-e7a3fc82ff7e",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc587159-ec36-43c2-a4cc-f2f79e828ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_average_per_threshold = np.array([np.mean([np.mean(participant, axis=0) for participant in threshold_], axis=0) for threshold_ in reconstructed_epochs_per_threshold])\n",
    "\n",
    "grand_average_peak_amplitude = np.min(grand_average[0][50:150])\n",
    "filtered_grand_averages_amplitudes = [np.min(item[50:150]) for item in grand_average_per_threshold[:,0,:]]\n",
    "\n",
    "diffs = [item/grand_average_peak_amplitude for item in filtered_grand_averages_amplitudes]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(thresholds, diffs)\n",
    "ax.set_xticks(np.arange(0.1, 1.0, 0.1))\n",
    "plt.axhline(y=0.99, c='r', linestyle='--')\n",
    "plt.axvline(x=0.45, c='orange', linestyle='--')\n",
    "\n",
    "# plt.axhline(y=1.01, c='r', linestyle='--')\n",
    "# plt.axvline(x=0.4, c='orange', linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Amplitude reduction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ffaab-5b79-46f7-a356-53d1cc7c7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "x = np.linspace(-0.1, 0.9, np.array(all_epochs_reconstructed[0]).shape[-1])\n",
    "\n",
    "plt.plot(x, grand_average.flatten(), linestyle='--', label='original signal')\n",
    "\n",
    "for i in range(0, len(thresholds)):\n",
    "    plt.plot(x, grand_average_per_threshold[i,0,:], label=str(round(thresholds[i], 2)))\n",
    "\n",
    "plt.legend()\n",
    "# ax.legend(bbox_to_anchor=(0.7, 1.0))\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude (V)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c7430-ee64-49a6-891c-8ef6703e9efe",
   "metadata": {},
   "source": [
    "### Perform wavelet filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e4141-b429-4391-bee1-b4f5edc7dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a custom logger for preprocessing INFO\n",
    "logger_preprocessing_info = logging.getLogger('preprocessing_info')\n",
    "logger_preprocessing_info.setLevel(logging.INFO)\n",
    "logger_preprocessing_info.propagate = False\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a custom logger for errors\n",
    "logger_errors_info = logging.getLogger('errors')\n",
    "logger_errors_info.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85399fdd-6af3-49a8-b2e0-a7cabee98b4a",
   "metadata": {},
   "source": [
    "Set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0360014-aa8b-41a6-a664-ad26c1e75d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:26:53.363658Z",
     "start_time": "2023-12-17T11:26:53.335052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GNG | SST | Flanker\n",
    "paradigm = 'SST'\n",
    "case = 'RE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240f977-9611-421d-a4c4-6c1429418b74",
   "metadata": {},
   "source": [
    "Set paths base on globals values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fd6ed-f8e3-49c7-ac0e-5dd57111040b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:28:10.280614Z",
     "start_time": "2023-12-17T11:28:10.230824Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "logger_dir_path = f'data/joint/{paradigm}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e5518-58b2-43d6-a066-dc744511ab55",
   "metadata": {},
   "source": [
    "Set output files for loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8cf37-2d06-4099-aa6b-f05e339ee936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a file handler for preprocessing and set the level to INFO\n",
    "file_handler_preprocessing = logging.FileHandler(f'data/joint/{paradigm}/{case}_wavelets_info.txt')\n",
    "file_handler_preprocessing.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for preprocessing\n",
    "formatter_preprocessing = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_preprocessing.setFormatter(formatter_preprocessing)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_preprocessing_info.addHandler(file_handler_preprocessing)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a file handler for errors and set the level to INFO\n",
    "file_handler_errors = logging.FileHandler(f'data/joint/{paradigm}/{case}_wavelets_errors.txt')\n",
    "file_handler_errors.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for errors\n",
    "formatter_errors = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_errors.setFormatter(formatter_errors)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_errors_info.addHandler(file_handler_errors)\n",
    "\n",
    "##### MNE ###################################################\n",
    "# Create logger for MNE logs\n",
    "logger_f_name = f'data/joint/{paradigm}/{case}_wavelets_MNE-logs.txt'\n",
    "set_log_file(fname=logger_f_name, output_format=\"%(asctime)s - %(message)s\", overwrite=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bdf0d-c59d-45a0-9d70-822c6d50fd15",
   "metadata": {},
   "source": [
    "#### 1. Read all participants data and create grand averages per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e13b21-c2c9-43b9-9582-e92ca8840bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clean_data_dict(path_to_dir):\n",
    "    events_data_dict = dict()\n",
    "    id_list = [item.split('.')[0] for item in os.listdir(path_to_dir)]\n",
    "\n",
    "    for participant_id in id_list:\n",
    "        preprocessed_epochs = pd.read_pickle(f'{path_to_dir}{participant_id}.pkl')\n",
    "\n",
    "        epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "        drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "\n",
    "        assert len(drop_log) == len(epochs)\n",
    "\n",
    "        epochs_copy = epochs.copy().pick('eeg')\n",
    "        ch_names = epochs_copy.ch_names\n",
    "\n",
    "        for trial_idx, _ in enumerate(epochs_copy):\n",
    "            epoch = epochs_copy[trial_idx]\n",
    "            epoch_type = list(epoch.event_id.keys())\n",
    "            drop_log_item = drop_log[trial_idx]\n",
    "\n",
    "            trial_data = []\n",
    "            for idx, channel in enumerate(epoch.get_data(copy=True)[0]):\n",
    "                ch_name = ch_names[idx]\n",
    "                if ch_name in drop_log_item:\n",
    "                    trial_data.append([np.nan] * len(channel))\n",
    "                else:\n",
    "                    trial_data.append(channel)\n",
    "\n",
    "            trial_data = np.array(trial_data).reshape(len(ch_names), -1)\n",
    "            events_data_dict.setdefault(epoch_type[0], []).append(trial_data)\n",
    "    \n",
    "    return events_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54090a-dab1-457f-85a9-235b6dd531e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grand_averages_dict(events_data_dict):\n",
    "    grand_averages_dict = {}\n",
    "\n",
    "    for key, array in events_data_dict.items():\n",
    "        grand_average = np.nanmean(array, axis=0)\n",
    "        grand_averages_dict[key] = grand_average\n",
    "\n",
    "    return grand_averages_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e45aee-0e11-4318-8bd7-8ea1adb8f8ca",
   "metadata": {},
   "source": [
    "Create dict of clean data per event to create grand averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351f404-eb0b-4d65-baf7-b16f93b045c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_data_dict = create_clean_data_dict(path_to_dir = preprocessed_data_dir_path)\n",
    "grand_averages_dict = create_grand_averages_dict(events_data_dict)\n",
    "print(grand_averages_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45909e1f-8359-48f1-8f1e-1c0991a89672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save grand averages dict\n",
    "grand_averages_dict_ = {key: [value] for key, value in grand_averages_dict.items()}\n",
    "grand_averages_dict_df = pd.DataFrame(grand_averages_dict_)\n",
    "\n",
    "grand_averages_dict_df.to_pickle(f'data/joint/{paradigm}/grand_averages_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824aaf9-cc6b-4812-bd01-5c8330a276e5",
   "metadata": {},
   "source": [
    "#### 2. Create wavelet filter based on erroneous trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742a9e2-ea09-4530-b5ad-e0ef28c7f560",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_of_interest = 'error_response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad3384-804e-4d34-b9fc-f58945a0024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = np.geomspace(16,500,200) # from 1 to 30 Hz\n",
    "central_freq = 6\n",
    "signal_freq = 500\n",
    "threshold_point = 0.5 # chosen based on the threshold tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81933c4-6d41-4ad4-8793-a4754cd706c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read grand averages dict\n",
    "grand_averages_dict_df = pd.read_pickle(f'data/joint/{paradigm}/grand_averages_dict.pkl')\n",
    "grand_averages_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785486d0-5a4f-4757-b12f-1b9017de764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_filters_per_channel = calculate_wavelet_filter(\n",
    "    grand_average = grand_averages_dict_df[event_of_interest].to_numpy()[0], \n",
    "    scales = scales,\n",
    "    central_freq = central_freq, \n",
    "    signal_freq=signal_freq, \n",
    "    threshold_point=threshold_point\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef3e42-e999-40c6-b555-3aaca8f9c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Filter created for: {len(wavelet_filters_per_channel)} channels')\n",
    "print(f'Number of scales: {len(wavelet_filters_per_channel[0][2])}')\n",
    "print(f'Shape of Wx: {wavelet_filters_per_channel[0][0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02bfd1-5475-4e40-8dff-ffa6a4b7e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filter || TODO\n",
    "# pd.DataFrame({'wavelet_filter_FLA': [wavelet_filters_per_channel]}).to_pickle(f'data/joint/{paradigm}/wavelet_filters_per_channel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9786af-19fc-4097-bd51-8915685992f0",
   "metadata": {},
   "source": [
    "#### 3. Transform all epochs into tfrs, apply filter, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b92187-32a9-4b8b-8b4f-be2bd16092ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_behavioral_file(participant_id):\n",
    "    \n",
    "    behavioral_data_df = pd.read_csv(f'{behavioral_dir_path}beh_{participant_id}.csv')\n",
    "\n",
    "    trial_numerator = 1\n",
    "    trial_numbers = []\n",
    "    for i in range(0, len(behavioral_data_df)):\n",
    "        # if behavioral_data_df.iloc[i]['block_type'] != 'experiment':\n",
    "        #     trial_numbers.append(0)\n",
    "        # else:\n",
    "        trial_numbers.append(trial_numerator)\n",
    "        trial_numerator+=1\n",
    "    \n",
    "    behavioral_data_df['trial number'] = trial_numbers\n",
    "    return behavioral_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b9480-9736-4537-bea0-0872ab75e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_epochs_with_behavioral_data_long(epochs, drop_log, participant_id, case='RE'):\n",
    "    \n",
    "    if paradigm == 'GNG':\n",
    "    \n",
    "        # read behavioral file\n",
    "        behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "        beh_data_uninhibited_nogo_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial type'] != 'go') &\n",
    "            (behavioral_data_df['reaction'] == False)\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of uninhibited NOGO trials: {len(beh_data_uninhibited_nogo_responses_df)}')\n",
    "\n",
    "        beh_data_inhibited_nogo_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial type'] != 'go') &\n",
    "            (behavioral_data_df['reaction'] == True)\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of inhibited NOGO trials: {len(beh_data_inhibited_nogo_responses_df)}')\n",
    "\n",
    "        beh_data_correct_go_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial type'] == 'go') &\n",
    "            (behavioral_data_df['response'] == 'num_separator')\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "        results_df = pd.DataFrame()\n",
    "        epochs_df = pd.DataFrame()\n",
    "        behavioral_df = pd.DataFrame()\n",
    "\n",
    "        if case == 'RE':\n",
    "            behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        elif case == 'STIM':\n",
    "            behavioral_df = pd.concat([beh_data_uninhibited_nogo_responses_df, beh_data_correct_go_responses_df, beh_data_inhibited_nogo_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "        assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "        results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{participant_id}.pkl')\n",
    "        logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "    \n",
    "    if paradigm == 'FLA':\n",
    "        # read behavioral file\n",
    "        print('in saving')\n",
    "        behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "        beh_data_incorrect_incongruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'incongruent') &\n",
    "            (behavioral_data_df['reaction'] == 'incorrect') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of incorrect incongruent trials: {len(beh_data_incorrect_incongruent_responses_df)}')\n",
    "\n",
    "        beh_data_correct_incongruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'incongruent') &\n",
    "            (behavioral_data_df['reaction'] == 'correct') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of correct incongruent trials: {len(beh_data_correct_incongruent_responses_df)}')\n",
    "        \n",
    "        beh_data_incorrect_congruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'congruent') &\n",
    "            (behavioral_data_df['reaction'] == 'incorrect') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number incorrect congruent trials: {len(beh_data_incorrect_congruent_responses_df)}')\n",
    "\n",
    "        beh_data_correct_congruent_responses_df = behavioral_data_df[\n",
    "            (behavioral_data_df['block_type'] == 'experiment') &\n",
    "            (behavioral_data_df['trial_type'] == 'congruent') &\n",
    "            (behavioral_data_df['reaction'] == 'correct') &\n",
    "            ((behavioral_data_df['response'] == 'l') | (behavioral_data_df['response'] == 'r'))\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number correct congruent trials: {len(beh_data_correct_congruent_responses_df)}')\n",
    "\n",
    "        results_df = pd.DataFrame()\n",
    "        epochs_df = pd.DataFrame()\n",
    "        behavioral_df = pd.DataFrame()\n",
    "        \n",
    "        if case == 'RE':\n",
    "            behavioral_df = pd.concat([beh_data_incorrect_incongruent_responses_df, beh_data_correct_incongruent_responses_df, beh_data_incorrect_congruent_responses_df, beh_data_correct_congruent_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        elif case == 'STIM':\n",
    "            behavioral_df = pd.concat([beh_data_incorrect_incongruent_responses_df, beh_data_correct_incongruent_responses_df, beh_data_incorrect_congruent_responses_df, beh_data_correct_congruent_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "        assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "        results_df.to_pickle(f'{preprocessed_data_dir_path}preprocessed-beh_{participant_id}.pkl')\n",
    "        logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "     \n",
    "    if paradigm == 'SST':\n",
    "        # read behavioral file\n",
    "        behavioral_data_df = read_behavioral_file(participant_id)\n",
    "\n",
    "        beh_data_inhibited_stop_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'] == 0) &\n",
    "            (behavioral_data_df['RE_time'].isna())\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of correctly inhibited STOP trials: {len(beh_data_inhibited_stop_df)}')\n",
    "\n",
    "        beh_data_uninhibited_stop_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'] == 0) &\n",
    "            (behavioral_data_df['RE_time'].notna())\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number of incorrectly uninhibited STOP trials: {len(beh_data_uninhibited_stop_df)}')\n",
    "\n",
    "        beh_data_correct_go_responses_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'].isna()) &\n",
    "            # (behavioral_data_df['STOP_TYPE'] != 1) &\n",
    "            (behavioral_data_df['RE_key'] == behavioral_data_df['RE_true'])\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number correct GO trials: {len(beh_data_correct_go_responses_df)}')\n",
    "\n",
    "        beh_data_incorrect_go_responses_df = behavioral_data_df.iloc[30:][\n",
    "            (behavioral_data_df['STOP_TYPE'] != 0) &\n",
    "            (behavioral_data_df['STOP_TYPE'] != 1) &\n",
    "            (behavioral_data_df['RE_key'] != behavioral_data_df['RE_true']) &\n",
    "            (behavioral_data_df['RE_key'].notna())\n",
    "            ]\n",
    "        logger_preprocessing_info.info(f'Number incorrect GO trials: {len(beh_data_incorrect_go_responses_df)}')\n",
    "\n",
    "        results_df = pd.DataFrame()\n",
    "        epochs_df = pd.DataFrame()\n",
    "        behavioral_df = pd.DataFrame()\n",
    "        \n",
    "        if case == 'RE':\n",
    "            behavioral_df = pd.concat([beh_data_uninhibited_stop_df, beh_data_incorrect_go_responses_df, beh_data_correct_go_responses_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        elif case == 'STIM':\n",
    "            behavioral_df = pd.concat([beh_data_inhibited_stop_df, beh_data_uninhibited_stop_df]).sort_values(by='trial number')\n",
    "\n",
    "            logger_preprocessing_info.info(f'Len drop log: {len(drop_log)}')\n",
    "            logger_preprocessing_info.info(f'Len behavioral df: {len(behavioral_df)}')\n",
    "            assert len(behavioral_df) == len(drop_log), f'Number of events read from behavioral file ({len(behavioral_df)}) not equals number of events from drop_log ({len(drop_log)})'\n",
    "\n",
    "            for idx, _ in enumerate(epochs):\n",
    "                epoch = epochs[idx]\n",
    "                epoch_type = list(epoch.event_id.keys())\n",
    "                assert len(epoch_type) == 1, \\\n",
    "                    f'Single trial is not single. Length of epoch: {len(epoch_type)}. Error during trial-wise saving.'\n",
    "                drop_log_item = drop_log[idx]\n",
    "\n",
    "                this_df = pd.DataFrame({\n",
    "                    'epoch': [epoch],\n",
    "                    'event': epoch_type,\n",
    "                    'drop_log': [drop_log_item],\n",
    "                })\n",
    "\n",
    "                epochs_df = pd.concat([epochs_df, this_df], ignore_index=True)\n",
    "\n",
    "            # Set the indexes of epochs to match reactions\n",
    "            indexes = behavioral_df.index\n",
    "            epochs_df.set_index(indexes, inplace=True)\n",
    "            results_df = pd.concat([behavioral_df, epochs_df], axis=1)\n",
    "\n",
    "        else:\n",
    "            logger_preprocessing_info.info('Not implemented')\n",
    "\n",
    "        assert len(results_df) == len(behavioral_df) == len(epochs_df), f'Length of trial-wise dataframe ({len(results_df)}) not equals number of events from behavioral file ({len(behavioral_df)}) and number of epochs ({len(epochs_df)})'\n",
    "\n",
    "        results_df.to_pickle(f'{preprocessed_data_dir_path}wavelets/preprocessed-beh_{participant_id}.pkl')\n",
    "        logger_preprocessing_info.info('Epochs and behavioral data in long format saved to pickle.')\n",
    "        \n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c553fa-e5a2-4246-9612-2d0a690af31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = 'eeg'\n",
    "scales = np.geomspace(16,500,200) # from 1 to 30 Hz\n",
    "central_freq = 6\n",
    "signal_freq = 500\n",
    "# threshold_point = 0.45 # chosen based on the threshold tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0d4c9-a758-4d7b-9260-2f522613b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_dir_path = f'data/{paradigm}/behavioral/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d3659-682e-4700-99ea-04626c9fc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [item.split('.')[0] for item in os.listdir(preprocessed_data_dir_path)]\n",
    "# id_list = id_list[49:]\n",
    "\n",
    "epochs_participants_reconstructed = []\n",
    "epochs_participants = []\n",
    "# tfr_epochs_participants = []\n",
    "\n",
    "for participant_id in id_list:\n",
    "    # read data\n",
    "    print(participant_id)\n",
    "    logger_preprocessing_info.info(f'#### PARTICIPANT ID: {participant_id} #########')\n",
    "    logger_errors_info.info(f'#### PARTICIPANT ID: {participant_id} #########')\n",
    "    \n",
    "    try:\n",
    "        preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{participant_id}.pkl')\n",
    "        epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "        drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "\n",
    "        # save unfiltered data\n",
    "        # epochs_participants.append(epochs.get_data(picks=picks))\n",
    "\n",
    "        # transform data into TFR space\n",
    "        tfr_epochs = epochs_to_tfr(\n",
    "            epochs, \n",
    "            scales=scales, \n",
    "            picks=picks,\n",
    "            events='all',\n",
    "        )\n",
    "\n",
    "        # tfr_epochs_participants.append(tfr_epochs)\n",
    "\n",
    "        # filter data with created wavelet filter\n",
    "        reconstructed_epochs = tfr_filter_epochs(\n",
    "            tfr_epochs,\n",
    "            epochs.get_data(picks=picks),\n",
    "            per_channel_cwt_results = wavelet_filters_per_channel\n",
    "        )\n",
    "        # epochs_participants_reconstructed.append(reconstructed_epochs)\n",
    "    \n",
    "        assert epochs.get_data(picks=picks).shape == reconstructed_epochs.shape\n",
    "    \n",
    "        epochs_copy = epochs.copy().pick('eeg')\n",
    "\n",
    "        e_arr = mne.EpochsArray(\n",
    "                data = reconstructed_epochs, \n",
    "                info = epochs_copy.info,\n",
    "                events = epochs_copy.events,\n",
    "                tmin=epochs_copy.tmin,\n",
    "            )\n",
    "\n",
    "        _ = save_epochs_with_behavioral_data_long(\n",
    "            e_arr,\n",
    "            drop_log,\n",
    "            participant_id.split('_')[1],\n",
    "            case=case,\n",
    "        )\n",
    "    except Exception as e:        \n",
    "        logger_errors_info.info(f\"{e}\")\n",
    "    \n",
    "    logger_preprocessing_info.info(f'\\n')\n",
    "    logger_errors_info.info(f'\\n')\n",
    "\n",
    "print(f'##########\\n DONE\\n')       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5439e35-b45e-45dd-bab5-feaa0c81c223",
   "metadata": {},
   "source": [
    "Check similarity between grand average of original and filtered signal per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314cc8-d593-4295-b3a8-42eae9384ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_idx = epochs.info.ch_names.index('FCz')\n",
    "channel_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1efd6d-dc3a-4bc0-ba98-cd0d7249f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, epochs_participants[0].shape[-1])\n",
    "for i in range(0, len(epochs_participants)):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(x, np.mean(epochs_participants[i], axis=0)[channel_idx].flatten())\n",
    "    plt.plot(x, np.mean(epochs_participants_reconstructed[i], axis=0)[channel_idx].flatten())\n",
    "    \n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (V)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca158a44-05f5-4582-b16e-fe4ced24a06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf77aba-86e8-4614-bf32-fc33123ca613",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177200b51e18d514",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-14T13:20:21.386590Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def signal_cwt(signal, central_freq = 6, signal_freq=500):\n",
    "    x = signal.flatten()\n",
    "\n",
    "    # construct wavelet function\n",
    "    wavelet = Wavelet(('morlet', {'mu': central_freq}))\n",
    "    Wx, scales = cwt(x, wavelet, fs=signal_freq, scales='log-piecewise', padtype='wrap', l1_norm=True, nv=10)\n",
    "    # Wx, scales = cwt(x, wavelet, fs=signal_freq, )\n",
    "\n",
    "    return Wx\n",
    "\n",
    "def epochs_to_tfr(epochs, picks=['FCz', 'Cz'], events=['error_response']):\n",
    "    '''\n",
    "    \n",
    "    :param epochs: \n",
    "    :param picks: \n",
    "    :param events: \n",
    "    :return: ndarray of shape (n_events, n_channels, n_freqs, n_timepoints)\n",
    "    '''\n",
    "    epochs_picked = epochs.copy()[events].pick(picks)\n",
    "    tfr_epochs = []\n",
    "    for idx, _ in enumerate(epochs_picked):\n",
    "        epoch = epochs_picked[idx]\n",
    "        epoch_data = epoch.get_data(copy=True)\n",
    "\n",
    "        tfr_channel_data = []\n",
    "        for ch_name, ch_idx in zip(epochs_picked.info['ch_names'],\n",
    "                                   np.arange(0, len(epochs_picked.info['ch_names']))):\n",
    "            channel_data = epoch_data[0,ch_idx,:]\n",
    "    \n",
    "            channel_wavelet_data = signal_cwt(channel_data)\n",
    "            tfr_channel_data.append(channel_wavelet_data)\n",
    "        tfr_epochs.append(tfr_channel_data)\n",
    "    \n",
    "    tfr_epochs = np.array(tfr_epochs)\n",
    "    return tfr_epochs\n",
    "\n",
    "def filter_signal(Wx, x, mask, wavelet, scales):\n",
    "    time_domain_signal = icwt(mask * Wx, wavelet, scales, nv=10, padtype='wrap', l1_norm=True, x_mean = np.mean(x))\n",
    "\n",
    "    return time_domain_signal\n",
    "\n",
    "# def tfr_filter_epochs(tfr, mask, wavelet, scales):\n",
    "#     filtered_epochs = []\n",
    "#     for epochs in tfr:\n",
    "#         filtered_channel_data = []\n",
    "#         for channel_data in epochs:\n",
    "#             signal = channel_data\n",
    "#             reconstructed_signal = filter_signal(signal, mask, wavelet, scales)\n",
    "#             filtered_channel_data.append(reconstructed_signal)\n",
    "#         filtered_epochs.append(filtered_channel_data)\n",
    "\n",
    "#     filtered_epochs = np.array(filtered_epochs)    \n",
    "#     return filtered_epochs\n",
    "\n",
    "def tfr_filter_epochs(tfr, original_signal, per_channel_cwt_results):\n",
    "    filtered_epochs = []\n",
    "    for epochs, org_epoch in zip(tfr,original_signal) :\n",
    "        filtered_channel_data = []\n",
    "        for idx, channel_data in enumerate(epochs):\n",
    "            mask = per_channel_cwt_results[idx][0]\n",
    "            wavelet = per_channel_cwt_results[idx][1]\n",
    "            scales = per_channel_cwt_results[idx][2]\n",
    "            signal = channel_data\n",
    "            reconstructed_signal = filter_signal(signal, org_epoch[idx], mask, wavelet, scales)\n",
    "            filtered_channel_data.append(reconstructed_signal)\n",
    "        filtered_epochs.append(filtered_channel_data)\n",
    "\n",
    "    filtered_epochs = np.array(filtered_epochs)    \n",
    "    return filtered_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de930e97f6788b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-14T13:20:22.998028Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wavelet_filter(grand_average, central_freq = 6, signal_freq=500, threshold_point=0.85):\n",
    "    results_per_channel = []\n",
    "    for channel_grand_average in grand_average:\n",
    "        x = channel_grand_average.flatten()\n",
    "        t = np.linspace(-0.1, 0.9, len(x))\n",
    "        # construct wavelet function\n",
    "        wavelet = Wavelet(('morlet', {'mu': central_freq}))\n",
    "        Wx, scales = cwt(x, wavelet, fs=signal_freq, scales='log-piecewise', padtype='wrap', l1_norm=True, nv=10)\n",
    "\n",
    "        freq = scale_to_freq(scales, wavelet, N=len(x), fs=signal_freq)\n",
    "        # Compute and normalize the power spectrum from the CWT coefficients\n",
    "        power_spectrum = np.abs(Wx)**2\n",
    "        normalized_power_spectrum = power_spectrum / np.sum(power_spectrum)\n",
    "\n",
    "        # Flatten the normalized power spectrum for CDF calculation\n",
    "        flattened_spectrum = normalized_power_spectrum.flatten()\n",
    "\n",
    "        # Use the Kaplan–Meier estimator\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(durations=flattened_spectrum, event_observed=np.ones_like(flattened_spectrum))\n",
    "\n",
    "        # Get the CDF values from the Kaplan–Meier estimator\n",
    "        cdf_values = 1 - kmf.survival_function_.KM_estimate\n",
    "\n",
    "        # Calculate the threshold\n",
    "        threshold = threshold_point * (np.max(cdf_values) - np.min(cdf_values)) + np.min(cdf_values)\n",
    "\n",
    "        # Plot the empirical CDF and the filtering model\n",
    "        plt.step(kmf.survival_function_.index, cdf_values, where='post', label='Empirical CDF')\n",
    "        plt.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "        plt.title('Empirical CDF and Filtering Model')\n",
    "        plt.xlabel('Wavelet Coefficient')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Find the value of wavelets coefficient that are above threshold\n",
    "        cutoff_wavelet_index = np.where(cdf_values > threshold)[0][0]\n",
    "        cutoff_wavelet_coef = kmf.survival_function_.index[cutoff_wavelet_index]\n",
    "        print(f'Estimated threshold value for wavelet coefficients: {cutoff_wavelet_coef}')\n",
    "\n",
    "        cwt_result_threshold_mask = np.where(normalized_power_spectrum >= cutoff_wavelet_coef, 1, 0)\n",
    "\n",
    "        # Plot the CWT result\n",
    "        plt.figure(figsize=(12, 16))\n",
    "\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.imshow(np.abs(Wx), extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('CWT Magnitude')\n",
    "\n",
    "        # \n",
    "        plt.subplot(4, 1, 2)\n",
    "        plt.imshow(normalized_power_spectrum, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('Normalized Power Spectrum')\n",
    "\n",
    "        plt.subplot(4, 1, 3)\n",
    "        plt.imshow(cwt_result_threshold_mask, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('Threshold Mask')\n",
    "\n",
    "        plt.subplot(4, 1, 4)\n",
    "        plt.imshow(cwt_result_threshold_mask*np.abs(Wx), extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        plt.title('Thresholded Grand Average - Examle')\n",
    "        plt.show()\n",
    "        \n",
    "        results_per_channel.append((cwt_result_threshold_mask, wavelet, scales))\n",
    "\n",
    "    return results_per_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c3f6e-f9ff-449e-9bcc-f5338b541000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grand_average(path_to_dir, picks, event):\n",
    "    id_list = [item.split('.')[0] for item in os.listdir(path_to_dir)]\n",
    "    all_evokeds = []\n",
    "    \n",
    "    for id_ in id_list:\n",
    "        preprocessed_epochs = pd.read_pickle(f'{path_to_dir}{id_}.pkl')\n",
    "        clean_epochs, _ = reject_bad_trials(preprocessed_epochs['epochs'].to_numpy().flatten()[0], preprocessed_epochs['drop_log'].to_numpy().flatten()[0])\n",
    "        if len(clean_epochs) < 6:\n",
    "            logger_errors_info.info(f'Participant has only {len(clean_epochs)} artifact-free trials')\n",
    "        else:\n",
    "            all_evokeds.append(clean_epochs[event].average().get_data(picks=picks))\n",
    "    \n",
    "    all_evokeds = np.array(all_evokeds)    \n",
    "    grand_average = np.mean(all_evokeds, axis=0)\n",
    "    \n",
    "    return grand_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b3601-b889-48b9-893f-41223a1b2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wavelet_filter(path_to_dir, picks=['FCz', 'Cz'], event='error_response', threshold=0.85, central_freq=6):\n",
    "    grand_average = get_grand_average(path_to_dir, picks=picks, event=event)\n",
    "    filter_per_channel = calculate_wavelet_filter(grand_average, central_freq=central_freq, threshold_point=threshold)\n",
    "    \n",
    "    return filter_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284f76f-54df-4dd2-8d03-b168ca4b32d3",
   "metadata": {},
   "source": [
    "## Wavelet filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7756824-2aaa-4c0a-b7bd-0b2c34e876d6",
   "metadata": {},
   "source": [
    "Set globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f4153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:26:53.363658Z",
     "start_time": "2023-12-17T11:26:53.335052Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GNG | SST | Flanker\n",
    "paradigm = 'SST'\n",
    "case = 'RE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffeb707-2392-4fb1-87c5-023d6ec0868b",
   "metadata": {},
   "source": [
    "Set paths base on globals values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99442d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T11:28:10.280614Z",
     "start_time": "2023-12-17T11:28:10.230824Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "logger_dir_path = f'data/joint/{paradigm}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d60eac-5684-49d7-9c3e-67038a53e512",
   "metadata": {},
   "source": [
    "Set output files for loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc5ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## PREPROCESSING ##############################################\n",
    "# Create a file handler for preprocessing and set the level to INFO\n",
    "file_handler_preprocessing = logging.FileHandler(f'data/joint/{paradigm}/{case}_wavelets_info.txt')\n",
    "file_handler_preprocessing.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for preprocessing\n",
    "formatter_preprocessing = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_preprocessing.setFormatter(formatter_preprocessing)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_preprocessing_info.addHandler(file_handler_preprocessing)\n",
    "\n",
    "######## ERRORS ##############################################\n",
    "# Create a file handler for errors and set the level to INFO\n",
    "file_handler_errors = logging.FileHandler(f'data/joint/{paradigm}/{case}_wavelets_errors.txt')\n",
    "file_handler_errors.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and add it to the file handler for errors\n",
    "formatter_errors = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler_errors.setFormatter(formatter_errors)\n",
    "\n",
    "# Add the file handler for method A to the logger for preprocessing\n",
    "logger_errors_info.addHandler(file_handler_errors)\n",
    "\n",
    "##### MNE ###################################################\n",
    "# Create logger for MNE logs\n",
    "logger_f_name = f'data/joint/{paradigm}/{case}_wavelets_MNE-logs.txt'\n",
    "set_log_file(fname=logger_f_name, output_format=\"%(asctime)s - %(message)s\", overwrite=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7029274-ca99-47c6-8fdc-fa572960f7c1",
   "metadata": {},
   "source": [
    "### Test wavelet filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e17a7a-f8ae-46a5-b30f-28022bf23dbd",
   "metadata": {},
   "source": [
    "#### Test quality of wavelet invers transform\n",
    "\n",
    "See: https://dsp.stackexchange.com/questions/87097/why-is-inverse-cwt-inexact-inaccurate/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86145bce-c374-48e3-8364-580bb04cf130",
   "metadata": {},
   "source": [
    "Perform wavelet deconstruction and inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "id_list = [item.split('.')[0] for item in os.listdir(preprocessed_data_dir_path)]\n",
    "\n",
    "all_epochs_reconstructed = []\n",
    "all_epochs_original = []\n",
    "diffs = []\n",
    "nv=None\n",
    "\n",
    "# create scales\n",
    "# preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_list[0]}.pkl')\n",
    "# epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "# drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "\n",
    "# x = epochs[event].get_data(picks=picks)[0].flatten()\n",
    "# wavelet = Wavelet(('morlet', {'mu': 6}))\n",
    "\n",
    "# Wx, scales = cwt(x, wavelet, fs=500, scales='log-piecewise', padtype='wrap', l1_norm=True, nv=nv)\n",
    "# new_scales = scales[34:]\n",
    "new_scales = np.geomspace(16,500,200)\n",
    "\n",
    "for id_ in id_list:\n",
    "    # read data\n",
    "    preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_}.pkl')\n",
    "    epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "         \n",
    "    drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "    \n",
    "    epochs_data = epochs.copy()['error_response'].pick(['FCz'])\n",
    "    \n",
    "    # transform data into TFR space\n",
    "    wavelet = Wavelet(('morlet', {'mu': 6}))\n",
    "    \n",
    "    participant_epochs_reconstructed = []\n",
    "    participant_epochs_original = []\n",
    "    participant_diffs = []\n",
    "    for epoch in epochs_data:\n",
    "        \n",
    "        Wx, scales = cwt(epoch.flatten(), wavelet, fs=500, scales=new_scales, padtype='wrap', l1_norm=True, nv=None)\n",
    "        time_domain_signal = icwt(Wx, wavelet, scales=new_scales, nv=None, padtype='wrap', l1_norm=True, x_mean=np.mean(epoch.flatten()))\n",
    "\n",
    "        diff = abs(np.mean(epoch.flatten()) - np.mean(time_domain_signal))\n",
    "        \n",
    "        participant_epochs_reconstructed.append(time_domain_signal)\n",
    "        participant_epochs_original.append(epoch.flatten())\n",
    "        participant_diffs.append(diff) \n",
    "        \n",
    "    all_epochs_reconstructed.append(participant_epochs_reconstructed)\n",
    "    all_epochs_original.append(participant_epochs_original)\n",
    "    diffs.append(participant_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693e01b-bd6e-4fbf-9111-c433d1b8bf30",
   "metadata": {},
   "source": [
    "Plot per participant grand average similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25007b6-b8e4-4742-a355-c864afe0d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, np.array(all_epochs_reconstructed[0]).shape[-1])\n",
    "for i in range(0, len(all_epochs_reconstructed)):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(x, np.mean(all_epochs_original[i], axis=0).flatten())\n",
    "    plt.plot(x, np.mean(all_epochs_reconstructed[i], axis=0).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792be28-dfa0-4a1d-8c33-512229cb95dc",
   "metadata": {},
   "source": [
    "Calculate differences between original and reconstructed signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242078d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-7 # 0.1 uV\n",
    "\n",
    "for idx, paricipant in enumerate(diffs):\n",
    "    print(f'IDX: {idx}\\n{paricipant}\\n')\n",
    "    exceed = np.array([True if x > epsilon else False for x in paricipant])\n",
    "    print(exceed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e07689-2f2e-4682-8e24-b2e39eb36b04",
   "metadata": {},
   "source": [
    "Test wavelet inverse transform quality per-participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, np.array(all_epochs_reconstructed[0]).shape[-1])\n",
    "\n",
    "idx = 100\n",
    "participant_reconstructed = all_epochs_reconstructed[idx]\n",
    "participant_original = all_epochs_original[idx]\n",
    "\n",
    "for i in range(0, len(participant_reconstructed)):\n",
    "    plt.figure()\n",
    "    print(i)\n",
    "    plt.plot(x, participant_original[i])\n",
    "    plt.plot(x, participant_reconstructed[i])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235d0b5-54ef-466b-8f38-e59ceb194cd6",
   "metadata": {},
   "source": [
    "#### Test thresholds and their impact into amplitude reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27790eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir_path = f'data/joint/{paradigm}/preprocessed/{case}/'\n",
    "thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "picks = ['FCz']\n",
    "event = 'error_response'\n",
    "\n",
    "id_list = [item.split('.')[0] for item in os.listdir(preprocessed_data_dir_path)]\n",
    "tfr_epochs_participants = []\n",
    "epochs_participants = []\n",
    "\n",
    "# create scales\n",
    "# preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_list[0]}.pkl')\n",
    "# epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "# drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "\n",
    "# x = epochs[event].get_data(picks=picks)[0].flatten()\n",
    "# wavelet = Wavelet(('morlet', {'mu': 6}))\n",
    "new_scales = np.geomspace(16,500,200) # from 1 to 30 Hz\n",
    "\n",
    "for id_ in id_list:\n",
    "    # read data\n",
    "    preprocessed_epochs = pd.read_pickle(f'{preprocessed_data_dir_path}{id_}.pkl')\n",
    "    epochs = preprocessed_epochs['epochs'].to_numpy().flatten()[0]\n",
    "    drop_log = preprocessed_epochs['drop_log'].to_numpy().flatten()[0]\n",
    "    \n",
    "    # save unfiltered data\n",
    "    epochs_participants.append(epochs[event].get_data(picks=picks))\n",
    "    \n",
    "    # transform data into TFR space\n",
    "    tfr_epochs = epochs_to_tfr(epochs, scales=new_scales, picks=picks, events=event)\n",
    "    # save tfr data\n",
    "    tfr_epochs_participants.append(tfr_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731714ba-fa82-4422-9851-0344eb03a2f6",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8f90a-3651-4c34-b92c-2ee501366503",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_average_per_threshold = np.array([np.mean([np.mean(participant, axis=0) for participant in threshold_], axis=0) for threshold_ in reconstructed_epochs_per_threshold])\n",
    "\n",
    "grand_average_peak_amplitude = np.min(grand_average[0][50:150])\n",
    "filtered_grand_averages_amplitudes = [np.min(item[50:150]) for item in grand_average_per_threshold[:,0,:]]\n",
    "\n",
    "diffs = [item/grand_average_peak_amplitude for item in filtered_grand_averages_amplitudes]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(thresholds[:-1], diffs)\n",
    "ax.set_xticks(np.arange(0.1, 1.0, 0.1))\n",
    "plt.axhline(y=0.85, c='r', linestyle='--')\n",
    "plt.axvline(x=0.7, c='orange', linestyle='--')\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Amplitude reduction\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0258fc-e33e-420a-b911-5528d512711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "x = np.linspace(-0.1, 0.9, np.array(all_epochs_reconstructed[0]).shape[-1])\n",
    "\n",
    "plt.plot(x, grand_average.flatten(), linestyle='--', label='original signal')\n",
    "\n",
    "for i in range(0, len(thresholds[:-1])):\n",
    "    plt.plot(x, grand_average_per_threshold[i,0,:], label=str(round(thresholds[i], 2)))\n",
    "\n",
    "plt.legend()\n",
    "# ax.legend(bbox_to_anchor=(0.7, 1.0))\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude (V)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657626a-a87d-459b-ab85-602e9e0ddd2e",
   "metadata": {},
   "source": [
    "### Perform wavelet filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ac4a9-1027-48d8-98cb-66df849cae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = ['FCz', 'Cz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bee8e801423d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:00:38.229417Z",
     "start_time": "2023-12-12T19:00:37.510917Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filter_per_channel = create_wavelet_filter(\n",
    "    preprocessed_data_dir_path, \n",
    "    picks=picks, \n",
    "    event='error_response', \n",
    "    threshold=0.7,\n",
    "    central_freq=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6fff3-6361-4660-9a9c-296d04a45682",
   "metadata": {},
   "source": [
    "Transform all epochs into tfrs and apply filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da0e7b4af40fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T18:59:49.242422Z",
     "start_time": "2023-12-12T18:59:48.878762Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d04fb235-be26-4ac7-8dae-2f8bd0532f6b",
   "metadata": {},
   "source": [
    "Check similarity between grand average of original and filtered signal per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d985b-92b9-4b1b-8532-7994f04c1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, all_epochs[0].shape[-1])\n",
    "for i in range(0, len(all_epochs)):\n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(x, np.mean(all_epochs[i], axis=0)[0].flatten())\n",
    "    plt.plot(x, np.mean(all_epochs_reconstructed2[i], axis=0)[0].flatten())\n",
    "    \n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (V)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88390d-10da-41c2-abee-8a0d8b47844c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e241e-4020-4d23-aa9d-feab5740cb8d",
   "metadata": {},
   "source": [
    "## Generate PCA components:\n",
    "\n",
    "1. Average filtered with wavelets single trials into grand average.\n",
    "2. Create variability matrix base on the grand average: variability in peak latency and variability in waveform compression.\n",
    "3. Perform PCA decomposition on set of modified ERPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea1df9af3c377e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:19:41.671664Z",
     "start_time": "2023-12-13T08:19:41.653115Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def ms_to_tp(value_in_ms, freq=500):\n",
    "    \"\"\"\n",
    "    Only for relative conversion of the lengths\n",
    "    :param value_in_ms: \n",
    "    :param freq: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ms_unit = freq/1000\n",
    "    value_in_tp = int(value_in_ms*ms_unit)\n",
    "    return value_in_tp\n",
    "\n",
    "def stretch(xs, coef, centre):\n",
    "    \"\"\"Scale a list by a coefficient around a point in the list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xs : list\n",
    "        Input values.\n",
    "    coef : float\n",
    "        Coefficient to scale by.\n",
    "    centre : int\n",
    "        Position in the list to use as a centre point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "\n",
    "    \"\"\"\n",
    "    grain = 100\n",
    "\n",
    "    stretched_array = np.repeat(xs, grain * coef)\n",
    "    stretched_array = np.array(stretched_array)\n",
    "    result = [chunk.mean() for chunk in chunks(stretched_array, grain)]\n",
    "\n",
    "    pivot_point = int(centre * coef)\n",
    "    first = pivot_point - centre\n",
    "    last = pivot_point + len(xs) - centre\n",
    "    result = result[first:last]\n",
    "\n",
    "    assert len(result) == len(xs), \"Length should be preserved\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunks(iterable, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from iterable.\n",
    "    Source: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python#answer-312464\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n",
    "# todo: implement compressing\n",
    "def generate_variability_matrix(X):\n",
    "    '''\n",
    "    \n",
    "    :param X: ndarray of shape (n_timepoints,)\n",
    "        Grand Average on given channel \n",
    "    :return: \n",
    "    '''\n",
    "    # Find the peak latency in the grand average signal /in tp\n",
    "    peak_latency_tp = np.argmin(X)\n",
    "    print(f'Peak latency in tp: {peak_latency_tp}')\n",
    "\n",
    "    # Parameters\n",
    "    latency_shifts = np.arange(-ms_to_tp(60), ms_to_tp(60), ms_to_tp(5))  # From -50 to 50 ms in steps of 5 ms\n",
    "    width_changes = np.arange(1, 1.5, 0.02)  # From 1 to 2 in steps of 0.05\n",
    "\n",
    "    # Initialize a list to store modified ERP responses\n",
    "    modified_responses = []\n",
    "\n",
    "    # Enumerate through latency shifts and width changes\n",
    "    for width_change in width_changes:\n",
    "        for latency_shift in latency_shifts:\n",
    "        # Apply latency shift\n",
    "            evoked_shifted = np.roll(X.flatten(), int(latency_shift))\n",
    "\n",
    "            # Calculate the stretched array\n",
    "            evoked_stretched = np.array(stretch(evoked_shifted, coef=width_change, centre=peak_latency_tp))\n",
    "            modified_responses.append(evoked_stretched)\n",
    "\n",
    "    # Convert the list of modified responses to a numpy array\n",
    "    modified_responses = np.array(modified_responses)\n",
    "\n",
    "    return modified_responses\n",
    "\n",
    "def create_variability_PCA_components(variability_matrix, n_components=3):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X = variability_matrix.T\n",
    "    X_transformed = pca.fit_transform(X)\n",
    "    \n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5df6560db0eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:19:41.989122Z",
     "start_time": "2023-12-13T08:19:41.967302Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create grand average of filtered signal\n",
    "wavelets_path = f'data/joint/{paradigm}/preprocessed/wavelets/'\n",
    "id_list = [item[:-4] for item in os.listdir(wavelets_path)]\n",
    "all_epochs = []\n",
    "\n",
    "for id_ in id_list:\n",
    "    preprocessed_filtered_epochs_df = pd.read_pickle(f'{wavelets_path}{id_}.pkl')\n",
    "    preprocessed_filtered_epochs = preprocessed_filtered_epochs_df['epochs'].to_numpy()[0]\n",
    "    # clean_epochs, _ = reject_bad_trials(preprocessed_epochs['epochs'].to_numpy().flatten()[0], preprocessed_epochs['drop_log'].to_numpy().flatten()[0])\n",
    "    # if len(clean_epochs) < 6:\n",
    "    #     logger_errors_info.info(f'Participant has only {len(clean_epochs)} artifact-free trials')\n",
    "    # else:\n",
    "    #     all_evokeds.append(clean_epochs[event].average().get_data(picks=picks))\n",
    "\n",
    "    all_epochs.append(preprocessed_filtered_epochs)\n",
    "grand_average = np.mean(np.array([np.mean(item, axis=0) for item in all_epochs]), axis=0)\n",
    "grand_average.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2305c2-3ef5-4125-897d-4be412dc33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-0.1, 0.9, grand_average.shape[-1]), grand_average[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcbe35427979d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:19:43.915026Z",
     "start_time": "2023-12-13T08:19:42.646329Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2. Generate variability matrix\n",
    "variability_matrices = []\n",
    "for channel_grand_average in grand_average:\n",
    "    variability_matrix = generate_variability_matrix(channel_grand_average)\n",
    "    variability_matrices.append(variability_matrix)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    sns.heatmap(\n",
    "        variability_matrix,\n",
    "        center=0,\n",
    "        cmap='Spectral'  \n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0489350e792eb70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:19:45.103244Z",
     "start_time": "2023-12-13T08:19:45.003880Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 3. get PCA components\n",
    "pca_per_channel = []\n",
    "for variability_matrix in variability_matrices:\n",
    "    pca = create_variability_PCA_components(variability_matrices[0])\n",
    "    pca_per_channel.append(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c21c392b85559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:19:45.980220Z",
     "start_time": "2023-12-13T08:19:45.906404Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "PCA_comp = pca_per_channel[0].T\n",
    "x = np.linspace(-0.1, 0.9, grand_average.shape[-1])\n",
    "plt.plot(x, PCA_comp[0])\n",
    "plt.plot(x, PCA_comp[1])\n",
    "plt.plot(x, PCA_comp[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9cea6-3060-4cd2-bc6c-2c17ef722504",
   "metadata": {},
   "source": [
    "### Regress signal on PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358626881b14e26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:19:48.737922Z",
     "start_time": "2023-12-13T08:19:48.715120Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def regress_signal_on_PCA(epochs, PCA_list):\n",
    "    filtered_epochs = []\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        filtered_channel_data = []\n",
    "        for idx, channel_data in enumerate(epoch):\n",
    "            pca = PCA_list[idx]\n",
    "            lm = LinearRegression()\n",
    "            lm.fit(X=pca, y=channel_data.flatten())\n",
    "            epoch_pred = lm.predict(pca)\n",
    "            filtered_channel_data.append(epoch_pred)\n",
    "        filtered_epochs.append(filtered_channel_data)\n",
    "    filtered_epochs = np.array(filtered_epochs)\n",
    "    return filtered_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdccca7-0431-48a2-86e9-3612a412cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs_pca_filtered = []\n",
    "for participant_data in all_epochs:\n",
    "    filtered_pca_epochs = regress_signal_on_PCA(participant_data, pca_per_channel)\n",
    "    all_epochs_pca_filtered.append(filtered_pca_epochs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ed94cb4bff67c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:21:51.366940Z",
     "start_time": "2023-12-13T08:21:51.287872Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# filtered_pca_epochs = regress_signal_on_PCA(reconstructed_epochs, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee473dd-49dc-40f6-a56e-38b108bf601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, grand_average.shape[-1])\n",
    "\n",
    "for i in range(0, len(all_epochs_pca_filtered)):\n",
    "    plt.figure()\n",
    "\n",
    "    # plt.plot(x, epochs_preprocessed['error_response'].average().get_data(picks=['FCz']).flatten(), label='original signal')\n",
    "    plt.plot(x, np.mean(all_epochs[i], axis=0)[0], label = 'wavelet filtered signal')\n",
    "    plt.plot(x, np.mean(all_epochs_pca_filtered[i], axis=0)[0], label='PCA filtered signal')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c68d96c5e5e5f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:21:55.394938Z",
     "start_time": "2023-12-13T08:21:55.253389Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, grand_average.shape[-1])\n",
    "\n",
    "for i in range(0, len(all_epochs_pca_filtered)):\n",
    "    plt.figure()\n",
    "\n",
    "    # plt.plot(x, epochs_preprocessed['error_response'].average().get_data(picks=['FCz']).flatten(), label='original signal')\n",
    "    plt.plot(x, np.mean(all_epochs[i], axis=0)[0], label = 'wavelet filtered signal')\n",
    "    plt.plot(x, np.mean(all_epochs_pca_filtered[i], axis=0)[0], label='PCA filtered signal')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf984c2-5ef2-4fd6-bce3-75801c6f1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.9, grand_average.shape[-1])\n",
    "idx = 222\n",
    "\n",
    "for i in range(0, len(all_epochs[idx])):\n",
    "    plt.figure()\n",
    "\n",
    "    # plt.plot(x, epochs_preprocessed['error_response'].average().get_data(picks=['FCz']).flatten(), label='original signal')\n",
    "    plt.plot(x, all_epochs[idx][i][0], label = 'wavelet filtered signal')\n",
    "    plt.plot(x, all_epochs_pca_filtered[idx][i][0], label='PCA filtered signal')\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474eac9c2534e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849a664e-56a2-4e92-a606-541b3f95524b",
   "metadata": {},
   "source": [
    "---\n",
    "## For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fa6422dc92d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:31.040870Z",
     "start_time": "2023-12-12T17:58:29.153416Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_fname = 'data/raw/A-GNG-000.bdf'\n",
    "raw = mne.io.read_raw_bdf(\n",
    "    input_fname,\n",
    "    eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "    exclude=['EXG5', 'EXG6'],\n",
    "    preload=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    raw = raw.set_montage('biosemi64')\n",
    "except ValueError as e:\n",
    "    if '[\\'EXG7\\', \\'EXG8\\']' in e.args[0]:\n",
    "        raw = raw.set_montage('biosemi64', on_missing='ignore')\n",
    "        print('On missing')\n",
    "    else:\n",
    "        print('Lacks important channels!')\n",
    "\n",
    "\n",
    "file_path = 'data/raw/triggerMap_A-GNG-000.txt'\n",
    "trigger_map = read_trigger_map(file_path)\n",
    "raw_new_triggers = replace_trigger_names(raw, trigger_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dd0f7f951dedd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:33.426265Z",
     "start_time": "2023-12-12T17:58:31.943568Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. re-reference: to mastoids\n",
    "raw_ref = raw_new_triggers.copy().set_eeg_reference(ref_channels=['EXG7', 'EXG8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcc712ab8f4773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:37.133842Z",
     "start_time": "2023-12-12T17:58:33.427322Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2. Resampling\n",
    "raw_resampled = raw_ref.copy().resample(sfreq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07bc951d9212687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:39.346161Z",
     "start_time": "2023-12-12T17:58:39.322018Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # (Filter)\n",
    "# # 2. 4-th order Butterworth filters\n",
    "# raw_filtered = raw_resampled.copy().filter(\n",
    "#     l_freq=.1,\n",
    "#     h_freq=30.0,\n",
    "#     n_jobs=10,\n",
    "#     method='iir',\n",
    "#     iir_params=None,\n",
    "#     picks=['eeg', 'eog']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54d8a39ef306e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:41.831006Z",
     "start_time": "2023-12-12T17:58:40.335739Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 3. Detrending, Segmentation, and first baseline correction\n",
    "\n",
    "epochs = create_epochs(\n",
    "    # raw_resampled,\n",
    "    raw_resampled,\n",
    "    tmin = -.1,\n",
    "    tmax = .9,\n",
    "    baseline = (-0.1, 0),\n",
    "    detrend = 1,\n",
    "    events_to_select = response_event_dict,  # response_event_dict\n",
    "    new_events_dict = new_response_event_dict,  # new_response_event_dict\n",
    "    events_mapping = events_mapping,  # events_mapping\n",
    "    reject = None,\n",
    "    reject_by_annotation = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba8a3f6c7a0015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:51.803335Z",
     "start_time": "2023-12-12T17:58:51.706866Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 4. ocular artifact correction with Gratton\n",
    "epochs_eog_corrected = ocular_correction_gratton(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86f232015f1388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:52.636811Z",
     "start_time": "2023-12-12T17:58:52.567261Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 5. Second re-baseline\n",
    "epochs_eog_corrected.apply_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea74eead5e0e019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:55.144635Z",
     "start_time": "2023-12-12T17:58:54.785753Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 6. Mark bad trials\n",
    "drop_log = find_bad_trials(epochs_eog_corrected, picks=['FCz','Cz'])\n",
    "drop_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d66418f5d06e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:58:58.745590Z",
     "start_time": "2023-12-12T17:58:58.684949Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 7. Reject bad trials\n",
    "clean_epochs, drop_log = reject_bad_trials(epochs_eog_corrected, drop_log)\n",
    "print(clean_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084caba1aa3fa2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T17:59:41.812636Z",
     "start_time": "2023-12-12T17:59:41.592703Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = clean_epochs.copy().pick(['FCz']).average().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe64e4-f2bf-435f-be02-86b1afcdd640",
   "metadata": {},
   "source": [
    "## Wavelets transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df115496a4dd8e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T11:12:47.997942Z",
     "start_time": "2023-12-12T11:12:47.965284Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pick = ['FCz']\n",
    "grand_average = clean_epochs['error_response'].average().get_data(picks=pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e7f05cda18343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:03:58.173472Z",
     "start_time": "2023-12-12T15:03:58.152237Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wavelet_filter(grand_average, central_freq = 6, signal_freq=500):\n",
    "    x = grand_average.flatten()\n",
    "    \n",
    "    # construct wavelet function\n",
    "    wavelet = Wavelet(('morlet', {'mu': central_freq}))\n",
    "    Wx, scales = cwt(x, wavelet, fs=500)\n",
    "    \n",
    "    freq = scale_to_freq(scales, wavelet, N=len(x), fs=signal_freq)\n",
    "    print(freq)\n",
    "    # Compute and normalize the power spectrum from the CWT coefficients\n",
    "    power_spectrum = np.abs(Wx)**2\n",
    "    normalized_power_spectrum = power_spectrum / np.sum(power_spectrum)\n",
    "    \n",
    "    # Flatten the normalized power spectrum for CDF calculation\n",
    "    flattened_spectrum = normalized_power_spectrum.flatten()\n",
    "    \n",
    "    # Use the Kaplan–Meier estimator from the lifelines library\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(durations=flattened_spectrum, event_observed=np.ones_like(flattened_spectrum))\n",
    "    \n",
    "    # Get the CDF values from the Kaplan–Meier estimator\n",
    "    cdf_values = 1 - kmf.survival_function_.KM_estimate\n",
    "    \n",
    "    # Calculate the threshold\n",
    "    threshold = 0.85 * (np.max(cdf_values) - np.min(cdf_values)) + np.min(cdf_values)\n",
    "    \n",
    "    # Plot the empirical CDF and the filtering model\n",
    "    plt.step(kmf.survival_function_.index, cdf_values, where='post', label='Empirical CDF')\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "    plt.title('Empirical CDF and Filtering Model')\n",
    "    plt.xlabel('Wavelet Coefficient')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # find the value of wavelets coefficient that are above threshold\n",
    "    cutoff_wavelet_index = np.where(cdf_values > threshold)[0][0]\n",
    "    cutoff_wavelet_coef = kmf.survival_function_.index[cutoff_wavelet_index]\n",
    "    print(f'Estimated threshold value for wavelet coefficients: {cutoff_wavelet_coef}')\n",
    "    \n",
    "    cwt_result_threshold_mask = np.where(normalized_power_spectrum >= cutoff_wavelet_coef, 1, 0)\n",
    "    \n",
    "    # Plot the CWT result\n",
    "    plt.figure(figsize=(12, 16))  \n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.imshow(abs_cwt, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title('CWT Magnitude')\n",
    "    \n",
    "    # \n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.imshow(normalized_power_spectrum, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title('Normalized Power Spectrum')\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.imshow(cwt_result_threshold_mask, extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title('Threshold Mask')\n",
    "    \n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.imshow(cwt_result_threshold_mask*np.abs(Wx), extent=[t[0], t[-1], freq[-1], freq[0]], aspect='auto', cmap='jet')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title('Thresholded Grand Average - Examle')\n",
    "    plt.show()\n",
    "    \n",
    "    return cwt_result_threshold_mask, wavelet, scales\n",
    "\n",
    "def filter_signal(Wx, mask, wavelet, scales):\n",
    "    time_domain_signal = icwt(mask * Wx, wavelet, scales)\n",
    "    \n",
    "    return time_domain_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f233bc888fccb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:04:00.681927Z",
     "start_time": "2023-12-12T15:03:59.566960Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cwt_result_threshold_mask, wavelet, scales = calculate_wavelet_filter(grand_average, central_freq=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5b4e83396a20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T14:35:37.059895Z",
     "start_time": "2023-12-12T14:35:37.035387Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = grand_average.flatten()\n",
    "\n",
    "# construct wavelet function\n",
    "wavelet_this = Wavelet(('morlet', {'mu': 6}))\n",
    "Wx, scales_this = cwt(x, wavelet, fs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f3bf1bdf8dda5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T14:35:38.100144Z",
     "start_time": "2023-12-12T14:35:38.082279Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "filtered_signal = filter_signal(Wx, cwt_result_threshold_mask, wavelet, scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edde1b3b8365661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T14:35:39.040326Z",
     "start_time": "2023-12-12T14:35:38.960555Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x)\n",
    "plt.plot(filtered_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45885b608ad11e3b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "freq = scale_to_freq(scales, wavelet, N=len(x), fs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef7a5785c77289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:36:55.132665Z",
     "start_time": "2023-12-12T15:36:55.098064Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(abs(Wx), index=freq, columns=np.linspace(-0.1, 0.9, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c7c108bf3d488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:36:55.850214Z",
     "start_time": "2023-12-12T15:36:55.842981Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466fa4deb4d0a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:36:56.852195Z",
     "start_time": "2023-12-12T15:36:56.520264Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data, cmap='jet', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3c14f81f053e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf7be61e9f5d97",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da49b959-db10-4dde-8993-0c47849fc673",
   "metadata": {},
   "source": [
    "## Regress single trial ERP on PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e053c1ccb0dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:48:28.524227Z",
     "start_time": "2023-12-12T09:48:28.416860Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "epochs_regressed=[]\n",
    "\n",
    "for idx, _ in enumerate(clean_epochs['error_response']):\n",
    "    epoch_data = clean_epochs['error_response'][idx].get_data(picks='FCz', tmin=-0.1, tmax=0.5).flatten()\n",
    "    lm.fit(X=X_transformed, y=epoch_data)\n",
    "    epoch_pred = lm.predict(X_transformed)\n",
    "    epochs_regressed.append(epoch_pred)\n",
    "    \n",
    "epochs_regressed = np.array(epochs_regressed)\n",
    "print(epochs_regressed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb7e432-1796-4727-b5c9-8a7d4a2badff",
   "metadata": {},
   "source": [
    "## Generate PCA components:\n",
    "\n",
    "1. Average filtered with wavelets single trials into grand average.\n",
    "2. Create variability matrix base on the grand average: variability in peak latency and variability in waveform compression.\n",
    "3. Perform PCA decomposition on set of modified ERPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd63e919e898ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:40:09.038467Z",
     "start_time": "2023-12-12T09:40:09.022310Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create grand average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353599b853ff321a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:40:09.647647Z",
     "start_time": "2023-12-12T09:40:09.637013Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pick = ['FCz']\n",
    "grand_average = clean_epochs['error_response'].average().get_data(picks=pick, tmin=-0.1, tmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf60c66e4d17d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:42:36.697985Z",
     "start_time": "2023-12-12T09:42:36.676747Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def ms_to_tp(value_in_ms, freq=500):\n",
    "    \"\"\"\n",
    "    Only for relative conversion of the lengths\n",
    "    :param value_in_ms: \n",
    "    :param freq: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ms_unit = freq/1000\n",
    "    value_in_tp = int(value_in_ms*ms_unit)\n",
    "    return value_in_tp\n",
    "\n",
    "def stretch(xs, coef, centre):\n",
    "    \"\"\"Scale a list by a coefficient around a point in the list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xs : list\n",
    "        Input values.\n",
    "    coef : float\n",
    "        Coefficient to scale by.\n",
    "    centre : int\n",
    "        Position in the list to use as a centre point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "\n",
    "    \"\"\"\n",
    "    grain = 100\n",
    "\n",
    "    stretched_array = np.repeat(xs, grain * coef)\n",
    "    stretched_array = np.array(stretched_array)\n",
    "    result = [chunk.mean() for chunk in chunks(stretched_array, grain)]\n",
    "\n",
    "    pivot_point = int(centre * coef)\n",
    "    first = pivot_point - centre\n",
    "    last = pivot_point + len(xs) - centre\n",
    "    result = result[first:last]\n",
    "\n",
    "    assert len(result) == len(xs), \"Length should be preserved\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunks(iterable, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from iterable.\n",
    "    Source: http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python#answer-312464\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n",
    "\n",
    "def generate_variability_matrix(X):\n",
    "    '''\n",
    "    \n",
    "    :param X: ndarray of shape (n_timepoints,)\n",
    "        Grand Average on given channel \n",
    "    :return: \n",
    "    '''\n",
    "    # Find the peak latency in the grand average signal /in tp\n",
    "    peak_latency_tp = np.argmin(X)\n",
    "    print(f'Peak latency in tp: {peak_latency_tp}')\n",
    "\n",
    "    # Parameters\n",
    "    latency_shifts = np.arange(-ms_to_tp(50), ms_to_tp(50), ms_to_tp(2))  # From -50 to 50 ms in steps of 5 ms\n",
    "    width_changes = np.arange(1, 1.5, 0.02)  # From 1 to 2 in steps of 0.05\n",
    "\n",
    "    # Initialize a list to store modified ERP responses\n",
    "    modified_responses = []\n",
    "\n",
    "    # Enumerate through latency shifts and width changes\n",
    "    for width_change in width_changes:\n",
    "        for latency_shift in latency_shifts:\n",
    "        # Apply latency shift\n",
    "            evoked_shifted = np.roll(X.flatten(), int(latency_shift))\n",
    "\n",
    "            # Calculate the stretched array\n",
    "            evoked_stretched = np.array(stretch(evoked_shifted, coef=width_change, centre=peak_latency_tp))\n",
    "            modified_responses.append(evoked_stretched)\n",
    "\n",
    "    # Convert the list of modified responses to a numpy array\n",
    "    modified_responses = np.array(modified_responses)\n",
    "\n",
    "    return modified_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35165c5087beb1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:43:54.797491Z",
     "start_time": "2023-12-12T09:43:51.897386Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2. Generate variability matrix\n",
    "variability_matrix = generate_variability_matrix(grand_average)\n",
    "\n",
    "sns.heatmap(\n",
    "    variability_matrix,\n",
    "    center=0,\n",
    "    cmap='Spectral'  \n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5537902e6035e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:44:16.146051Z",
     "start_time": "2023-12-12T09:44:15.180105Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 3. fit PCA\n",
    "pca = PCA(n_components=3)\n",
    "X = variability_matrix.T\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850fbe527428bf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:45:00.947036Z",
     "start_time": "2023-12-12T09:44:52.756540Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "PCA_comp = X_transformed.T\n",
    "x = np.linspace(0, 0.5, len(variability_matrix[0]))\n",
    "plt.plot(x, PCA_comp[0])\n",
    "plt.plot(x, PCA_comp[1])\n",
    "plt.plot(x, PCA_comp[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a76f7e99f641c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:45:59.712754Z",
     "start_time": "2023-12-07T08:45:59.708221Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def pre_process_eeg(input_fname, trigger_map=None, parameters=None):\n",
    "#     raw = mne.io.read_raw_bdf(input_fname, eog=['EX7', 'EX8'])\n",
    "# \n",
    "#     # 1. re-reference: to mastoids\n",
    "#     raw.set_eeg_reference(ref_channels=['M1', 'M2'])\n",
    "# \n",
    "#     # 2. segmentation -100 to 900 ms around the response\n",
    "#     epochs = create_epochs(raw_filtered, tmin=-.1, tmax=.9)\n",
    "# \n",
    "#     # 3. ocular artifact correction with ICA\n",
    "#     refined_epochs = ocular_correction_gratton(epochs)\n",
    "# \n",
    "#     # 6. Second re-baseline\n",
    "#     refined_epochs.apply_baseline()\n",
    "#\n",
    "#     # 7. Find bad trials: trials in which the EEG signal at the FCz or Cz site was greater than ± 150 μV are marked\n",
    "#     drop_log = find_bad_trials(refined_epochs, picks=['FCz','Cz'])\n",
    "# \n",
    "#     # 9. Wavelet filter (1 to 30 Hz in steps of 0.3 Hz)\n",
    "#     # todo\n",
    "\n",
    "#     # 10. Slicing wavelets: -100 - 500 around response\n",
    "#     # todo\n",
    "#\n",
    "#     # 11. PCA on grand average of inverted wavelets (after wavelets -> invert to get signal, average, do PCA)\n",
    "#     # todo -> this on cleaned_epochs = reject_bad_trials(refined_epochs, drop_log))\n",
    "#\n",
    "#     # 12. Regression: Y (invert single-trial wavelets) = PCA_3 .fit(); y_hat = .predict()\n",
    "#     # todo\n",
    "#\n",
    "#     # 13. peak amplitude of y_hat (single trail denoised signal)\n",
    "#     # todo\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ee243d0dbd508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:43:29.546710Z",
     "start_time": "2023-12-07T16:43:29.527110Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response_event_dict = {\n",
    "    'Stimulus/RE*ex*1_n*1_c_1*R*FB': 10003,\n",
    "    'Stimulus/RE*ex*1_n*1_c_1*R*FG': 10004,\n",
    "    'Stimulus/RE*ex*1_n*1_c_2*R': 10005,\n",
    "    'Stimulus/RE*ex*1_n*2_c_1*R': 10006,\n",
    "    'Stimulus/RE*ex*2_n*1_c_1*R': 10007,\n",
    "    'Stimulus/RE*ex*2_n*2_c_1*R*FB': 10008,\n",
    "    'Stimulus/RE*ex*2_n*2_c_1*R*FG': 10009,\n",
    "    'Stimulus/RE*ex*2_n*2_c_2*R': 10010,\n",
    "}\n",
    "\n",
    "new_response_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "events_mapping = {\n",
    "    'correct_response': [10003, 10004, 10008, 10009],\n",
    "    'error_response': [10005, 10006, 10007, 10010],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f510e19f5ab5a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:43:30.276213Z",
     "start_time": "2023-12-07T16:43:29.795203Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_brainvision(\n",
    "    vhdr_fname = 'data/GNG_AA0303-64 el.vhdr', preload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24661212c7cc043e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:43:32.044018Z",
     "start_time": "2023-12-07T16:43:31.945702Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_resampled = raw.copy().resample(sfreq=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d2545dcc16ca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:43:37.516448Z",
     "start_time": "2023-12-07T16:43:36.624047Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 2. 4-th order Butterworth filters\n",
    "raw_filtered = raw_resampled.copy().filter(\n",
    "        l_freq=.1,\n",
    "        h_freq=30.0,\n",
    "        n_jobs=10,\n",
    "        method='iir',\n",
    "        iir_params=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4f17491b2f76e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:57:20.185334Z",
     "start_time": "2023-12-07T16:57:19.048840Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = create_epochs(\n",
    "    raw_filtered,\n",
    "    tmin=-.1,\n",
    "    tmax=.5,\n",
    "    baseline=(-0.1, 0),\n",
    "    detrend=1,\n",
    "    events_to_select=response_event_dict,  # response_event_dict\n",
    "    new_events_dict=new_response_event_dict,  # new_response_event_dict\n",
    "    events_mapping=events_mapping,  # events_mapping\n",
    "    reject=None,\n",
    "    reject_by_annotation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9235a94835016c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:57:20.623063Z",
     "start_time": "2023-12-07T16:57:20.192015Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "epochs_copy = epochs.copy()\n",
    "epochs_picked_channels = epochs_copy.pick(picks=['FCz', 'Cz'])\n",
    "\n",
    "epochs_picked_channels.drop_bad()\n",
    "drop_log = epochs_picked_channels.drop_log\n",
    "print(drop_log)\n",
    "\n",
    "# channels with more than a 30 μV difference with the nearest six neighbors\n",
    "for idx, _ in enumerate(epochs_picked_channels):\n",
    "    epoch = epochs[idx]\n",
    "    epoch_data = epoch.get_data(copy=True)\n",
    "    # epoch_data[0] = np.random.normal(0,5,epoch_data[0].shape)\n",
    "    for ch_name, ch_idx in zip(epochs_picked_channels.info['ch_names'], np.arange(0, len(epochs_picked_channels.info['ch_names']))):\n",
    "        channel_data = epoch_data[0,ch_idx,:]\n",
    "\n",
    "        # EEG signal at the FCz or Cz site was greater than ± 150 μV were removed\n",
    "        if(abs(channel_data) > 150e-6).any():\n",
    "            print(f'BAD------ trail index {idx}, channel: {ch_name}')\n",
    "            new_drop_log_item = drop_log[idx] + (ch_name, ) if ch_name not in drop_log[idx] else drop_log[idx]\n",
    "            drop_log = tuple(new_drop_log_item if i == idx else item for i, item in enumerate(drop_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a9939fe3662d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:57:20.627176Z",
     "start_time": "2023-12-07T16:57:20.624521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "drop_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1821b0a53b97b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:57:20.667290Z",
     "start_time": "2023-12-07T16:57:20.628509Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs_copy = epochs.copy()\n",
    "cleaned_epochs = reject_bad_trials(epochs_copy, drop_log)\n",
    "evokes = []\n",
    "picks=['FCz', 'Cz']\n",
    "\n",
    "for ch_name in picks:\n",
    "    evoked = cleaned_epochs.copy().pick(picks=ch_name)['error_response'].average()\n",
    "    print(evoked)\n",
    "    evokes.append(evoked)\n",
    "    # print(X.shape)\n",
    "\n",
    "#     pca = PCA(n_components=3)\n",
    "#     X_transformed = pca.fit_transform(X)\n",
    "#     transformed_evokes[ch_name] = X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab624ef3967125",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d5ca1a2065057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:57:21.490196Z",
     "start_time": "2023-12-07T16:57:21.483771Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if len(epochs) == len(drop_log):\n",
    "#     epochs_to_drop_indices = []\n",
    "#     for idx, item in enumerate(drop_log):\n",
    "#         if ('FCz' in item) or ('Cz' in item):\n",
    "#             print(f'In item: {idx}')\n",
    "#             epochs_to_drop_indices.append(idx)\n",
    "# \n",
    "#     clean_epochs = epochs.copy().drop(\n",
    "#         indices = epochs_to_drop_indices,\n",
    "#         reason = 'EXCEED 150uV', \n",
    "#     )\n",
    "#     \n",
    "# else:\n",
    "#     print(f'Epochs length is not equal drop_log length:\\nepochs: {len(epochs)}\\ndrop_log{len(drop_log)}')\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25e1dff8ad3de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:44:22.437156Z",
     "start_time": "2023-12-07T16:44:22.432467Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# _, value = mne.preprocessing.peak_finder(evokes[0].copy().crop(0.02, 0.1).get_data().flatten(), extrema=-1)\n",
    "# idx, _ = np.where(evokes[0] == value)\n",
    "_, lat, amp = evokes[1].get_peak(tmin=0.02, tmax=0.1, return_amplitude=True, mode='abs')\n",
    "print(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ed16d7e68ee28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:44:25.459743Z",
     "start_time": "2023-12-07T16:44:25.275122Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = evokes[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132099af42134baa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:44:41.793630Z",
     "start_time": "2023-12-07T16:44:41.779247Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "evokes[0].get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645cb3595969133d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T15:56:37.496762Z",
     "start_time": "2023-12-07T15:56:37.426835Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(evokes[1])\n",
    "plt.axvline(x = idx, color = 'b')\n",
    "plt.plot(np.roll(evokes[1],12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed978783ea819538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T16:29:13.463645Z",
     "start_time": "2023-12-07T16:29:13.446583Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c287f66b70dd6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8be2708962382",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:29.321803Z",
     "start_time": "2023-12-12T09:35:29.315882Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb934a4cffc462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:29.806084Z",
     "start_time": "2023-12-12T09:35:29.794214Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grand_average = clean_epochs['error_response'].average().get_data(picks='FCz', tmin=-0.1, tmax=0.5)\n",
    "grand_average = grand_average.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6173bf042fd80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:30.302547Z",
     "start_time": "2023-12-12T09:35:30.297200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# x = np.linspace(-0.1, 0.5, len(grand_average))\n",
    "# plt.plot(x, grand_average)\n",
    "# \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc91f7e89f66ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:31.022422Z",
     "start_time": "2023-12-12T09:35:31.017119Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vm = generate_variability_matrix(grand_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f32bd4759dc3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:31.633959Z",
     "start_time": "2023-12-12T09:35:31.629630Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sns.heatmap(\n",
    "#     vm,\n",
    "#     center=0,\n",
    "#     cmap='Spectral',\n",
    "#     # xticklabels=np.arange(0, len(grand_average))\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450b72123bc9936",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:33.087669Z",
     "start_time": "2023-12-12T09:35:32.036739Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X = vm.T\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c103c2aa5123da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:36.755054Z",
     "start_time": "2023-12-12T09:35:34.019535Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(X_transformed.shape)\n",
    "\n",
    "PCA_comp = X_transformed.T\n",
    "\n",
    "x = np.linspace(0, 0.5, len(vm[0]))\n",
    "plt.plot(x, PCA_comp[0])\n",
    "plt.plot(x, PCA_comp[1])\n",
    "plt.plot(x, PCA_comp[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c627a332abc5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:40.923113Z",
     "start_time": "2023-12-12T09:35:40.820295Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "results=[]\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "for idx, _ in enumerate(clean_epochs['error_response']):\n",
    "    epoch_data = clean_epochs['error_response'][idx].get_data(picks='FCz', tmin=-0.1, tmax=0.5).flatten()\n",
    "    lm.fit(X=X_transformed, y=epoch_data)\n",
    "    epoch_pred = lm.predict(X_transformed)\n",
    "    results.append(epoch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4eeaa322090437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:35:42.561918Z",
     "start_time": "2023-12-12T09:35:42.551637Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931365f533f6614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:32:07.456454Z",
     "start_time": "2023-12-12T09:32:07.451387Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54506d818ae0e5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T09:33:18.386959Z",
     "start_time": "2023-12-12T09:32:08.242837Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-0.1, 0.5, len(grand_average))\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    plt.figure()\n",
    "    plt.plot(x, clean_epochs['error_response'][i].get_data(picks='FCz', tmin=-0.1, tmax=0.5).flatten())\n",
    "    plt.plot(x, results[i].flatten())\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525a282535c8d39",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19139b1fd3b9687",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9238b2cd9d05e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T08:36:14.950693Z",
     "start_time": "2023-12-12T08:36:14.945811Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "latency_shifts = np.arange(-ms_to_tp(50), ms_to_tp(50), ms_to_tp(5))  # From -50 to 50 ms in steps of 5 ms\n",
    "len(latency_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963d1e9a6fe757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T08:59:20.517363Z",
     "start_time": "2023-12-12T08:59:05.010967Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[0])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[1])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[2])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[3])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[4])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[5])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[6])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[7])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[8])\n",
    "plt.plot(np.linspace(-0.1, 0.5, len(vm[0])), vm[9])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7b5ec78d0caf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T08:39:00.528239Z",
     "start_time": "2023-12-12T08:39:00.526375Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff395220e14a00a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bed69c0d6c1b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T18:36:24.671582Z",
     "start_time": "2023-12-07T18:36:24.656007Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "a = this_evoked\n",
    "b = a\n",
    "for i in range(0,1):\n",
    "    b = np.array(stretch(b, 2, centre=28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d4fc68374c43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T18:36:25.102365Z",
     "start_time": "2023-12-07T18:36:25.097499Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aee72e98ba4cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T18:36:25.682653Z",
     "start_time": "2023-12-07T18:36:25.618845Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(a)\n",
    "plt.axvline(x=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f3bb87bbc0006",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T18:36:26.355913Z",
     "start_time": "2023-12-07T18:36:26.281771Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f3417d498975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T12:37:04.978157Z",
     "start_time": "2023-12-07T12:37:04.964743Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinability",
   "language": "python",
   "name": "erpinability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
